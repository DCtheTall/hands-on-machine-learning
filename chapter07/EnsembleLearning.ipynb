{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EnsembleLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "pm1FksGr0E8O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter 7: Ensemble Learning and Random Forests\n",
        "\n",
        "<i>Ensemble Learning</i> is the technique of aggregating the decisions made by many Machine Learning models in order to get a final result. An ensemble learning algorithm is called an <i>Ensemble method</i>. An ensemble of Decision Trees is called a <i>Random Forest</i>. Models that win Machine Learning competitions often combine several Ensemble methods, e.g. the winner of the [Netflix Prize competition](http://netflixprize.com/).\n",
        "\n",
        "## Voting Classifiers\n",
        "\n",
        "A <i>hard voting</i> classifier is a model that train multiple classifiers and then predicts the class that gets the most votes. Even if the models are <i>weak learners</i> (slightly better than random guessing) then an ensemble could be a <i>strong learner</i> provided there are enough diverse models in the ensemble.\n",
        "\n",
        "This is possible due to the fact that even if the models are just slightly better than random guessing, the more models' decisions you consider the more likely that the majority will select the correct class. However, this only is true if the models are different enough to not make the same errors while classifying data."
      ]
    },
    {
      "metadata": {
        "id": "VsodT0hb0BL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b77fabc2-8c4f-4f3f-b318-272695bcc1a4"
      },
      "cell_type": "code",
      "source": [
        "# Example of a VotingClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "  estimators=[('lr',log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "  voting='hard')\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train, y_train)\n",
        "  print('{}:'.format(clf.__class__.__name__), clf.score(X_test, y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression: 0.864\n",
            "RandomForestClassifier: 0.896\n",
            "SVC: 0.888\n",
            "VotingClassifier: 0.904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NZHcurcc5Z-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bagging and Pasting\n",
        "\n",
        "One way to use an Ensemble method is to train different types of classifiers, as shown above. Another is to train the same type of model on random subsets of the training set. Random sampling with replacement is called [bagging](http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf). Sampling without replacement is called [pasting](https://link.springer.com/article/10.1023/A:1007563306331).\n",
        "\n",
        "Bagging and pasting classifiers using the statistical mode, just like hard voting classifiers. Regressors will tend to use the average. Each individual predictor has a higher bias on the whole training set, but aggregation reduces both bias and variance. Generally the bias remains similar to the bias of a single model but have a lower variance, so ensemble models are less likely to overfit the training data.\n",
        "\n",
        "### Bagging and Pasting in Scikit-Learn"
      ]
    },
    {
      "metadata": {
        "id": "8rb5fv-t9Ked",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a37bdb8-a131-4044-eee5-abdfd3b61e13"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Setting bootstrap=False will have the classifier use pasting\n",
        "# instead of bagging.\n",
        "bag_clf = BaggingClassifier(\n",
        "  DecisionTreeClassifier(), n_estimators=500,\n",
        "  max_samples=100, bootstrap=True, n_jobs=-1)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.score(X_test, y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.904"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "HPQjzXRU9qPo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bootstrapping introduces more diversity into the subsets that each predictor is trained on, so bagging ends up with slightly higher bias than pasting, but lower variance. Generally, bagging results in better models thant pasting.\n",
        "\n",
        "### Out-of-Bag Evaluation\n",
        "\n",
        "Since bagging randomly selects proper subsets of the training set to train each model, the instances not included in a particular subset used for training a single model is called an <i>out-of-bag</i> instance. You can evaluate an ensemble by taking an average of how each model does on its oob instances."
      ]
    },
    {
      "metadata": {
        "id": "5kB_WVW4H5Oq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f65f841-c370-4004-8cee-98fa69ac5995"
      },
      "cell_type": "code",
      "source": [
        "# An example of including the oob_score of a BaggingClassifier in evaluation.\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "  DecisionTreeClassifier(), n_estimators=500,\n",
        "  bootstrap=True, n_jobs=-1, oob_score=True)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8933333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "rCfjH7B_JP_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "532a97e0-99b6-4263-d925-e611a172bb9e"
      },
      "cell_type": "code",
      "source": [
        "# The score on the test set should be approximately the average oob_score_.\n",
        "\n",
        "bag_clf.score(X_test, y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.904"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "ckT4haBLIeI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "58cb899a-fb08-4c3e-a8e3-e2e3b1f1b670"
      },
      "cell_type": "code",
      "source": [
        "# The oob_decision_function variable contains what the average probability\n",
        "# of each instance belongs to each class when its an oob instance.\n",
        "\n",
        "bag_clf.oob_decision_function_[:10]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.39393939, 0.60606061],\n",
              "       [0.34183673, 0.65816327],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.06741573, 0.93258427],\n",
              "       [0.35057471, 0.64942529],\n",
              "       [0.01546392, 0.98453608],\n",
              "       [1.        , 0.        ],\n",
              "       [0.97340426, 0.02659574]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "Lwyt5iUXKjk0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Random Patches and Random Subspaces\n",
        "\n",
        "The `BaggingClassifier` also supports sampling the features as well, using the `max_features` and `bootstrap_features` hyperparameters. This is helpful for training sets with a large number of features.\n",
        "\n",
        "Sampling both the training instances and features is called the <i>Random Patches</i> method. Keeping all training instances but sampling features is called the <i>Random Subspaces</i> method.\n",
        "\n",
        "## Random Forest\n",
        "\n",
        "A <i>Random Forest</i> is an ensemble of Decision Trees generally trained via the bagging method typically with `max_samples` set to the training set size."
      ]
    },
    {
      "metadata": {
        "id": "TeHKJ-1HdOgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f915d6a2-0f72-4f1d-f1cd-0d80054cb47a"
      },
      "cell_type": "code",
      "source": [
        "# An example of training Scikit-Learn's RandomForest class.\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "rnd_clf.score(X_test, y_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.92"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "lysmOilGxgv6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With a few exceptions, `RandomForestClassifier` has all of the hyperparameters of both a `DecisionTreeClassifier` and `BaggingClassifier`."
      ]
    },
    {
      "metadata": {
        "id": "kPCPqxPsxyrJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The following BaggingClassifier is equivalent to the previous\n",
        "# RandomForestClassifier.\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "  DecisionTreeClassifier(splitter='random', max_leaf_nodes=16),\n",
        "  n_estimators=500, max_samples=1., bootstrap=True, n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtlTMQ_syRvp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extra-Trees\n",
        "\n",
        "When you train Random Forests, you can add additional randomness by having the Decision Trees use random thresholds for each feature rather than trying to find the optimal threshold. These forests are called [Extremely Randomized Trees](https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf) (or <i>Extra-Trees</i> for short).\n",
        "\n",
        "Extra-Trees take much less time to train and introduce less variance into the system for the price of more bias. It is difficult to tell whether a `RandomForestClassifier` or an `ExtraTreesClassifier` will perform better for a certain Machine Learning problem, so often you have to try both to see which is a better model to use.\n",
        "\n",
        "### Feature Importance\n",
        "\n",
        "Random Forest classifiers can also be used to determine <i>feature importance</i> which Scikit-Learn measures as a weighted average how often a feature is used to reduce a node's Gini impurity. The weights are how many training instances are associated in a node or its descendants.\n",
        "\n",
        "Scikit-Learn's `RandomForestClassifier` computes the feature importances automatically during training, then scales the result so that the sum of the importances equals 1."
      ]
    },
    {
      "metadata": {
        "id": "ipqrTfl61TAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f81c8529-4e30-45eb-e6f6-f475c72f88b9"
      },
      "cell_type": "code",
      "source": [
        "# An example of using RandomForestClassifier to determine and compare\n",
        "# feature importances of a training set.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
        "rnd_clf.fit(iris.data, iris.target)\n",
        "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
        "  print(name, score)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sepal length (cm) 0.09232365491637388\n",
            "sepal width (cm) 0.024334044306895022\n",
            "petal length (cm) 0.4486165186699227\n",
            "petal width (cm) 0.4347257821068083\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}