{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EnsembleLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "pm1FksGr0E8O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter 7: Ensemble Learning and Random Forests\n",
        "\n",
        "<i>Ensemble Learning</i> is the technique of aggregating the decisions made by many Machine Learning models in order to get a final result. An ensemble learning algorithm is called an <i>Ensemble method</i>. An ensemble of Decision Trees is called a <i>Random Forest</i>. Models that win Machine Learning competitions often combine several Ensemble methods, e.g. the winner of the [Netflix Prize competition](http://netflixprize.com/).\n",
        "\n",
        "## Voting Classifiers\n",
        "\n",
        "A <i>hard voting</i> classifier is a model that train multiple classifiers and then predicts the class that gets the most votes. Even if the models are <i>weak learners</i> (slightly better than random guessing) then an ensemble could be a <i>strong learner</i> provided there are enough diverse models in the ensemble.\n",
        "\n",
        "This is possible due to the fact that even if the models are just slightly better than random guessing, the more models' decisions you consider the more likely that the majority will select the correct class. However, this only is true if the models are different enough to not make the same errors while classifying data."
      ]
    },
    {
      "metadata": {
        "id": "VsodT0hb0BL_",
        "colab_type": "code",
        "outputId": "2fcec635-0873-42b9-f647-a797dad8426d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Example of a VotingClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "  estimators=[('lr',log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "  voting='hard')\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train, y_train)\n",
        "  print('{}:'.format(clf.__class__.__name__), clf.score(X_test, y_test))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression: 0.864\n",
            "RandomForestClassifier: 0.896\n",
            "SVC: 0.888\n",
            "VotingClassifier: 0.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NZHcurcc5Z-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bagging and Pasting\n",
        "\n",
        "One way to use an Ensemble method is to train different types of classifiers, as shown above. Another is to train the same type of model on random subsets of the training set. Random sampling with replacement is called [bagging](http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf). Sampling without replacement is called [pasting](https://link.springer.com/article/10.1023/A:1007563306331).\n",
        "\n",
        "Bagging and pasting classifiers using the statistical mode, just like hard voting classifiers. Regressors will tend to use the average. Each individual predictor has a higher bias on the whole training set, but aggregation reduces both bias and variance. Generally the bias remains similar to the bias of a single model but have a lower variance, so ensemble models are less likely to overfit the training data.\n",
        "\n",
        "### Bagging and Pasting in Scikit-Learn"
      ]
    },
    {
      "metadata": {
        "id": "8rb5fv-t9Ked",
        "colab_type": "code",
        "outputId": "fb5f9b03-106b-48ef-dc94-4555870dc768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Setting bootstrap=False will have the classifier use pasting\n",
        "# instead of bagging.\n",
        "bag_clf = BaggingClassifier(\n",
        "  DecisionTreeClassifier(), n_estimators=500,\n",
        "  max_samples=100, bootstrap=True, n_jobs=-1)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.score(X_test, y_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.928"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "HPQjzXRU9qPo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bootstrapping introduces more diversity into the subsets that each predictor is trained on, so bagging ends up with slightly higher bias than pasting, but lower variance. Generally, bagging results in better models thant pasting.\n",
        "\n",
        "### Out-of-Bag Evaluation\n",
        "\n",
        "Since bagging randomly selects proper subsets of the training set to train each model, the instances not included in a particular subset used for training a single model is called an <i>out-of-bag</i> instance. You can evaluate an ensemble by taking an average of how each model does on its oob instances."
      ]
    },
    {
      "metadata": {
        "id": "5kB_WVW4H5Oq",
        "colab_type": "code",
        "outputId": "0789bd6c-3d7f-473d-a5ba-e58141e6b978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# An example of including the oob_score of a BaggingClassifier in evaluation.\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "  DecisionTreeClassifier(), n_estimators=500,\n",
        "  bootstrap=True, n_jobs=-1, oob_score=True)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8933333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "rCfjH7B_JP_7",
        "colab_type": "code",
        "outputId": "56cd40bd-4c45-4e06-8221-1154a7d7f9c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# The score on the test set should be approximately the average oob_score_.\n",
        "\n",
        "bag_clf.score(X_test, y_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.904"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "ckT4haBLIeI4",
        "colab_type": "code",
        "outputId": "7418a1c8-7cf1-4d49-e289-4ef2b32a74d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# The oob_decision_function variable contains what the average probability\n",
        "# of each instance belongs to each class when its an oob instance.\n",
        "\n",
        "bag_clf.oob_decision_function_[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.39459459, 0.60540541],\n",
              "       [0.34269663, 0.65730337],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.08994709, 0.91005291],\n",
              "       [0.36507937, 0.63492063],\n",
              "       [0.02061856, 0.97938144],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98830409, 0.01169591]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "Lwyt5iUXKjk0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Random Patches and Random Subspaces\n",
        "\n",
        "The `BaggingClassifier` also supports sampling the features as well, using the `max_features` and `bootstrap_features` hyperparameters. This is helpful for training sets with a large number of features.\n",
        "\n",
        "Sampling both the training instances and features is called the <i>Random Patches</i> method. Keeping all training instances but sampling features is called the <i>Random Subspaces</i> method.\n",
        "\n",
        "## Random Forest\n",
        "\n",
        "A <i>Random Forest</i> is an ensemble of Decision Trees generally trained via the bagging method typically with `max_samples` set to the training set size."
      ]
    },
    {
      "metadata": {
        "id": "TeHKJ-1HdOgI",
        "colab_type": "code",
        "outputId": "baf5986d-e91b-41d2-b62e-606662809cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# An example of training Scikit-Learn's RandomForest class.\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "rnd_clf.score(X_test, y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.912"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "lysmOilGxgv6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With a few exceptions, `RandomForestClassifier` has all of the hyperparameters of both a `DecisionTreeClassifier` and `BaggingClassifier`."
      ]
    },
    {
      "metadata": {
        "id": "kPCPqxPsxyrJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The following BaggingClassifier is equivalent to the previous\n",
        "# RandomForestClassifier.\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "  DecisionTreeClassifier(splitter='random', max_leaf_nodes=16),\n",
        "  n_estimators=500, max_samples=1., bootstrap=True, n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtlTMQ_syRvp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extra-Trees\n",
        "\n",
        "When you train Random Forests, you can add additional randomness by having the Decision Trees use random thresholds for each feature rather than trying to find the optimal threshold. These forests are called [Extremely Randomized Trees](https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf) (or <i>Extra-Trees</i> for short).\n",
        "\n",
        "Extra-Trees take much less time to train and introduce less variance into the system for the price of more bias. It is difficult to tell whether a `RandomForestClassifier` or an `ExtraTreesClassifier` will perform better for a certain Machine Learning problem, so often you have to try both to see which is a better model to use.\n",
        "\n",
        "### Feature Importance\n",
        "\n",
        "Random Forest classifiers can also be used to determine <i>feature importance</i> which Scikit-Learn measures as a weighted average how often a feature is used to reduce a node's Gini impurity. The weights are how many training instances are associated in a node or its descendants.\n",
        "\n",
        "Scikit-Learn's `RandomForestClassifier` computes the feature importances automatically during training, then scales the result so that the sum of the importances equals 1."
      ]
    },
    {
      "metadata": {
        "id": "ipqrTfl61TAV",
        "colab_type": "code",
        "outputId": "aff58343-3aa1-4811-eadf-084f344df345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# An example of using RandomForestClassifier to determine and compare\n",
        "# feature importances of a training set.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
        "rnd_clf.fit(iris.data, iris.target)\n",
        "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
        "  print(name, score)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sepal length (cm) 0.10312153243524984\n",
            "sepal width (cm) 0.026895705898274815\n",
            "petal length (cm) 0.4578268958787679\n",
            "petal width (cm) 0.41215586578770763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9uPB91ofZYhy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Boosting\n",
        "\n",
        "<i>Boosting</i> refers to an Ensemble method which trains each model sequentially, learning from the mistakes of the past model. The two most popular boosting algorithms are [AdaBoost](https://www.sciencedirect.com/science/article/pii/S002200009791504X) (short for <i>Adaptive Boosting</i>) and <i>Gradient Boosting</i>.\n",
        "\n",
        "### AdaBoost\n",
        "\n",
        "Adaptive Boosting (AdaBoost) is a Ensemble method where each successive model puts more weight on training instances that its predecessor gets wrong. The Ensemble generalizes like models that use bagging and pasting, except the models decisions are weighted based on their accuracy on the training set.\n",
        "\n",
        "Initially, each training instance's weight, $w^{\\,(i)}$ is set to $\\frac{1}{m}$. A first predictor is trained and its weighted error rate, $r_1$ is computed using the equation\n",
        "\n",
        "$$ r_j = \\frac{\\underset{\\large{\\hat{y}_j^{\\,(i)}\\neq y^{\\,(i)}}}{\\sum\\limits_{i=1}^m}w^{\\,(i)}}{\\sum\\limits_{i=1}^m w^{\\,(i)}} $$\n",
        "\n",
        "where $\\hat{y}_j^{\\,(i)}$ is the $j$<sup>th</sup> predictor's prediction on $i$<sup>th</sup> instance. The weight each predictor is given in the final result is given by\n",
        "\n",
        "$$ \\alpha_j = \\eta \\log{\\frac{1-r_j}{r_j}} $$\n",
        "\n",
        "where $\\eta$ is the <i>learning rate</i> hyperparameter (defaults to 1). The more accurate a predictor is, the higher its weight will be. Random guessing yields a weight of 0, and classifiers which do worse than random guessing are given a negative weight.\n",
        "\n",
        "The weights of each instances for training subsequent classifiers is given by\n",
        "\n",
        "$$ w^{\\,(i)} \\leftarrow \\left\\{\\begin{matrix}\n",
        "w^{\\,(i)} && \\text{if} \\;\\; \\hat{y}^{\\,(i)} = y^{\\,(i)} \\\\\n",
        "w^{\\,(i)}\\exp\\left(\\alpha_j\\right) && \\text{if} \\;\\; \\hat{y}^{\\,(i)} \\neq y^{\\,(i)}\n",
        "\\end{matrix} \\right. $$\n",
        "\n",
        "and then are normalized so that their sum equals 1. The algorithm then repeats this process for each classifier, and predicts new instances by seeing which class gets the most weighted votes. The Ensemble stops training when either it finds a model that is a perfect predictor or if it trains the maximum number of models specified. The final prediction is given by\n",
        "\n",
        "$$ \\hat{y}\\,(\\mathbf{x}) = \\underset{\\large{k}}{\\text{argmax}}\n",
        "\\underset{\\large{\\hat{y}_j(\\mathbf{x})\\,=\\,k}}{\\sum\\limits_{j\\,=\\,1}^N} \\alpha_j $$\n",
        "\n",
        "where $N$ is the total number of predictors.\n",
        "\n",
        "Scikit-Learn uses a multiclass variation of AdaBoost called [Stagewise Additive Modeling using a Multiclass Exponential log loss](https://web.stanford.edu/~hastie/Papers/samme.pdf) (SAMME) and a variant which relies on class probabilities and generally performs better called SAMME.R."
      ]
    },
    {
      "metadata": {
        "id": "zBnMQ5gra8h2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# An example of using AdaBoost with Scikit-Learn.\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "  DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "  algorithm='SAMME.R', learning_rate=0.5).fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_S7J0s5bQcn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting\n",
        "\n",
        "[Gradient Boosting](http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf) is another boosting algorithm which tries to fit each new model to the <i>residual errors</i> made by the previous model. Gradient boosting with Decision Trees is known as <i>Gradient Tree Boosting</i>, or <i>Gradient Boosted Regression Trees</i> (GBRT).\n",
        "\n",
        "Below is an example of working through a regression task using GBRT on a noisy quadratic dataset."
      ]
    },
    {
      "metadata": {
        "id": "It9BwkVKdBlp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating the dataset.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "m = 1000\n",
        "X = 10 * np.random.rand(m, 1) - 5\n",
        "y = (0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)).flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swyKJYzFd1RW",
        "colab_type": "code",
        "outputId": "1c73aff8-1e8c-4355-8370-d96cc70e2f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Training a single Decision Tree Regressor.\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2).fit(X_train, y_train)\n",
        "y_pred = tree_reg1.predict(X_test)\n",
        "mean_squared_error(y_test, y_pred) ** .5"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1913935801638433"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "Sc5vpXaT4Mmn",
        "colab_type": "code",
        "outputId": "6beefadc-b14c-4179-fdda-a854c79a7ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Training a second and third Decision Tree on the residual error of their\n",
        "# predecessor. We see that this method improves the performance of the\n",
        "# regressor.\n",
        "\n",
        "y_train2 = y_train - tree_reg1.predict(X_train)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2).fit(X_train, y_train2)\n",
        "\n",
        "y_train3 = y_train2 - tree_reg2.predict(X_train)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2).fit(X_train, y_train3)\n",
        "\n",
        "y_pred = tree_reg1.predict(X_test) + tree_reg2.predict(X_test) + \\\n",
        "         tree_reg3.predict(X_test)\n",
        "mean_squared_error(y_test, y_pred) ** .5"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.478201109283578"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "PDOupw2S2z2X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1271348c-96fd-49cc-8e49-f81ced43ce14"
      },
      "cell_type": "code",
      "source": [
        "# A simpler way is to train a Gradient Boosted Decision Tree\n",
        "# using Sci-Kit Learn.\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.)\n",
        "gbrt.fit(X_train, y_train)\n",
        "y_pred = gbrt.predict(X_test)\n",
        "mean_squared_error(y_test, y_pred) ** .5"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4782011092835778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Wq9yOWMt3N9J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `learning_rate` hyperparameter controls how much influence each tree has on the final decision. A smaller learning rate requires more trees to train but tends to generalize better."
      ]
    },
    {
      "metadata": {
        "id": "sYUCz-HM3nqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f4a3ff71-ebe0-47a1-fbc2-3893187bdde9"
      },
      "cell_type": "code",
      "source": [
        "# The staged_predict() method allows us to compare the MSE of the GBRT\n",
        "# as a function of the number of trees used for training. This is used to\n",
        "# implement early stopping.\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val, y_pred)\n",
        "          for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors)\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2,\n",
        "                                      n_estimators=bst_n_estimators)\n",
        "gbrt_best.fit(X_train, y_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
              "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
              "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
              "             min_impurity_split=None, min_samples_leaf=1,\n",
              "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "             n_estimators=86, n_iter_no_change=None, presort='auto',\n",
              "             random_state=None, subsample=1.0, tol=0.0001,\n",
              "             validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "sB5gtTJn4miw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Implementing early stopping during training.\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
        "\n",
        "min_val_error = float('inf')\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1, 120):\n",
        "  gbrt.n_estimators = n_estimators\n",
        "  gbrt.fit(X_train, y_train)\n",
        "  y_pred = gbrt.predict(X_val)\n",
        "  val_error = mean_squared_error(y_val, y_pred)\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    error_going_up += 1\n",
        "  else:\n",
        "    error_going_up += 1\n",
        "    if error_going_up == 5:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J3xAeQzg5qpk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `GradientBoostingRegressor` class also supports a `subsample` hyperparameter which lets you select the size of a random subset of the training data to train each tree. This method is called <i>Stochastic Gradient Boosting</i>.\n",
        "\n",
        "## Stacking\n",
        "\n",
        "The final Ensemble method is called <i>stacking</i> (short for <i>stacked generalization</i>) where we train a model to aggregate the results of each individual predictor in the ensemble.\n",
        "\n",
        "One method of stacking is to use a <i>hold-out set</i>. While training, split the training set into 2 subsets. The predictors in the ensemble are trained with the first subset. Then, the predictors make predictions on the second subset and the aggregation model is trained using the predictors' outputs as its input features.\n",
        "\n",
        "It is possible to split the training set into more than 2 subsets and train multiple layers of predictors using each subset. For instance, you can split the training set into 3 subsets. The first layer of predictors are trained using the first subset. Then their predictions on the 2<sup>nd</sup> subset are used the train the 2<sup>nd</sup> layer of predictors. Finally both layers make predictions on the 3<sup>rd</sup> subset to train the aggregation model which makes the final predictions.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "### 1. If you have trained five different models on the exact same training data and they all have achieved 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
        "\n",
        "Yes, it is possible to aggregate the models to get a better result. If the models make mistakes on different training samples, then its possible that by aggregating the results with hard voting. But, if the models make mistakes on the same instances, aggregating the results will not improve the performance.\n",
        "\n",
        "### 2. What is the difference between hard and soft voting classifiers?\n",
        "\n",
        "Hard voting is when an ensemble of classifiers make predictions about the class of a new instance and then the model chooses the most frequent class. Soft voting is when the predictors in the ensemble output the probabilites that a new instance belongs to each class, and the model selects the prediction with the highest probability.\n",
        "\n",
        "### 3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
        "\n",
        "Yes, you can speed up bagging ensembles by distributing the work across multiple servers. Once the subsets are chosen, you can send copies of the data to each server, so even with replacement bagging can be distributed. Pasting ensembles can also be distributed across multiple servers.\n",
        "\n",
        "Boosting ensembles cannot be improved by distributing the work. Since each predictor is trained successively based on the errors made by the previous one, each predictor needs to wait for the previous to finish in order.\n",
        "\n",
        "Random forests, which are ensembles of decision trees trained using bagging, can have their performance improved by distributing the work.\n",
        "\n",
        "Stacking ensembles can distribute the training of each layer of the ensemble across different servers, but each layer still needs to be trained in series.\n",
        "\n",
        "### 4. What is the benefit of out-of-bag evaluation?\n",
        "\n",
        "You can use out-of-bag evaluation in order to evaluate each predictor that is trained during each phase of a bagging ensemble.\n",
        "\n",
        "### 5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\n",
        "\n",
        "Extra-Trees are more random than Random Forests because they use random decision thresholds. This extra randomness reduces the model's variance, which helps if you are overfitting the training data. Extra-Trees are faster to train than regular Random Forests since they do not need to find the optimal splitting threshold at each node.\n",
        "\n",
        "### 6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\n",
        "\n",
        "You can add more estimators to the AdaBoost ensemble. You can also try decreasing the learning rate of the ensemble so that the model more variance. Also if the current model is using the SAMME algorithm, you can try using the SAMME.R algorithm as well.\n",
        "\n",
        "### 7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
        "\n",
        "You should increase the learning rate, since it will reduce the model's variance.\n",
        "\n",
        "### 8. Load the MNIST dat and split it into a training set, a validation set, and a test set. Then train various classifiers such as Random Forest, Extra-Trees, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. How much better does it perform on the test set than the individual classifiers?"
      ]
    },
    {
      "metadata": {
        "id": "easp4QxlYei9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fetching the MNIST data set.\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "mnist.target = mnist.target.astype(np.int8)\n",
        "sorted_indices = np.argsort(mnist.target)\n",
        "X = mnist.data[sorted_indices]\n",
        "y = mnist.target[sorted_indices]\n",
        "rand_idx = np.random.permutation(len(X))\n",
        "X, y = X[rand_idx], y[rand_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E4eJkyaoaa4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "40dfc032-afce-47a9-e004-8bfc21969090"
      },
      "cell_type": "code",
      "source": [
        "# Using RandomizedSearchCV to find the best parameters for a RandomForest\n",
        "\n",
        "from scipy.stats import randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = \\\n",
        "  train_test_split(X, y, random_state=42, test_size=10000)\n",
        "\n",
        "X_train, X_val, y_train, y_val = \\\n",
        "  train_test_split(X_train_val, y_train_val, random_state=42, test_size=10000)\n",
        "\n",
        "param_dist = {\n",
        "  'n_estimators': randint(200, 300),\n",
        "  'max_depth': randint(5, 20),\n",
        "  'max_features': randint(50, 150)\n",
        "}\n",
        "\n",
        "rnd_search = RandomizedSearchCV(RandomForestClassifier(), param_dist,\n",
        "                                n_iter=10, cv=3)\n",
        "rnd_search.fit(X_train[:1000], y_train[:1000])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
              "            oob_score=False, random_state=None, verbose=0,\n",
              "            warm_start=False),\n",
              "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
              "          param_distributions={'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fcd8f9f6518>, 'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fcd8db9fa20>, 'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fcd8db9f860>},\n",
              "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "          return_train_score='warn', scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "VosbRXureg_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5982c281-8d35-4fa9-d384-16f38c712041"
      },
      "cell_type": "code",
      "source": [
        "rnd_search.best_score_"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.891"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "MlyqOBSBec_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7beccff4-7e3a-4da1-c430-0afd19c905d5"
      },
      "cell_type": "code",
      "source": [
        "rnd_search.best_params_"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 19, 'max_features': 66, 'n_estimators': 282}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "4Q-WTD9Zenp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6dac0ddd-a826-4dc4-e5b6-7db3fdb4e34b"
      },
      "cell_type": "code",
      "source": [
        "rf_clf = rnd_search.best_estimator_\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_clf.score(X_train, y_train)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "YDjdZtgSfqYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca70b328-e0fd-45a8-e29f-9a835c87614f"
      },
      "cell_type": "code",
      "source": [
        "# The model is still overfitting, but able to get 96% accuracy on the validation\n",
        "# set.\n",
        "\n",
        "rf_clf.score(X_val, y_val)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "7wz0Nq0vkwlP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "7054e35d-e84e-4b8c-ef73-7964664ee948"
      },
      "cell_type": "code",
      "source": [
        "# Now training an Extra-Tree using RandomizedSearchCV.\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "param_dist = {\n",
        "  'n_estimators': randint(2, 10),\n",
        "  'max_depth': randint(2, 10),\n",
        "  'max_features': randint(10, len(X[0])),\n",
        "}\n",
        "\n",
        "rnd_search = RandomizedSearchCV(ExtraTreesClassifier(), param_dist,\n",
        "                                n_iter=10, cv=5)\n",
        "rnd_search.fit(X_train[:1000], y_train[:1000])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
              "          estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
              "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "           min_samples_leaf=1, min_samples_split=2,\n",
              "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
              "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
              "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
              "          param_distributions={'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fcd8db90898>, 'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fcd8f9c22b0>, 'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fcd8f9c2ac8>},\n",
              "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "          return_train_score='warn', scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "2P6imo30lkob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "491baffd-ed56-4fc0-d557-1bb80c1296f3"
      },
      "cell_type": "code",
      "source": [
        "rnd_search.best_score_"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.819"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "H6YnRWrTliLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a1f6734-e830-43d6-f617-9d2ee6374c23"
      },
      "cell_type": "code",
      "source": [
        "rnd_search.best_params_"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 9, 'max_features': 695, 'n_estimators': 7}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "5kcMuXRclqI9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4ad0e72-abdc-4f54-d595-593acdf334c7"
      },
      "cell_type": "code",
      "source": [
        "et_clf = rnd_clf.base_estimator_\n",
        "et_clf.fit(X_train, y_train)\n",
        "et_clf.score(X_train, y_train)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "metadata": {
        "id": "QFyX5WT5mCEk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdafb303-9073-4e81-b728-ada92cbcf3fd"
      },
      "cell_type": "code",
      "source": [
        "et_clf.score(X_val, y_val)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8671"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    }
  ]
}