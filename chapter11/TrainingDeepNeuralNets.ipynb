{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainingDeepNeuralNets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axcfgr1z1naq",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 11: Training Deep Neural Nets\n",
        "\n",
        "In the previous chapter, we trained a neural network with 2 hidden layers. More complex problems require networks with more hidden layers with hundreds of neurons per layer. Training these can lead to several problems:\n",
        "\n",
        "- The _vanishing gradients_ and _exploding gradients_ problem makes lower levels hard to train.\n",
        "- Training a large network can be very slow.\n",
        "- A model with millions of parameters risks overfitting the training data.\n",
        "\n",
        "Below we will discuss methods for solving all of these problems.\n",
        "\n",
        "## Vanishing/Exploding Gradients Problem\n",
        "\n",
        "While training a neural network with backpropagation, the algorithm finds the components of the error contributed by each layer to compute the error gradient.  Gradients can often get smaller and smaller as the algorithm progresses, resulting in the gradient contribution from the lower layers approaching zero. This is known as the _vanishing gradient_ problem. Alternatively, the gradient can also can grow bigger and bigger which can cause the algorithm to diverge. This is called the _exploding gradient problem_.\n",
        "\n",
        "Around 2010, a paper titled [\"Understanding the Difficulty of Training Deep Feedforward Neural Networks\"](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) found some reasons for this. The sigmoid activation function as well as the random initialization of the weight matrices' elements using a normal distribution with a mean of 0 and a standard deviation of 1. The paper showed the variance of the outputs was much larger than the variance of the inputs. Going forward in the network, the variance kept getting larger and it results in the activation saturating near the horizontal asymptotes, which causes the gradient to vanish.\n",
        "\n",
        "### Xavier and He Initialization\n",
        "\n",
        "The authors of the paper found that one way to prevent the vanishing/exploding gradient problem is to ensure that the variance of the input and output of each layer is the same. One way to do this is to initialize the weights matrix using a normal distribution with a mean of 0 and a standard deviation given by\n",
        "\n",
        "$$ \\sigma = \\sqrt{\\frac{2}{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "or a uniform distribution centered at 0 with a radius, $r$, given by\n",
        "\n",
        "$$ r = \\sqrt{\\frac{6}{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "where $n_\\text{ inputs}$ and $n_\\text{ outputs}$ is the number of input  or output connections in that particular layer. This is often known as _Xavier initialization_ after the author's first name, or sometimes _Glorot initialization_.\n",
        "\n",
        "For the ReLU activation function, we use a normal distribution with a standard deviation given by\n",
        "\n",
        "$$ \\sigma = \\frac{2}{\\sqrt{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "or a uniform distribution with a radius given by\n",
        "\n",
        "$$ r = \\sqrt{\\frac{24}{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "which is known as _He initialization_. Below is an example of creating a layer of a neural network which uses _He initialization_. By default, `tf.layers.dense()` uses Xavier initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW0y5GYZFS11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden = 100\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.relu,\n",
        "                          kernel_initializer=he_init, name='hidden1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUVypontGCD4",
        "colab_type": "text"
      },
      "source": [
        "### Nonsaturating Activation Functions\n",
        "\n",
        "One of the causes of the vanishing/exploding gradient problem discussed in the paper is the sigmoid activation function. The ReLU activation function performs much better, but it has a different problem. If some neurons output negative values, after the application of the activation function, their output will be stuck at 0. Since the gradient is also 0, the neuron remains \"dead.\"\n",
        "\n",
        "One solution to this problem is to use a \"leaky\" ReLU function, given by\n",
        "\n",
        "$$ \\text{LeakyReLU}(z) = \\max(\\alpha z, z) $$\n",
        "\n",
        "where $\\alpha$ is the slope ofthe ReLU function when the value of $z$ is less than 0. Researchers have found that this activation function performs better than the \"hard\" ReLU function. You can even have $\\alpha$ be a parameter that the model learns during training. This prevents neurons from completely dying.\n",
        "\n",
        "Another activation function that performs better than leaky ReLU that was proposed in this [paper](https://arxiv.org/pdf/1511.07289v5.pdf) by Djork-Arn√© Clevert called the _exponential linear unit_ (ELU) given by\n",
        "\n",
        "$$ \\text{ELU}_\\alpha(z) = \\left\\{ \\begin{matrix}\n",
        "\\alpha\\,(\\exp(z) - 1) && \\text{if}\\;z < 0 \\\\\n",
        "z && \\text{if}\\; z \\geq 0\n",
        "\\end{matrix} \\right. $$\n",
        "\n",
        "It has the following differences from the ReLU function:\n",
        "\n",
        "- It takes negative values when $z < 0$ . which allows the unit to have an average output closer to 0. This helps alleviate the vanishing radient problem. You can tweak the hyperparameter, $\\alpha$, sets the negative number that ELU approaches.\n",
        "\n",
        "- It has a nonzero gradient when $z < 0$, preventing the dying units issue.\n",
        "\n",
        "- The function is differentiable everywhere, which helps the speed of Gradient Descent.\n",
        "\n",
        "The disadvantage of ELU is that it takes longer to compute than ReLU. The extra time is compensated for the fact that it helps Gradient Descent converge fasted, but it does cause the model to make predictions more slowly.\n",
        "\n",
        "TensorFlow offeres an implementation of ELU which is used in the code example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m1ATbFgN1Th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden = 100\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, name='hidden1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx9wtyEcN-1W",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow does not have an implementation of leaky ReLU, but it is easy to define ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7eAj0MgOJTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "  return tf.maximum(alpha * z, z)\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden = 100\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden, activation=leaky_relu, name='hidden1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3lOO3R7OYpc",
        "colab_type": "text"
      },
      "source": [
        "### Batch Normalization\n",
        "\n",
        "In this [paper](https://arxiv.org/pdf/1502.03167v3.pdf) Sergey Ioffe and Christian Szegedy proposed a technique called _Batch Normalization_ (BN) to address both the vanishing/exploding gradient problem and the problem that the distribution of each layer's inputs change when the parameters of the previous layers change (i.e. the _Internal Covariate Shift_ problem).\n",
        "\n",
        "The technique adds an operation to the model just before applying the activation function of each layer. It zero-centers and normalizes the inputs, then it scales and shifts the result using two new parameters per layer. This lets the model learn the optimal mean and shift for each layer.\n",
        "\n",
        "The algorithm starts by first computing the empirical mean for the current mini-batch, $B$, given by\n",
        "\n",
        "$$ \\mu_B = \\frac{1}{m_B} \\sum\\limits_{i\\,=\\,1}^{m_B} \\mathbf{x}^{(i)} $$\n",
        "\n",
        "Next, we find the empirical standard deviation, given by\n",
        "\n",
        "$$ \\sigma_B^{\\;\\;2} = \\frac{1}{m_B} \\sum\\limits_{i\\,=\\,1}^{m_B} \\left( \\mathbf{x}^{(i)} - \\mu_B \\right)^2 $$\n",
        "\n",
        "Then we zero-center and normalize the inputs in the mini-batch\n",
        "\n",
        "$$ \\hat{\\mathbf{x}}^{(i)} = \\frac{\\mathbf{x}^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^{;\\;2} + \\epsilon}} $$\n",
        "\n",
        "where $\\epsilon$ is a small number, typically $10^{-5}$, called the _smoothing term_ to avoid division by zero. Finally it computes the output given by\n",
        "\n",
        "$$ \\mathbf{z}^{(i)} = \\gamma\\,\\hat{\\mathbf{x}}^{(i)} + \\beta $$\n",
        "\n",
        "where $\\gamma$ is the scaling parameter and $\\beta$ is the shift parameter which are learned during training.\n",
        "\n",
        "When the model makes predictions, it uses the empirical mean and standard deviation of the entire training set. In the end, the model ends up learning 4 parameters: the mean of the training set, $\\mu$; the standard deviation of the training set, $\\sigma$; the scaling parameter, $\\gamma$; and the shift parameter, $\\beta$.\n",
        "\n",
        "Adding Batch Normalization to a deep neural network improves the performance of the model, lets you skip normalizing the data before training the data, and helps the model converge to the optimal parameters in fewer training iterations. However, using Batch Normalization causes the model to make predictions slower since it adds another computational step for making predictions.\n",
        "\n",
        "#### Implementing Batch Normalization with TensorFlow\n",
        "\n",
        "TensorFlow provides a `tf.nn.batch_normalization()` function which normalizes and centers the data, but you must compute the mean and standard deviation yourself.You also have to handle the creation of the scaling and offfset parameters. TensorFlow also includes a `tf.layers.batch_normalization()` function which handles all of batch normalization for you. Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jir_-CpyJfvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2 # MNIST dataset.\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "\n",
        "# Indicates if the batch normalization should be using the mini-batch's mean\n",
        "# or the mean of the entire training set (same with standard deviation).\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logts = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
        "                                      momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbzsx0D4M1Kq",
        "colab_type": "text"
      },
      "source": [
        "The BN algorithm uses _exponential decay_ to compute a running average, which is why it requires the _momentum_ parameter. Given a new value, $v$, it updates the running average $\\hat{v}$ given by\n",
        "\n",
        "$$ \\hat{v} \\leftarrow \\hat{v} \\times \\text{momentum} + v \\times (1 - \\text{momentum}) $$\n",
        "\n",
        "Momentum values should be typically close to 1, e.g. 0.9, 0.99, or 0.999.\n",
        "\n",
        "Below is an example of using _partial application_ using the `functools` library in order to make the code less repetitive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3ESzVlLNmbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "\n",
        "# Indicates if the batch normalization should be using the mini-batch's mean\n",
        "# or the mean of the entire training set (same with standard deviation).\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saqpC1nRRtEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up the rest of the graph for training.\n",
        "\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxI0G_EvSbV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading the MNIST dataset.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
        "\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "  rnd_idx = np.random.permutation(len(X))\n",
        "  n_batches = len(X) // batch_size\n",
        "  for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "    X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "    yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szFxyIuFTSXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "05ac9a19-5cce-4e86-88cb-a74f88155ab9"
      },
      "source": [
        "# Training the neural network using Batch Normalization.\n",
        "# In just 20 training iterations it achieves 97% accuracy on the\n",
        "# validation set.\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "# These extra ops are for training the scaling and offset parameters\n",
        "# in batch normalization.\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run([training_op, extra_update_ops],\n",
        "               feed_dict={training: True, X: X_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Validation Accuracy: 0.8845999836921692\n",
            "Epoch: 1 Validation Accuracy: 0.9092000126838684\n",
            "Epoch: 2 Validation Accuracy: 0.9232000112533569\n",
            "Epoch: 3 Validation Accuracy: 0.9301999807357788\n",
            "Epoch: 4 Validation Accuracy: 0.9386000037193298\n",
            "Epoch: 5 Validation Accuracy: 0.9422000050544739\n",
            "Epoch: 6 Validation Accuracy: 0.9452000260353088\n",
            "Epoch: 7 Validation Accuracy: 0.9506000280380249\n",
            "Epoch: 8 Validation Accuracy: 0.9538000226020813\n",
            "Epoch: 9 Validation Accuracy: 0.9557999968528748\n",
            "Epoch: 10 Validation Accuracy: 0.9595999717712402\n",
            "Epoch: 11 Validation Accuracy: 0.9602000117301941\n",
            "Epoch: 12 Validation Accuracy: 0.9624000191688538\n",
            "Epoch: 13 Validation Accuracy: 0.9642000198364258\n",
            "Epoch: 14 Validation Accuracy: 0.9648000001907349\n",
            "Epoch: 15 Validation Accuracy: 0.9682000279426575\n",
            "Epoch: 16 Validation Accuracy: 0.9675999879837036\n",
            "Epoch: 17 Validation Accuracy: 0.9679999947547913\n",
            "Epoch: 18 Validation Accuracy: 0.9679999947547913\n",
            "Epoch: 19 Validation Accuracy: 0.9706000089645386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K51njD5WZmm",
        "colab_type": "text"
      },
      "source": [
        "An alternate syntax to training a model this way is to define the `training_op` the following way:\n",
        "\n",
        "```python\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(extra_update_ops):\n",
        "        training_op = optimizer.minimize(loss)\n",
        "```\n",
        "\n",
        "this lets you train the model using the more simple syntax:\n",
        "\n",
        "```python\n",
        "sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "```\n",
        "\n",
        "### Gradient Clipping\n",
        "\n",
        "One way to solve the exploding gradients problem is to clip the gradients' values to a defined range. This technique is called [_Gradient Clipping_](http://proceedings.mlr.press/v28/pascanu13.pdf). Though in general people prefer Batch Normalization. Below is an example of Gradient Clipping using TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-JHcp_LXL3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example of gradient clipping.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "hidden1_act = tf.nn.elu(hidden1)\n",
        "\n",
        "hidden2 = tf.layers.dense(hidden1_act, n_hidden2, name='hidden2')\n",
        "hidden2_act = tf.nn.elu(hidden2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(hidden2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "# Gradient clipping is here.\n",
        "threshold = 1.0\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "grads_and_vars = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
        "              for grad, var in grads_and_vars]\n",
        "training_op = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Z-m3dzZ5Sm",
        "colab_type": "text"
      },
      "source": [
        "## Reusing Pretrained Layers\n",
        "\n",
        "Instead of training a large DNN from scratch, it is generally better to reuse an existing neural network used for a similar task, then reuse the lower layers of this network. This technique is called _transfer learning_.\n",
        "\n",
        "### Reusing a TensorFlow model\n",
        "\n",
        "Below is an example of saving and restoring a TensorFlow model using the `tr.train.import_meta_graph()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mswAchPZdh6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the graph and saving it.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# New code here!\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  saver.save(sess, './my_model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3qPPnLIfYii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting nodes in the previous graph for reuse.\n",
        "\n",
        "X = tf.get_default_graph().get_tensor_by_name('X:0')\n",
        "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
        "accuracy = tf.get_default_graph().get_tensor_by_name('accuracy:0')\n",
        "training_op = tf.get_default_graph().get_operation_by_name('GradientDescent')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJTAb84Bf2Pp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "11e2b6df-e2e0-416e-95c7-6e8bb1c0617f"
      },
      "source": [
        "# Listing the operations in the predefined graph, truncated for readability.\n",
        "\n",
        "for op in tf.get_default_graph().get_operations()[:20]:\n",
        "  print(op.name)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X\n",
            "y\n",
            "training/input\n",
            "training\n",
            "hidden1/kernel/Initializer/random_uniform/shape\n",
            "hidden1/kernel/Initializer/random_uniform/min\n",
            "hidden1/kernel/Initializer/random_uniform/max\n",
            "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden1/kernel/Initializer/random_uniform/sub\n",
            "hidden1/kernel/Initializer/random_uniform/mul\n",
            "hidden1/kernel/Initializer/random_uniform\n",
            "hidden1/kernel\n",
            "hidden1/kernel/Assign\n",
            "hidden1/kernel/read\n",
            "hidden1/bias/Initializer/zeros\n",
            "hidden1/bias\n",
            "hidden1/bias/Assign\n",
            "hidden1/bias/read\n",
            "hidden1/MatMul\n",
            "hidden1/BiasAdd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRkkdLD-jU1v",
        "colab_type": "text"
      },
      "source": [
        "Below is an example of creating a collection of important operations. This is often helpful if the graph is large and you only want to reuse certain operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bc_2l9ggN-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the original graph.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# New code here!\n",
        "for op in (X, y, accuracy, training_op):\n",
        "  tf.add_to_collection('my_important_ops', op)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  saver.save(sess, './my_model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGuXXeZ9jmrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Restoring the graph and getting the operations from the collection.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "saver = tf.train.import_meta_graph('./my_model.ckpt.meta')\n",
        "\n",
        "X, y, accuracy, training_op = tf.get_collection('my_important_ops')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjw5pBVmqC5Q",
        "colab_type": "text"
      },
      "source": [
        "You can also define a restore Saver which will only restore specified variables. This is useful if you only want to restore the lower layers of a neural network. Below is an example of restoring only the lower layers of a neural network using a saver which only restores specified variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kqrygEnlNDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a graph with 5 hidden layers. First implementing gradient clipping.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None,), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(None))\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  layer = X\n",
        "  for i, n_hidden in enumerate((300, 100, 50, 20)):\n",
        "    hidden = tf.layers.dense(layer, n_hidden1,\n",
        "                             name='hidden{}'.format(i+1))\n",
        "    layer = tf.nn.relu(hidden, name='relu{}'.format(i+1))\n",
        "  logits = tf.layers.dense(layer, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                             labels=y)\n",
        "  loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iUbtgyCqapj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the savers and running the training.\n",
        "\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]') # Regex string\n",
        "reuse_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwbNBPC4CxFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "77c89e2a-7129-4f6e-eb4a-6d312ab0cace"
      },
      "source": [
        "# Training the model one time to train the lower layers of the neural network.\n",
        "\n",
        "import os\n",
        "\n",
        "model_path = './my_model.ckpt'\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, model_path)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Validation set accuracy: 0.7983999848365784\n",
            "Epoch: 1 Validation set accuracy: 0.8791999816894531\n",
            "Epoch: 2 Validation set accuracy: 0.8996000289916992\n",
            "Epoch: 3 Validation set accuracy: 0.9128000140190125\n",
            "Epoch: 4 Validation set accuracy: 0.920199990272522\n",
            "Epoch: 5 Validation set accuracy: 0.9265999794006348\n",
            "Epoch: 6 Validation set accuracy: 0.930400013923645\n",
            "Epoch: 7 Validation set accuracy: 0.9337999820709229\n",
            "Epoch: 8 Validation set accuracy: 0.9381999969482422\n",
            "Epoch: 9 Validation set accuracy: 0.9405999779701233\n",
            "Epoch: 10 Validation set accuracy: 0.9426000118255615\n",
            "Epoch: 11 Validation set accuracy: 0.9472000002861023\n",
            "Epoch: 12 Validation set accuracy: 0.9484000205993652\n",
            "Epoch: 13 Validation set accuracy: 0.9508000016212463\n",
            "Epoch: 14 Validation set accuracy: 0.9527999758720398\n",
            "Epoch: 15 Validation set accuracy: 0.954800009727478\n",
            "Epoch: 16 Validation set accuracy: 0.9575999975204468\n",
            "Epoch: 17 Validation set accuracy: 0.9584000110626221\n",
            "Epoch: 18 Validation set accuracy: 0.9581999778747559\n",
            "Epoch: 19 Validation set accuracy: 0.9610000252723694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbbnS6dcLPys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8c960f36-fc27-4101-8316-3cd91eeb3a8b"
      },
      "source": [
        "# Training the model again, this time we restore the lower layers\n",
        "\n",
        "new_model_path = './my_model_new.ckpt'\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, new_model_path)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model.ckpt\n",
            "Epoch: 0 Validation set accuracy: 0.9381999969482422\n",
            "Epoch: 1 Validation set accuracy: 0.9459999799728394\n",
            "Epoch: 2 Validation set accuracy: 0.951200008392334\n",
            "Epoch: 3 Validation set accuracy: 0.9531999826431274\n",
            "Epoch: 4 Validation set accuracy: 0.9545999765396118\n",
            "Epoch: 5 Validation set accuracy: 0.9567999839782715\n",
            "Epoch: 6 Validation set accuracy: 0.9603999853134155\n",
            "Epoch: 7 Validation set accuracy: 0.9603999853134155\n",
            "Epoch: 8 Validation set accuracy: 0.9617999792098999\n",
            "Epoch: 9 Validation set accuracy: 0.9634000062942505\n",
            "Epoch: 10 Validation set accuracy: 0.9649999737739563\n",
            "Epoch: 11 Validation set accuracy: 0.9646000266075134\n",
            "Epoch: 12 Validation set accuracy: 0.9664000272750854\n",
            "Epoch: 13 Validation set accuracy: 0.9661999940872192\n",
            "Epoch: 14 Validation set accuracy: 0.9667999744415283\n",
            "Epoch: 15 Validation set accuracy: 0.9688000082969666\n",
            "Epoch: 16 Validation set accuracy: 0.9675999879837036\n",
            "Epoch: 17 Validation set accuracy: 0.9706000089645386\n",
            "Epoch: 18 Validation set accuracy: 0.9703999757766724\n",
            "Epoch: 19 Validation set accuracy: 0.9702000021934509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81TV-KeqS3dx",
        "colab_type": "text"
      },
      "source": [
        "### Reusing Models From another Frameworks\n",
        "\n",
        "Below is an example to illustrate how to import models from other frameworks into TensorFlow. This code sets the kernel and bias of a hidden layer at the start of the TensorFlow session using the initializer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH543xCWTFtS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4418f17-cc92-4e4e-b095-653648b1345a"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]]\n",
        "original_b = [7., 8., 9.]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, name='hidden')\n",
        "\n",
        "graph = tf.get_default_graph()\n",
        "assign_kernel = graph.get_operation_by_name('hidden/kernel/Assign')\n",
        "assign_bias = graph.get_operation_by_name('hidden/bias/Assign')\n",
        "\n",
        "init_kernel = assign_kernel.inputs[1]\n",
        "init_bias = assign_bias.inputs[1]\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init, feed_dict={\n",
        "    init_kernel: original_w,\n",
        "    init_bias: original_b,\n",
        "  })\n",
        "  print(hidden.eval(feed_dict={X: [[10., 11.]]}))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHrxoDnfXgYF",
        "colab_type": "text"
      },
      "source": [
        "Another way is to make dedicated nodes for assigning the hidden layer's kernel and bias, then set them at any point using placeholders. This is more verbose but allows more control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzUkDhs7YGEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc45741f-ce09-400e-f2b3-152d7b85d27a"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]]\n",
        "original_b = [7., 8., 9.]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, name='hidden')\n",
        "\n",
        "with tf.variable_scope('', default_name='', reuse=True):\n",
        "  hidden_weights = tf.get_variable('hidden/kernel')\n",
        "  hidden_bias = tf.get_variable('hidden/bias')\n",
        "  \n",
        "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden))\n",
        "original_bias = tf.placeholder(tf.float32, shape=(n_hidden))\n",
        "\n",
        "assign_hidden_weights = tf.assign(hidden_weights, original_weights)\n",
        "assign_hidden_bias = tf.assign(hidden_bias, original_bias)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  sess.run(assign_hidden_weights, feed_dict={original_weights: original_w})\n",
        "  sess.run(assign_hidden_bias, feed_dict={original_bias: original_b})\n",
        "  print(hidden.eval(feed_dict={X: [[10., 11.]]}))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}