{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainingDeepNeuralNets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axcfgr1z1naq",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 11: Training Deep Neural Nets\n",
        "\n",
        "In the previous chapter, we trained a neural network with 2 hidden layers. More complex problems require networks with more hidden layers with hundreds of neurons per layer. Training these can lead to several problems:\n",
        "\n",
        "- The _vanishing gradients_ and _exploding gradients_ problem makes lower levels hard to train.\n",
        "- Training a large network can be very slow.\n",
        "- A model with millions of parameters risks overfitting the training data.\n",
        "\n",
        "Below we will discuss methods for solving all of these problems.\n",
        "\n",
        "## Vanishing/Exploding Gradients Problem\n",
        "\n",
        "While training a neural network with backpropagation, the algorithm finds the components of the error contributed by each layer to compute the error gradient.  Gradients can often get smaller and smaller as the algorithm progresses, resulting in the gradient contribution from the lower layers approaching zero. This is known as the _vanishing gradient_ problem. Alternatively, the gradient can also can grow bigger and bigger which can cause the algorithm to diverge. This is called the _exploding gradient problem_.\n",
        "\n",
        "Around 2010, a paper titled [\"Understanding the Difficulty of Training Deep Feedforward Neural Networks\"](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) found some reasons for this. The sigmoid activation function as well as the random initialization of the weight matrices' elements using a normal distribution with a mean of 0 and a standard deviation of 1. The paper showed the variance of the outputs was much larger than the variance of the inputs. Going forward in the network, the variance kept getting larger and it results in the activation saturating near the horizontal asymptotes, which causes the gradient to vanish.\n",
        "\n",
        "### Xavier and He Initialization\n",
        "\n",
        "The authors of the paper found that one way to prevent the vanishing/exploding gradient problem is to ensure that the variance of the input and output of each layer is the same. One way to do this is to initialize the weights matrix using a normal distribution with a mean of 0 and a standard deviation given by\n",
        "\n",
        "$$ \\sigma = \\sqrt{\\frac{2}{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "or a uniform distribution centered at 0 with a radius, $r$, given by\n",
        "\n",
        "$$ r = \\sqrt{\\frac{6}{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "where $n_\\text{ inputs}$ and $n_\\text{ outputs}$ is the number of input  or output connections in that particular layer. This is often known as _Xavier initialization_ after the author's first name, or sometimes _Glorot initialization_.\n",
        "\n",
        "For the ReLU activation function, we use a normal distribution with a standard deviation given by\n",
        "\n",
        "$$ \\sigma = \\frac{2}{\\sqrt{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "or a uniform distribution with a radius given by\n",
        "\n",
        "$$ r = \\sqrt{\\frac{24}{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "which is known as _He initialization_. Below is an example of creating a layer of a neural network which uses _He initialization_. By default, `tf.layers.dense()` uses Xavier initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW0y5GYZFS11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden = 100\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.relu,\n",
        "                          kernel_initializer=he_init, name='hidden1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUVypontGCD4",
        "colab_type": "text"
      },
      "source": [
        "### Nonsaturating Activation Functions\n",
        "\n",
        "One of the causes of the vanishing/exploding gradient problem discussed in the paper is the sigmoid activation function. The ReLU activation function performs much better, but it has a different problem. If some neurons output negative values, after the application of the activation function, their output will be stuck at 0. Since the gradient is also 0, the neuron remains \"dead.\"\n",
        "\n",
        "One solution to this problem is to use a \"leaky\" ReLU function, given by\n",
        "\n",
        "$$ \\text{LeakyReLU}(z) = \\max(\\alpha z, z) $$\n",
        "\n",
        "where $\\alpha$ is the slope ofthe ReLU function when the value of $z$ is less than 0. Researchers have found that this activation function performs better than the \"hard\" ReLU function. You can even have $\\alpha$ be a parameter that the model learns during training. This prevents neurons from completely dying.\n",
        "\n",
        "Another activation function that performs better than leaky ReLU that was proposed in this [paper](https://arxiv.org/pdf/1511.07289v5.pdf) by Djork-Arn√© Clevert called the _exponential linear unit_ (ELU) given by\n",
        "\n",
        "$$ \\text{ELU}_\\alpha(z) = \\left\\{ \\begin{matrix}\n",
        "\\alpha\\,(\\exp(z) - 1) && \\text{if}\\;z < 0 \\\\\n",
        "z && \\text{if}\\; z \\geq 0\n",
        "\\end{matrix} \\right. $$\n",
        "\n",
        "It has the following differences from the ReLU function:\n",
        "\n",
        "- It takes negative values when $z < 0$ . which allows the unit to have an average output closer to 0. This helps alleviate the vanishing radient problem. You can tweak the hyperparameter, $\\alpha$, sets the negative number that ELU approaches.\n",
        "\n",
        "- It has a nonzero gradient when $z < 0$, preventing the dying units issue.\n",
        "\n",
        "- The function is differentiable everywhere, which helps the speed of Gradient Descent.\n",
        "\n",
        "The disadvantage of ELU is that it takes longer to compute than ReLU. The extra time is compensated for the fact that it helps Gradient Descent converge fasted, but it does cause the model to make predictions more slowly.\n",
        "\n",
        "TensorFlow offeres an implementation of ELU which is used in the code example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m1ATbFgN1Th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden = 100\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, name='hidden1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx9wtyEcN-1W",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow does not have an implementation of leaky ReLU, but it is easy to define ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7eAj0MgOJTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "  return tf.maximum(alpha * z, z)\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden = 100\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden, activation=leaky_relu, name='hidden1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3lOO3R7OYpc",
        "colab_type": "text"
      },
      "source": [
        "### Batch Normalization\n",
        "\n",
        "In this [paper](https://arxiv.org/pdf/1502.03167v3.pdf) Sergey Ioffe and Christian Szegedy proposed a technique called _Batch Normalization_ (BN) to address both the vanishing/exploding gradient problem and the problem that the distribution of each layer's inputs change when the parameters of the previous layers change (i.e. the _Internal Covariate Shift_ problem).\n",
        "\n",
        "The technique adds an operation to the model just before applying the activation function of each layer. It zero-centers and normalizes the inputs, then it scales and shifts the result using two new parameters per layer. This lets the model learn the optimal mean and shift for each layer.\n",
        "\n",
        "The algorithm starts by first computing the empirical mean for the current mini-batch, $B$, given by\n",
        "\n",
        "$$ \\mu_B = \\frac{1}{m_B} \\sum\\limits_{i\\,=\\,1}^{m_B} \\mathbf{x}^{(i)} $$\n",
        "\n",
        "Next, we find the empirical standard deviation, given by\n",
        "\n",
        "$$ \\sigma_B^{\\;\\;2} = \\frac{1}{m_B} \\sum\\limits_{i\\,=\\,1}^{m_B} \\left( \\mathbf{x}^{(i)} - \\mu_B \\right)^2 $$\n",
        "\n",
        "Then we zero-center and normalize the inputs in the mini-batch\n",
        "\n",
        "$$ \\hat{\\mathbf{x}}^{(i)} = \\frac{\\mathbf{x}^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^{;\\;2} + \\epsilon}} $$\n",
        "\n",
        "where $\\epsilon$ is a small number, typically $10^{-5}$, called the _smoothing term_ to avoid division by zero. Finally it computes the output given by\n",
        "\n",
        "$$ \\mathbf{z}^{(i)} = \\gamma\\,\\hat{\\mathbf{x}}^{(i)} + \\beta $$\n",
        "\n",
        "where $\\gamma$ is the scaling parameter and $\\beta$ is the shift parameter which are learned during training.\n",
        "\n",
        "When the model makes predictions, it uses the empirical mean and standard deviation of the entire training set. In the end, the model ends up learning 4 parameters: the mean of the training set, $\\mu$; the standard deviation of the training set, $\\sigma$; the scaling parameter, $\\gamma$; and the shift parameter, $\\beta$.\n",
        "\n",
        "Adding Batch Normalization to a deep neural network improves the performance of the model, lets you skip normalizing the data before training the data, and helps the model converge to the optimal parameters in fewer training iterations. However, using Batch Normalization causes the model to make predictions slower since it adds another computational step for making predictions.\n",
        "\n",
        "#### Implementing Batch Normalization with TensorFlow\n",
        "\n",
        "TensorFlow provides a `tf.nn.batch_normalization()` function which normalizes and centers the data, but you must compute the mean and standard deviation yourself.You also have to handle the creation of the scaling and offfset parameters. TensorFlow also includes a `tf.layers.batch_normalization()` function which handles all of batch normalization for you. Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jir_-CpyJfvo",
        "colab_type": "code",
        "outputId": "04219c86-11bf-44df-cd07-d0bcfdb41ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2 # MNIST dataset.\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "\n",
        "# Indicates if the batch normalization should be using the mini-batch's mean\n",
        "# or the mean of the entire training set (same with standard deviation).\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logts = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
        "                                      momentum=0.9)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-674256f4b25b>:15: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbzsx0D4M1Kq",
        "colab_type": "text"
      },
      "source": [
        "The BN algorithm uses _exponential decay_ to compute a running average, which is why it requires the _momentum_ parameter. Given a new value, $v$, it updates the running average $\\hat{v}$ given by\n",
        "\n",
        "$$ \\hat{v} \\leftarrow \\hat{v} \\times \\text{momentum} + v \\times (1 - \\text{momentum}) $$\n",
        "\n",
        "Momentum values should be typically close to 1, e.g. 0.9, 0.99, or 0.999.\n",
        "\n",
        "Below is an example of using _partial application_ using the `functools` library in order to make the code less repetitive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3ESzVlLNmbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "\n",
        "# Indicates if the batch normalization should be using the mini-batch's mean\n",
        "# or the mean of the entire training set (same with standard deviation).\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saqpC1nRRtEb",
        "colab_type": "code",
        "outputId": "46e61c88-4724-4b9d-cf6f-55af6dfa3b6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Setting up the rest of the graph for training.\n",
        "\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxI0G_EvSbV3",
        "colab_type": "code",
        "outputId": "44637dc4-5f90-4349-f596-a179d160a61e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Downloading the MNIST dataset.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
        "\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "  rnd_idx = np.random.permutation(len(X))\n",
        "  n_batches = len(X) // batch_size\n",
        "  for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "    X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "    yield X_batch, y_batch"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szFxyIuFTSXD",
        "colab_type": "code",
        "outputId": "607275b3-84f5-4a08-9b5a-02a873a72b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Training the neural network using Batch Normalization.\n",
        "# In just 20 training iterations it achieves 97% accuracy on the\n",
        "# validation set.\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "# These extra ops are for training the scaling and offset parameters\n",
        "# in batch normalization.\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run([training_op, extra_update_ops],\n",
        "               feed_dict={training: True, X: X_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Validation Accuracy: 0.8848000168800354\n",
            "Epoch: 1 Validation Accuracy: 0.9088000059127808\n",
            "Epoch: 2 Validation Accuracy: 0.9233999848365784\n",
            "Epoch: 3 Validation Accuracy: 0.9308000206947327\n",
            "Epoch: 4 Validation Accuracy: 0.9399999976158142\n",
            "Epoch: 5 Validation Accuracy: 0.9434000253677368\n",
            "Epoch: 6 Validation Accuracy: 0.9491999745368958\n",
            "Epoch: 7 Validation Accuracy: 0.9521999955177307\n",
            "Epoch: 8 Validation Accuracy: 0.9552000164985657\n",
            "Epoch: 9 Validation Accuracy: 0.9592000246047974\n",
            "Epoch: 10 Validation Accuracy: 0.9620000123977661\n",
            "Epoch: 11 Validation Accuracy: 0.9642000198364258\n",
            "Epoch: 12 Validation Accuracy: 0.9649999737739563\n",
            "Epoch: 13 Validation Accuracy: 0.9674000144004822\n",
            "Epoch: 14 Validation Accuracy: 0.9682000279426575\n",
            "Epoch: 15 Validation Accuracy: 0.9710000157356262\n",
            "Epoch: 16 Validation Accuracy: 0.9702000021934509\n",
            "Epoch: 17 Validation Accuracy: 0.9715999960899353\n",
            "Epoch: 18 Validation Accuracy: 0.9714000225067139\n",
            "Epoch: 19 Validation Accuracy: 0.972599983215332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K51njD5WZmm",
        "colab_type": "text"
      },
      "source": [
        "An alternate syntax to training a model this way is to define the `training_op` the following way:\n",
        "\n",
        "```python\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(extra_update_ops):\n",
        "        training_op = optimizer.minimize(loss)\n",
        "```\n",
        "\n",
        "this lets you train the model using the more simple syntax:\n",
        "\n",
        "```python\n",
        "sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "```\n",
        "\n",
        "### Gradient Clipping\n",
        "\n",
        "One way to solve the exploding gradients problem is to clip the gradients' values to a defined range. This technique is called [_Gradient Clipping_](http://proceedings.mlr.press/v28/pascanu13.pdf). Though in general people prefer Batch Normalization. Below is an example of Gradient Clipping using TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-JHcp_LXL3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example of gradient clipping.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "hidden1_act = tf.nn.elu(hidden1)\n",
        "\n",
        "hidden2 = tf.layers.dense(hidden1_act, n_hidden2, name='hidden2')\n",
        "hidden2_act = tf.nn.elu(hidden2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(hidden2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "# Gradient clipping is here.\n",
        "threshold = 1.0\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "grads_and_vars = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
        "              for grad, var in grads_and_vars]\n",
        "training_op = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Z-m3dzZ5Sm",
        "colab_type": "text"
      },
      "source": [
        "## Reusing Pretrained Layers\n",
        "\n",
        "Instead of training a large DNN from scratch, it is generally better to reuse an existing neural network used for a similar task, then reuse the lower layers of this network. This technique is called _transfer learning_.\n",
        "\n",
        "### Reusing a TensorFlow model\n",
        "\n",
        "Below is an example of saving and restoring a TensorFlow model using the `tr.train.import_meta_graph()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mswAchPZdh6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the graph and saving it.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# New code here!\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  saver.save(sess, './my_model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3qPPnLIfYii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting nodes in the previous graph for reuse.\n",
        "\n",
        "X = tf.get_default_graph().get_tensor_by_name('X:0')\n",
        "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
        "accuracy = tf.get_default_graph().get_tensor_by_name('accuracy:0')\n",
        "training_op = tf.get_default_graph().get_operation_by_name('GradientDescent')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJTAb84Bf2Pp",
        "colab_type": "code",
        "outputId": "e0ab258f-55fd-48e3-c227-f86a08a04eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Listing the operations in the predefined graph, truncated for readability.\n",
        "\n",
        "for op in tf.get_default_graph().get_operations()[:20]:\n",
        "  print(op.name)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X\n",
            "y\n",
            "training/input\n",
            "training\n",
            "hidden1/kernel/Initializer/random_uniform/shape\n",
            "hidden1/kernel/Initializer/random_uniform/min\n",
            "hidden1/kernel/Initializer/random_uniform/max\n",
            "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden1/kernel/Initializer/random_uniform/sub\n",
            "hidden1/kernel/Initializer/random_uniform/mul\n",
            "hidden1/kernel/Initializer/random_uniform\n",
            "hidden1/kernel\n",
            "hidden1/kernel/Assign\n",
            "hidden1/kernel/read\n",
            "hidden1/bias/Initializer/zeros\n",
            "hidden1/bias\n",
            "hidden1/bias/Assign\n",
            "hidden1/bias/read\n",
            "hidden1/MatMul\n",
            "hidden1/BiasAdd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRkkdLD-jU1v",
        "colab_type": "text"
      },
      "source": [
        "Below is an example of creating a collection of important operations. This is often helpful if the graph is large and you only want to reuse certain operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bc_2l9ggN-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the original graph.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# New code here!\n",
        "for op in (X, y, accuracy, training_op):\n",
        "  tf.add_to_collection('my_important_ops', op)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  saver.save(sess, './my_model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGuXXeZ9jmrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Restoring the graph and getting the operations from the collection.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "saver = tf.train.import_meta_graph('./my_model.ckpt.meta')\n",
        "\n",
        "X, y, accuracy, training_op = tf.get_collection('my_important_ops')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjw5pBVmqC5Q",
        "colab_type": "text"
      },
      "source": [
        "You can also define a restore Saver which will only restore specified variables. This is useful if you only want to restore the lower layers of a neural network. Below is an example of restoring only the lower layers of a neural network using a saver which only restores specified variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kqrygEnlNDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a graph with 5 hidden layers. First implementing gradient clipping.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None,), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(None))\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  layer = X\n",
        "  for i, n_hidden in enumerate((300, 100, 50, 20)):\n",
        "    hidden = tf.layers.dense(layer, n_hidden1,\n",
        "                             name='hidden{}'.format(i+1))\n",
        "    layer = tf.nn.relu(hidden, name='relu{}'.format(i+1))\n",
        "  logits = tf.layers.dense(layer, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                             labels=y)\n",
        "  loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iUbtgyCqapj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the savers and running the training.\n",
        "\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]') # Regex string\n",
        "reuse_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwbNBPC4CxFZ",
        "colab_type": "code",
        "outputId": "8bfbf0d9-4f77-4ffe-baba-05b6c39ff633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Training the model one time to train the lower layers of the neural network.\n",
        "\n",
        "import os\n",
        "\n",
        "model_path = './my_model.ckpt'\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, model_path)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Validation set accuracy: 0.8141999840736389\n",
            "Epoch: 1 Validation set accuracy: 0.8755999803543091\n",
            "Epoch: 2 Validation set accuracy: 0.8989999890327454\n",
            "Epoch: 3 Validation set accuracy: 0.9093999862670898\n",
            "Epoch: 4 Validation set accuracy: 0.9172000288963318\n",
            "Epoch: 5 Validation set accuracy: 0.9236000180244446\n",
            "Epoch: 6 Validation set accuracy: 0.9296000003814697\n",
            "Epoch: 7 Validation set accuracy: 0.9348000288009644\n",
            "Epoch: 8 Validation set accuracy: 0.9366000294685364\n",
            "Epoch: 9 Validation set accuracy: 0.9405999779701233\n",
            "Epoch: 10 Validation set accuracy: 0.9444000124931335\n",
            "Epoch: 11 Validation set accuracy: 0.946399986743927\n",
            "Epoch: 12 Validation set accuracy: 0.9491999745368958\n",
            "Epoch: 13 Validation set accuracy: 0.9488000273704529\n",
            "Epoch: 14 Validation set accuracy: 0.9535999894142151\n",
            "Epoch: 15 Validation set accuracy: 0.9552000164985657\n",
            "Epoch: 16 Validation set accuracy: 0.9581999778747559\n",
            "Epoch: 17 Validation set accuracy: 0.9588000178337097\n",
            "Epoch: 18 Validation set accuracy: 0.9593999981880188\n",
            "Epoch: 19 Validation set accuracy: 0.9610000252723694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbbnS6dcLPys",
        "colab_type": "code",
        "outputId": "a92f67b9-790d-4bf0-eea3-01555a5c6602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# Training the model again, this time we restore the lower layers\n",
        "\n",
        "new_model_path = './my_model_new.ckpt'\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, new_model_path)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./my_model.ckpt\n",
            "Epoch: 0 Validation set accuracy: 0.9404000043869019\n",
            "Epoch: 1 Validation set accuracy: 0.946399986743927\n",
            "Epoch: 2 Validation set accuracy: 0.9488000273704529\n",
            "Epoch: 3 Validation set accuracy: 0.9534000158309937\n",
            "Epoch: 4 Validation set accuracy: 0.9575999975204468\n",
            "Epoch: 5 Validation set accuracy: 0.9570000171661377\n",
            "Epoch: 6 Validation set accuracy: 0.9592000246047974\n",
            "Epoch: 7 Validation set accuracy: 0.9607999920845032\n",
            "Epoch: 8 Validation set accuracy: 0.9620000123977661\n",
            "Epoch: 9 Validation set accuracy: 0.9643999934196472\n",
            "Epoch: 10 Validation set accuracy: 0.9638000130653381\n",
            "Epoch: 11 Validation set accuracy: 0.9660000205039978\n",
            "Epoch: 12 Validation set accuracy: 0.9664000272750854\n",
            "Epoch: 13 Validation set accuracy: 0.9664000272750854\n",
            "Epoch: 14 Validation set accuracy: 0.967199981212616\n",
            "Epoch: 15 Validation set accuracy: 0.9696000218391418\n",
            "Epoch: 16 Validation set accuracy: 0.9693999886512756\n",
            "Epoch: 17 Validation set accuracy: 0.9685999751091003\n",
            "Epoch: 18 Validation set accuracy: 0.9693999886512756\n",
            "Epoch: 19 Validation set accuracy: 0.9715999960899353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81TV-KeqS3dx",
        "colab_type": "text"
      },
      "source": [
        "### Reusing Models From another Frameworks\n",
        "\n",
        "Below is an example to illustrate how to import models from other frameworks into TensorFlow. This code sets the kernel and bias of a hidden layer at the start of the TensorFlow session using the initializer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH543xCWTFtS",
        "colab_type": "code",
        "outputId": "66d6e00a-29a5-4d7a-ef1b-7ecfb94f9f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]]\n",
        "original_b = [7., 8., 9.]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, name='hidden')\n",
        "\n",
        "graph = tf.get_default_graph()\n",
        "assign_kernel = graph.get_operation_by_name('hidden/kernel/Assign')\n",
        "assign_bias = graph.get_operation_by_name('hidden/bias/Assign')\n",
        "\n",
        "init_kernel = assign_kernel.inputs[1]\n",
        "init_bias = assign_bias.inputs[1]\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init, feed_dict={\n",
        "    init_kernel: original_w,\n",
        "    init_bias: original_b,\n",
        "  })\n",
        "  print(hidden.eval(feed_dict={X: [[10., 11.]]}))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHrxoDnfXgYF",
        "colab_type": "text"
      },
      "source": [
        "Another way is to make dedicated nodes for assigning the hidden layer's kernel and bias, then set them at any point using placeholders. This is more verbose but allows more control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzUkDhs7YGEX",
        "colab_type": "code",
        "outputId": "5deb8569-fb9e-4481-dbc3-808ef19f35bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]]\n",
        "original_b = [7., 8., 9.]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, name='hidden')\n",
        "\n",
        "with tf.variable_scope('', default_name='', reuse=True):\n",
        "  hidden_weights = tf.get_variable('hidden/kernel')\n",
        "  hidden_bias = tf.get_variable('hidden/bias')\n",
        "  \n",
        "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden))\n",
        "original_bias = tf.placeholder(tf.float32, shape=(n_hidden))\n",
        "\n",
        "assign_hidden_weights = tf.assign(hidden_weights, original_weights)\n",
        "assign_hidden_bias = tf.assign(hidden_bias, original_bias)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  sess.run(assign_hidden_weights, feed_dict={original_weights: original_w})\n",
        "  sess.run(assign_hidden_bias, feed_dict={original_bias: original_b})\n",
        "  print(hidden.eval(feed_dict={X: [[10., 11.]]}))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCgFlOhUtNZO",
        "colab_type": "text"
      },
      "source": [
        "### Freezing Lower Layers\n",
        "\n",
        "Since it is likely that the lower layers of the DNN have learned to detect the lower level patterns in the training set, you can reuse the layers as they are by \"freezing\" their weights. As a result, the higher level layers will be easier to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tfRdZ2IvJ80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a new graph which will use the lower level\n",
        "# layers from the section on using pre-trained layers.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None,), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(None))\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  layer = X\n",
        "  for i, n_hidden in enumerate((300, 100, 50, 20)):\n",
        "    hidden = tf.layers.dense(layer, n_hidden1,\n",
        "                             name='hidden{}'.format(i+1))\n",
        "    layer = tf.nn.relu(hidden, name='relu{}'.format(i+1))\n",
        "  logits = tf.layers.dense(layer, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                             labels=y)\n",
        "  loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "# New code here!\n",
        "with tf.name_scope('train'):\n",
        "  training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                    scope='hidden[34]|outputs')\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "  training_op = optimizer.minimize(loss, var_list=training_vars)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]') # Regex string\n",
        "reuse_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGxpZs2ev3XV",
        "colab_type": "code",
        "outputId": "a47ec02e-21ff-4cc4-f268-abca6ab3bd1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Training the model with the frozen layers.\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, new_model_path)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model.ckpt\n",
            "Epoch: 0 Validation set accuracy: 0.9422000050544739\n",
            "Epoch: 1 Validation set accuracy: 0.9470000267028809\n",
            "Epoch: 2 Validation set accuracy: 0.9502000212669373\n",
            "Epoch: 3 Validation set accuracy: 0.9509999752044678\n",
            "Epoch: 4 Validation set accuracy: 0.953000009059906\n",
            "Epoch: 5 Validation set accuracy: 0.9534000158309937\n",
            "Epoch: 6 Validation set accuracy: 0.9557999968528748\n",
            "Epoch: 7 Validation set accuracy: 0.954200029373169\n",
            "Epoch: 8 Validation set accuracy: 0.9553999900817871\n",
            "Epoch: 9 Validation set accuracy: 0.9570000171661377\n",
            "Epoch: 10 Validation set accuracy: 0.9563999772071838\n",
            "Epoch: 11 Validation set accuracy: 0.9584000110626221\n",
            "Epoch: 12 Validation set accuracy: 0.9592000246047974\n",
            "Epoch: 13 Validation set accuracy: 0.9611999988555908\n",
            "Epoch: 14 Validation set accuracy: 0.9603999853134155\n",
            "Epoch: 15 Validation set accuracy: 0.9593999981880188\n",
            "Epoch: 16 Validation set accuracy: 0.9603999853134155\n",
            "Epoch: 17 Validation set accuracy: 0.9603999853134155\n",
            "Epoch: 18 Validation set accuracy: 0.9613999724388123\n",
            "Epoch: 19 Validation set accuracy: 0.9595999717712402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-t_5pzMwK8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Another way to freeze the lower layers is to add a stop_gradient()\n",
        "# layer in the graph. Any layer below it will be frozen.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None,), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(None))\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  layer = X\n",
        "  for i, n_hidden in enumerate((300, 100, 50, 20)):\n",
        "    hidden = tf.layers.dense(layer, n_hidden1,\n",
        "                             name='hidden{}'.format(i+1))\n",
        "    layer = tf.nn.relu(hidden, name='relu{}'.format(i+1))\n",
        "    # New code here!\n",
        "    if i == 1:\n",
        "      hidden2 = layer\n",
        "      layer = tf.stop_gradient(layer)\n",
        "  logits = tf.layers.dense(layer, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                             labels=y)\n",
        "  loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]') # Regex string\n",
        "reuse_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiZrIkI-w3mi",
        "colab_type": "code",
        "outputId": "b6f0b0fc-6769-439d-80c3-faa2df8645eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, new_model_path)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model.ckpt\n",
            "Epoch: 0 Validation set accuracy: 0.9440000057220459\n",
            "Epoch: 1 Validation set accuracy: 0.9467999935150146\n",
            "Epoch: 2 Validation set accuracy: 0.949999988079071\n",
            "Epoch: 3 Validation set accuracy: 0.9502000212669373\n",
            "Epoch: 4 Validation set accuracy: 0.9520000219345093\n",
            "Epoch: 5 Validation set accuracy: 0.9556000232696533\n",
            "Epoch: 6 Validation set accuracy: 0.9531999826431274\n",
            "Epoch: 7 Validation set accuracy: 0.954800009727478\n",
            "Epoch: 8 Validation set accuracy: 0.9549999833106995\n",
            "Epoch: 9 Validation set accuracy: 0.95660001039505\n",
            "Epoch: 10 Validation set accuracy: 0.9557999968528748\n",
            "Epoch: 11 Validation set accuracy: 0.95660001039505\n",
            "Epoch: 12 Validation set accuracy: 0.9559999704360962\n",
            "Epoch: 13 Validation set accuracy: 0.9589999914169312\n",
            "Epoch: 14 Validation set accuracy: 0.9553999900817871\n",
            "Epoch: 15 Validation set accuracy: 0.9574000239372253\n",
            "Epoch: 16 Validation set accuracy: 0.9584000110626221\n",
            "Epoch: 17 Validation set accuracy: 0.9588000178337097\n",
            "Epoch: 18 Validation set accuracy: 0.9589999914169312\n",
            "Epoch: 19 Validation set accuracy: 0.9595999717712402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C02iirCBxHuc",
        "colab_type": "text"
      },
      "source": [
        "### Caching Frozen Layers\n",
        "\n",
        "One way to improve the speed of training when you have frozen the lower layers is to run the training set through the frozen layers at the start of training. The code below shows an example of training a model this way, reusing the TensorFlow graph defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZaxwDuAzZJ-",
        "colab_type": "code",
        "outputId": "d3afc8c2-b230-4b5c-e143-4854a873e3c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "  h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
        "  for epoch in range(n_epochs):\n",
        "    for h2_batch, y_batch in shuffle_batch(h2_cache, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={hidden2: h2_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, new_model_path)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model.ckpt\n",
            "Epoch: 0 Validation set accuracy: 0.9455999732017517\n",
            "Epoch: 1 Validation set accuracy: 0.949999988079071\n",
            "Epoch: 2 Validation set accuracy: 0.9509999752044678\n",
            "Epoch: 3 Validation set accuracy: 0.953000009059906\n",
            "Epoch: 4 Validation set accuracy: 0.9526000022888184\n",
            "Epoch: 5 Validation set accuracy: 0.9552000164985657\n",
            "Epoch: 6 Validation set accuracy: 0.9559999704360962\n",
            "Epoch: 7 Validation set accuracy: 0.9567999839782715\n",
            "Epoch: 8 Validation set accuracy: 0.9556000232696533\n",
            "Epoch: 9 Validation set accuracy: 0.9574000239372253\n",
            "Epoch: 10 Validation set accuracy: 0.9570000171661377\n",
            "Epoch: 11 Validation set accuracy: 0.9585999846458435\n",
            "Epoch: 12 Validation set accuracy: 0.9592000246047974\n",
            "Epoch: 13 Validation set accuracy: 0.9592000246047974\n",
            "Epoch: 14 Validation set accuracy: 0.9607999920845032\n",
            "Epoch: 15 Validation set accuracy: 0.9595999717712402\n",
            "Epoch: 16 Validation set accuracy: 0.9607999920845032\n",
            "Epoch: 17 Validation set accuracy: 0.9606000185012817\n",
            "Epoch: 18 Validation set accuracy: 0.9621999859809875\n",
            "Epoch: 19 Validation set accuracy: 0.9602000117301941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4WhKgIR2AmA",
        "colab_type": "text"
      },
      "source": [
        "### Tweaking, Dropping, or Replacing the Upper Layers\n",
        "\n",
        "The higher the layer is in the previously trained neural network, the less likely it will be useful in training a new network for different tasks. The output layer is generally always replaced, in many cases the old output layer may be a different shape than the output for the new layer.\n",
        "\n",
        "One way to try to determine how many layers to freeze is to try freezing all of the hidden layers first, then training the neural network again after unfreezing one or two of the top layers and seeing if performance improves. The more training data, the more layers you can unfreeze.\n",
        "\n",
        "If you still cannot get good performance with little training data, you can try dropping the top hidden layers and freezing the lower ones. You can keep trying until you find the right number of layers to reuse. If you have a lot of training data, you can replace the top layers instead of dropping them or add more layers.\n",
        "\n",
        "### Model Zoos\n",
        "\n",
        "A _model zoo_ is a collection of machine learning models that other people have trained for different machine learning tasks. TensorFlow has its own [model zoo](https://github.com/tensorflow/models). Another popular model zoo is the [Caffe Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo). Saumitro Dasgupta wrote a [converter](https://github.com/ethereon/caffe-tensorflow) to convert Caffe models to TensorFlow.\n",
        "\n",
        "### Unsupervised Training\n",
        "\n",
        "If you do not have a large labeled training set and there is not a previously trained model for a similar task, but you do have a large unlabled training set, you can use an _unsupervised pretraining_ algorithms such as _Restricted Boltzmann Machines_ (RBMs) or autoencoders to train successlive DNN layers to find low level features in the training set. Afterwards you can tune the model using supervised learning and backpropagation.\n",
        "\n",
        "### Pretraining an Auxilary Task\n",
        "\n",
        "One way to train a DNN if you have limited labeled training data is to train a neural network for a similar task then reuse the lower layers to train a new DNN for the actual task.\n",
        "\n",
        "Another strategy is to take unlabeled training data and take some data and modify it. You label the unmodified data as \"good\" and the modified data as \"bad\" so that you can train a DNN classifier using a supervised algorithm to get lower layers which recognize lower level features for the actual task.\n",
        "\n",
        "## Faster Optimizers\n",
        "\n",
        "In this section we will examine optimizers which are faster than plain Gradient Descent which can help speed up training DNNs.\n",
        "\n",
        "### Momentum Optimizers\n",
        "\n",
        "Recall that Gradient Descent updates the weight vector, $\\theta$ by subtracting the weight of the gradient of the cost function, $\\nabla_\\theta J(\\theta)$ multiplied by the learning rate, $\\eta$, i.e.\n",
        "\n",
        "$$ \\theta \\leftarrow \\theta - \\eta\\nabla_\\theta J(\\theta) $$\n",
        "\n",
        "[Momentum optimization](https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods), proposed by Boris Polyak in 1964 subtracts the local gradient from a momentum vector, $\\mathbf{m}$, multiplied by the learning rate $\\eta$ and it updates the weights by adding the momentum vector to the weight vector, $\\theta$. To prevent the momentum from growing too large, the algorithm introduces a hyperparameter, $\\beta$, called the _momentum_, which is between 0 and 1 (typically 0.9). The algorithm can be written in two stages:\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "1. && \\mathbf{m} \\leftarrow \\beta\\,\\mathbf{m} - \\eta\\nabla_\\theta J(\\theta) \\\\\n",
        "2. && \\theta \\leftarrow \\theta + \\mathbf{m}\n",
        "\\end{matrix}  $$\n",
        "\n",
        "It follows that if the gradient remains constant, the terminal velocity (the maximum size of the weight updates) is given by the learning rate, $\\eta$, multiplied by $\\frac{1}{1\\,-\\,\\beta}$. If $\\beta = 0.9$ then Momentum optimization converges 10 times as quickly as Gradient Descent. Due to the larger steps, Momentum optimization can escape local optima much more quickly than Gradient Descent. Below is an example of implementing Momentum optimization in TensorFlow:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0OFkaNcLfMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rjSAkPILoF4",
        "colab_type": "text"
      },
      "source": [
        "The main drawback of Momentum optimization is it adds another hyperparameter to tune, but generally 0.9 works in practice.\n",
        "\n",
        "### Nesterov Accelerated Gradient\n",
        "\n",
        "One improvement to Momentum optimization proposed by Yuli Nesterov in 1983 is called [Nesterov Momentum Optimization](https://scholar.google.com/citations?view_op=view_citation&citation_for_view=DJ8Ep8YAAAAJ:hkOj_22Ku90C) or _Nesterov Accelerated Gradient_ (NAG). The algorithm works in the following steps:\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "1. && \\mathbf{m} \\leftarrow \\beta\\,\\mathbf{m} - \\eta\\nabla_\\theta J(\\theta + \\beta\\,\\mathbf{m}) \\\\\n",
        "2. && \\theta \\leftarrow \\theta + \\mathbf{m}\n",
        "\\end{matrix} $$\n",
        "\n",
        "The only difference between NAG and Momentum optimization is that it computes the gradient of the cost function at $\\theta + \\beta\\,\\mathbf{m}$ instead of at the current value of $\\theta$. This improves the algorithm since the momentum vector is usually pointing in the direction of the optimal value. Below is an example of NAG using TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1VNy0SfQAth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9,\n",
        "                                       use_nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh7oCaBhSbpE",
        "colab_type": "text"
      },
      "source": [
        "### AdaGrad Optimizer\n",
        "\n",
        "The [AdaGrad algorithm](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) is an algorithm designed to decay the learning rate, doing so faster for steeper gradients, in order to converge more directly towards the global optimum. This is called an _adaptive learning rate_. The algorithm works in two stages:\n",
        "\n",
        "The first stage computes a vector, $\\mathbf{s}$, given by\n",
        "\n",
        "$$ \\mathbf{s} \\leftarrow \\mathbf{s} + \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta) $$\n",
        "\n",
        "where $\\otimes$ denotes component-wise multiplication. This is a vectorized form of the following operation:\n",
        "\n",
        "$$ s_i \\leftarrow s_i + \\left( \\frac{\\partial J(\\theta)}{\\partial \\theta_i} \\right)^2 $$\n",
        "\n",
        "where $s_i$ denotes a component of $\\mathbf{s}$.The vector $\\mathbf{s}$ accumulates the squares of each component of the gradient, and it grows larger when the gradient is steeper.\n",
        "\n",
        "The second step is similar to Gradient Descent but with a modification. It is given by\n",
        "\n",
        "$$ \\theta \\leftarrow \\theta - \\eta\\nabla_\\theta J(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$\n",
        "\n",
        "where $\\oslash$ denotes component-wise division and $\\epsilon$ is a smoothing term to avoid division by zero (it is typically 10<sup>-10</sup>). This operation is a vectorized form of the following operation:\n",
        "\n",
        "$$ \\theta_i \\leftarrow \\theta_i - \\eta \\left( \\frac{\\partial J(\\theta)}{\\partial \\theta_i} \\right) \\left( s_i + \\epsilon \\right)^{-1/2} $$\n",
        "\n",
        "where $\\theta_i$ is each component of the vector $\\theta$.\n",
        "\n",
        "AdaGrad performs well for simple quadratic problems, but often stops too early when training DNNs because the learning rate degrades to zero. Below is an example of an AdaGrad optimizer in TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi8CR9TDWrSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdagradOptimizer(learning_rate=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmQg86NtYSZD",
        "colab_type": "text"
      },
      "source": [
        "### RMSProp\n",
        "\n",
        "Since AdaGrad can decay the learning rate too quickly, [RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) slows the rate of decay of the learning rate by accumulating only the most recent iterations using exponential decay. The algorithm works as follows:\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "1. && \\mathbf{s} \\leftarrow \\beta\\,\\mathbf{s} + (1 - \\beta\\, ) \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta) \\\\\n",
        "2. && \\theta \\leftarrow \\theta - \\eta\\nabla_\\theta J(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon}\n",
        "\\end{matrix} $$\n",
        "\n",
        "where $\\beta$ is the decay rate and is typically set to 0.9. This value typically works in practice so you do not have to tune it. Except for simple problems, RMSProp typically performs better than AdaGrad. Below is an example of RMSProp with TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THExdj-mbBgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.1, momentum=0.9,\n",
        "                                      decay=0.9, epsilon=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0M0N3OBdJoS",
        "colab_type": "text"
      },
      "source": [
        "### Adam Optimizer\n",
        "\n",
        "[Adam](https://arxiv.org/pdf/1412.6980v8.pdf) which stands for _adaptive moment estimation_ combines Momentum optimizers and RMSProp, keeping track of an exponentially decaying average of past gradients and past squared gradients. The algorithm works as follows:\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "1. && \\mathbf{m} \\leftarrow \\beta_1\\mathbf{m} - (1 - \\beta_1) \\nabla_\\theta J(\\theta) \\\\\n",
        "2. && \\mathbf{s} \\leftarrow \\beta_2\\mathbf{s} + (1 - \\beta_2) \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta) \\\\\n",
        "3. && \\mathbf{m} \\leftarrow (1 - \\beta_1)^{-\\,t}\\,\\mathbf{m} \\\\\n",
        "4. && \\mathbf{s} \\leftarrow (1 - \\beta_2)^{-\\,t} \\,\\mathbf{s} \\\\\n",
        "5. && \\theta \\leftarrow \\theta + \\eta \\, \\mathbf{m} \\oslash \\sqrt{\\mathbf{s} + \\epsilon}\n",
        "\\end{matrix}$$\n",
        "\n",
        "where $t$ is the training iteration number, $\\beta_1$ is the momentum decay rate, and $\\beta_2$ is the scaling decay rate. Steps 1, 2, and 5 resemble both Momentum optimization and RMSProp. Steps 3 and 4 account for the fact that $\\mathbf{m}$ and $\\mathbf{s}$ are initialized to zero vectors, so these steps prevent the algorithm from being biased towards zero at the beginning.\n",
        "\n",
        "The momentum decay rate, $\\beta_1$, is typically initialized to 0.9. The scaling decay rate, $\\beta_2$ is initialized to 0.99. These values perform well in practice so its rare you have to tune them. The smoothing parameter, $\\epsilon$, is typically initialized to 10<sup>-10</sup>. Since the model's learning rate is adaptive, it is not as necessary to tune the learning rate, $\\eta$.\n",
        "\n",
        "Below is an example of an Adam optimizer in TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmFQP723jBDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1ZduGrOKgwo",
        "colab_type": "text"
      },
      "source": [
        "### Learning Rate Scheduling\n",
        "\n",
        "Finding a good learning rate can be difficult. If the learning rate is too high the algorithm can diverge. If it is too low the algorithm will take too long to train. If it is slightly too high the algorithm may dance around the optimum and not converge unless you are using an apadtive learning rate algorithm like AdaGrad, RMSProp, or Adam.\n",
        "\n",
        "Below are some strategies for _learning rate scheduling_, training methods which start with a high learning rate and gradually reduce it as you get closer to the optimum:\n",
        "\n",
        "#### Predetermined piecewise constant learning rate\n",
        "\n",
        "Setting the learning rate to a high value at the start of training, e.g. $\\eta_0 = 0.01$ then reducing it to a lower rate, e.g. $\\eta_1 = 0.001$ after a constant number of training iterations. This performs well but requires tuning to find which training epoch is the right one to reduce the learning rate at.\n",
        "\n",
        "#### Performance scheduling\n",
        "\n",
        "Measure the validation error every $N$ steps and reduce the learning rate by some constant factor, $\\lambda$, when the error starts increasing.\n",
        "\n",
        "#### Exponential scheduling\n",
        "\n",
        "The learning rate is an exponential function of the iteration number, i.e.\n",
        "\n",
        "$$ \\eta(t) = \\eta_0^{\\;-t/r} $$\n",
        "\n",
        "This requires tuning of the initial learning rate, $\\eta_0$, and the rate of decay, $r$.\n",
        "\n",
        "#### Power scheduling\n",
        "\n",
        "Set the learning rate to the exponential function\n",
        "\n",
        "$$ \\eta(t) = \\eta_0 (1 + t/r)^{-c} $$\n",
        "\n",
        "where $c$ is typically set to 1. This also requires tuning like exponential scheduling, but in this case the learning rate drops much more slowly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfyjXLyFNW2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example of implementing a learning schedule with TensorFlow.\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "decay_steps = 10000\n",
        "decay_rate = 0.1\n",
        "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
        "                                           decay_steps, decay_rate)\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
        "training_op = optimizer.minimize(loss, global_step=global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iU1wF0DOETu",
        "colab_type": "text"
      },
      "source": [
        "For adaptive learning rate optimization methods like AdaGrad, RMSProp or Adam, learning rate scheduling is not necessary.\n",
        "\n",
        "## Avoiding Overfitting Through Regularization\n",
        "\n",
        "Since neural networks have tens of thousands of parameters (sometimes millions) they are prone to overfitting the data. The following section goes over the most common ways of introducing regularization into the model to prevent overfitting.\n",
        "\n",
        "### Early Stopping\n",
        "\n",
        "One way to prevent overfitting is to implement early stopping (introduced in chapter 4). After a certain number of training iterations (e.g. every 50 iterations), you measure the model's error on the validation set. If after a certain number of training iterations, the error does not decrease, stop training the model. Early stopping typically works best when combined with another regularization technique.\n",
        "\n",
        "### $\\ell_1$ and $\\ell_2$ Regularization\n",
        "\n",
        "You can add $\\ell_1$ or $\\ell_2$ regularization to neural networks' weights (typically not the biases) just like linear models in chapter 4. One way to do this with TensorFlow is to simply add the regularization to the cost function. Below is an example of adding $\\ell_1$ regularization to a neural network with one hidden layer using TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzggR7xMcJix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 300\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                            name='hidden1')\n",
        "  logits = tf.layers.dense(hidden1, n_outputs, name='outputs')\n",
        "\n",
        "W1 = tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')\n",
        "W2 = tf.get_default_graph().get_tensor_by_name('outputs/kernel:0')\n",
        "\n",
        "# New code here!\n",
        "scale = 0.001 # L1 regularization parameter\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                             logits=logits)\n",
        "  base_loss = tf.reduce_mean(x_entropy, name='avg_x_entropy')\n",
        "  reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
        "  loss = tf.add(base_loss, scale * reg_losses, name='loss')\n",
        "  \n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "  \n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVUQTJDefzXX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e618868e-04e2-4329-f9bf-5df1309477c0"
      },
      "source": [
        "# Training the model and printing the validation error.\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Accuracy: 0.8238000273704529\n",
            "Epoch: 1 Accuracy: 0.8661999702453613\n",
            "Epoch: 2 Accuracy: 0.8805999755859375\n",
            "Epoch: 3 Accuracy: 0.8880000114440918\n",
            "Epoch: 4 Accuracy: 0.892799973487854\n",
            "Epoch: 5 Accuracy: 0.8956000208854675\n",
            "Epoch: 6 Accuracy: 0.899399995803833\n",
            "Epoch: 7 Accuracy: 0.9028000235557556\n",
            "Epoch: 8 Accuracy: 0.906000018119812\n",
            "Epoch: 9 Accuracy: 0.9053999781608582\n",
            "Epoch: 10 Accuracy: 0.9053999781608582\n",
            "Epoch: 11 Accuracy: 0.9064000248908997\n",
            "Epoch: 12 Accuracy: 0.9065999984741211\n",
            "Epoch: 13 Accuracy: 0.9071999788284302\n",
            "Epoch: 14 Accuracy: 0.9070000052452087\n",
            "Epoch: 15 Accuracy: 0.9061999917030334\n",
            "Epoch: 16 Accuracy: 0.9049999713897705\n",
            "Epoch: 17 Accuracy: 0.9056000113487244\n",
            "Epoch: 18 Accuracy: 0.9074000120162964\n",
            "Epoch: 19 Accuracy: 0.9046000242233276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32qtj6LxhsQY",
        "colab_type": "text"
      },
      "source": [
        "Below is an alternative way of adding $\\ell_1$ regularization using `tf.layers.dense()`. You can use `l1_regularizer()`, `l2_regularizer()`, or `l1_l2_regularizer()` functions to add regularization to each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVjMhRq8hrmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "scale = 0.001\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "regularized_dense_layer = partial(\n",
        "    tf.layers.dense, activation=tf.nn.relu,\n",
        "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  hidden1 = regularized_dense_layer(X, n_hidden1, name='hidden1')\n",
        "  hidden2 = regularized_dense_layer(hidden1, n_hidden2, name='hidden2')\n",
        "  logits = regularized_dense_layer(hidden2, n_outputs, name='outputs')\n",
        "  \n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                             logits=logits)\n",
        "  base_loss = tf.reduce_mean(x_entropy, name='avg_x_entropy')\n",
        "  reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "  loss = tf.add_n([base_loss] + reg_losses, name='loss')\n",
        "  \n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "  \n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOByHPZSkmad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "08cfd116-f30f-4eb6-feb1-27224c2012ad"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Accuracy: 0.6510000228881836\n",
            "Epoch: 1 Accuracy: 0.8587999939918518\n",
            "Epoch: 2 Accuracy: 0.8826000094413757\n",
            "Epoch: 3 Accuracy: 0.8948000073432922\n",
            "Epoch: 4 Accuracy: 0.9017999768257141\n",
            "Epoch: 5 Accuracy: 0.9083999991416931\n",
            "Epoch: 6 Accuracy: 0.9083999991416931\n",
            "Epoch: 7 Accuracy: 0.9111999869346619\n",
            "Epoch: 8 Accuracy: 0.9120000004768372\n",
            "Epoch: 9 Accuracy: 0.9161999821662903\n",
            "Epoch: 10 Accuracy: 0.9151999950408936\n",
            "Epoch: 11 Accuracy: 0.9154000282287598\n",
            "Epoch: 12 Accuracy: 0.9160000085830688\n",
            "Epoch: 13 Accuracy: 0.9169999957084656\n",
            "Epoch: 14 Accuracy: 0.9165999889373779\n",
            "Epoch: 15 Accuracy: 0.9165999889373779\n",
            "Epoch: 16 Accuracy: 0.9165999889373779\n",
            "Epoch: 17 Accuracy: 0.9169999957084656\n",
            "Epoch: 18 Accuracy: 0.9154000282287598\n",
            "Epoch: 19 Accuracy: 0.9146000146865845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS1AkDfNlABy",
        "colab_type": "text"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "The most popular regularization technique for DNNs is [_dropout_](https://arxiv.org/pdf/1207.0580.pdf) proposed by G. E. Hinton in 2012 and in this [paper](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) by Nitish Srivastava et al. which has been shown to improve DNN accuracy by 1-2%.\n",
        "\n",
        "The algoritm is simple, at every training step each neuron has a probability, $p$, of being temporarily excluded during that round of training. That hyperparameter, $p$, is called the _dropout rate_. Dropout improves the performance of DNNs because it lets you train the DNN as if it were an ensemble of $2^N$ possible DNNs (where $N$ is the number of neurons) so it prevents overfitting. After training, you need to multiply each connection by $(1 - p)$ or the _keep rate_ to make up for the fact that each neuron on average had fewer connections during training. Alternatively you can divide each connection by $(1-p)$ during training, which as a similar effect but is not exactly equivalent.\n",
        "\n",
        "Below is an example of implementing dropout using TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymCfxQYpOsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "dropout_rate = 0.5 # == 1 - keep_rate\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
        "                            name='hidden1')\n",
        "  hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
        "  hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
        "                            name='hidden2')\n",
        "  hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
        "  logits = tf.layers.dense(hidden2_drop, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                             logits=logits)\n",
        "  loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "  \n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "  \n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ahRQjGirye1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f081c5b5-cf43-4e6b-9a15-3cfa4bd7bfc8"
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Accuracy: 0.9065999984741211\n",
            "Epoch: 1 Accuracy: 0.9215999841690063\n",
            "Epoch: 2 Accuracy: 0.9340000152587891\n",
            "Epoch: 3 Accuracy: 0.9404000043869019\n",
            "Epoch: 4 Accuracy: 0.9440000057220459\n",
            "Epoch: 5 Accuracy: 0.9473999738693237\n",
            "Epoch: 6 Accuracy: 0.9509999752044678\n",
            "Epoch: 7 Accuracy: 0.9562000036239624\n",
            "Epoch: 8 Accuracy: 0.9584000110626221\n",
            "Epoch: 9 Accuracy: 0.9616000056266785\n",
            "Epoch: 10 Accuracy: 0.9624000191688538\n",
            "Epoch: 11 Accuracy: 0.9652000069618225\n",
            "Epoch: 12 Accuracy: 0.9679999947547913\n",
            "Epoch: 13 Accuracy: 0.9679999947547913\n",
            "Epoch: 14 Accuracy: 0.9692000150680542\n",
            "Epoch: 15 Accuracy: 0.9700000286102295\n",
            "Epoch: 16 Accuracy: 0.9718000292778015\n",
            "Epoch: 17 Accuracy: 0.9710000157356262\n",
            "Epoch: 18 Accuracy: 0.972000002861023\n",
            "Epoch: 19 Accuracy: 0.9742000102996826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBjEMWl-sXkk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}