{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainingDeepNeuralNets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axcfgr1z1naq",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 11: Training Deep Neural Nets\n",
        "\n",
        "In the previous chapter, we trained a neural network with 2 hidden layers. More complex problems require networks with more hidden layers with hundreds of neurons per layer. Training these can lead to several problems:\n",
        "\n",
        "- The _vanishing gradients_ and _exploding gradients_ problem makes lower levels hard to train.\n",
        "- Training a large network can be very slow.\n",
        "- A model with millions of parameters risks overfitting the training data.\n",
        "\n",
        "Below we will discuss methods for solving all of these problems.\n",
        "\n",
        "## Vanishing/Exploding Gradients Problem\n",
        "\n",
        "While training a neural network with backpropagation, the algorithm finds the components of the error contributed by each layer to compute the error gradient.  Gradients can often get smaller and smaller as the algorithm progresses, resulting in the gradient contribution from the lower layers approaching zero. This is known as the _vanishing gradient_ problem. Alternatively, the gradient can also can grow bigger and bigger which can cause the algorithm to diverge. This is called the _exploding gradient problem_.\n",
        "\n",
        "Around 2010, a paper titled [\"Understanding the Difficulty of Training Deep Feedforward Neural Networks\"](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) found some reasons for this. The sigmoid activation function as well as the random initialization of the weight matrices' elements using a normal distribution with a mean of 0 and a standard deviation of 1. The paper showed the variance of the outputs was much larger than the variance of the inputs. Going forward in the network, the variance kept getting larger and it results in the activation saturating near the horizontal asymptotes, which causes the gradient to vanish.\n",
        "\n",
        "### Xavier and He Initialization\n",
        "\n",
        "The authors of the paper found that one way to prevent the vanishing/exploding gradient problem is to ensure that the variance of the input and output of each layer is the same. One way to do this is to initialize the weights matrix using a normal distribution with a mean of 0 and a standard deviation given by\n",
        "\n",
        "$$ \\sigma = \\sqrt{\\frac{2}{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "or a uniform distribution centered at 0 with a radius, $r$, given by\n",
        "\n",
        "$$ r = \\sqrt{\\frac{6}{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "where $n_\\text{ inputs}$ and $n_\\text{ outputs}$ is the number of input  or output connections in that particular layer. This is often known as _Xavier initialization_ after the author's first name, or sometimes _Glorot initialization_.\n",
        "\n",
        "For the ReLU activation function, we use a normal distribution with a standard deviation given by\n",
        "\n",
        "$$ \\sigma = \\frac{2}{\\sqrt{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "or a uniform distribution with a radius given by\n",
        "\n",
        "$$ r = \\sqrt{\\frac{24}{n_\\text{ inputs} + n_\\text{ outputs}}} $$\n",
        "\n",
        "which is known as _He initialization_. Below is an example of creating a layer of a neural network which uses _He initialization_. By default, `tf.layers.dense()` uses Xavier initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW0y5GYZFS11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "b815318b-30e3-4428-dd01-4875113c70b7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden = 100\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.relu,\n",
        "                          kernel_initializer=he_init, name='hidden1')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-0e19af659d5a>:9: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl7-5YydJ_tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def reset_graph(seed=42):\n",
        "  tf.reset_default_graph()\n",
        "  tf.set_random_seed(seed)\n",
        "  np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUVypontGCD4",
        "colab_type": "text"
      },
      "source": [
        "### Nonsaturating Activation Functions\n",
        "\n",
        "One of the causes of the vanishing/exploding gradient problem discussed in the paper is the sigmoid activation function. The ReLU activation function performs much better, but it has a different problem. If some neurons output negative values, after the application of the activation function, their output will be stuck at 0. Since the gradient is also 0, the neuron remains \"dead.\"\n",
        "\n",
        "One solution to this problem is to use a \"leaky\" ReLU function, given by\n",
        "\n",
        "$$ \\text{LeakyReLU}(z) = \\max(\\alpha z, z) $$\n",
        "\n",
        "where $\\alpha$ is the slope ofthe ReLU function when the value of $z$ is less than 0. Researchers have found that this activation function performs better than the \"hard\" ReLU function. You can even have $\\alpha$ be a parameter that the model learns during training. This prevents neurons from completely dying.\n",
        "\n",
        "Another activation function that performs better than leaky ReLU that was proposed in this [paper](https://arxiv.org/pdf/1511.07289v5.pdf) by Djork-Arn√© Clevert called the _exponential linear unit_ (ELU) given by\n",
        "\n",
        "$$ \\text{ELU}_\\alpha(z) = \\left\\{ \\begin{matrix}\n",
        "\\alpha\\,(\\exp(z) - 1) && \\text{if}\\;z < 0 \\\\\n",
        "z && \\text{if}\\; z \\geq 0\n",
        "\\end{matrix} \\right. $$\n",
        "\n",
        "It has the following differences from the ReLU function:\n",
        "\n",
        "- It takes negative values when $z < 0$ . which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradient problem. You can tweak the hyperparameter, $\\alpha$, sets the negative number that ELU approaches.\n",
        "\n",
        "- It has a nonzero gradient when $z < 0$, preventing the dying units issue.\n",
        "\n",
        "- The function is differentiable everywhere, which helps the speed of Gradient Descent.\n",
        "\n",
        "The disadvantage of ELU is that it takes longer to compute than ReLU. The extra time is compensated for the fact that it helps Gradient Descent converge fasted, but it does cause the model to make predictions more slowly.\n",
        "\n",
        "TensorFlow offeres an implementation of ELU which is used in the code example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m1ATbFgN1Th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden = 100\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, name='hidden1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx9wtyEcN-1W",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow does not have an implementation of leaky ReLU, but it is easy to define ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7eAj0MgOJTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "  return tf.maximum(alpha * z, z)\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden = 100\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden, activation=leaky_relu, name='hidden1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3lOO3R7OYpc",
        "colab_type": "text"
      },
      "source": [
        "### Batch Normalization\n",
        "\n",
        "In this [paper](https://arxiv.org/pdf/1502.03167v3.pdf) Sergey Ioffe and Christian Szegedy proposed a technique called _Batch Normalization_ (BN) to address both the vanishing/exploding gradient problem and the problem that the distribution of each layer's inputs change when the parameters of the previous layers change (i.e. the _Internal Covariate Shift_ problem).\n",
        "\n",
        "The technique adds an operation to the model just before applying the activation function of each layer. It zero-centers and normalizes the inputs, then it scales and shifts the result using two new parameters per layer. This lets the model learn the optimal mean and shift for each layer.\n",
        "\n",
        "The algorithm starts by first computing the empirical mean for the current mini-batch, $B$, given by\n",
        "\n",
        "$$ \\mu_B = \\frac{1}{m_B} \\sum\\limits_{i\\,=\\,1}^{m_B} \\mathbf{x}^{(i)} $$\n",
        "\n",
        "Next, we find the empirical standard deviation, given by\n",
        "\n",
        "$$ \\sigma_B^{\\;\\;2} = \\frac{1}{m_B} \\sum\\limits_{i\\,=\\,1}^{m_B} \\left( \\mathbf{x}^{(i)} - \\mu_B \\right)^2 $$\n",
        "\n",
        "Then we zero-center and normalize the inputs in the mini-batch\n",
        "\n",
        "$$ \\hat{\\mathbf{x}}^{(i)} = \\frac{\\mathbf{x}^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^{;\\;2} + \\epsilon}} $$\n",
        "\n",
        "where $\\epsilon$ is a small number, typically $10^{-5}$, called the _smoothing term_ to avoid division by zero. Finally it computes the output given by\n",
        "\n",
        "$$ \\mathbf{z}^{(i)} = \\gamma\\,\\hat{\\mathbf{x}}^{(i)} + \\beta $$\n",
        "\n",
        "where $\\gamma$ is the scaling parameter and $\\beta$ is the shift parameter which are learned during training.\n",
        "\n",
        "When the model makes predictions, it uses the empirical mean and standard deviation of the entire training set. In the end, the model ends up learning 4 parameters: the mean of the training set, $\\mu$; the standard deviation of the training set, $\\sigma$; the scaling parameter, $\\gamma$; and the shift parameter, $\\beta$.\n",
        "\n",
        "Adding Batch Normalization to a deep neural network improves the performance of the model, lets you skip normalizing the data before training the data, and helps the model converge to the optimal parameters in fewer training iterations. However, using Batch Normalization causes the model to make predictions slower since it adds another computational step for making predictions.\n",
        "\n",
        "#### Implementing Batch Normalization with TensorFlow\n",
        "\n",
        "TensorFlow provides a `tf.nn.batch_normalization()` function which normalizes and centers the data, but you must compute the mean and standard deviation yourself.You also have to handle the creation of the scaling and offfset parameters. TensorFlow also includes a `tf.layers.batch_normalization()` function which handles all of batch normalization for you. Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jir_-CpyJfvo",
        "colab_type": "code",
        "outputId": "767ccff5-981c-47ed-e848-0468ec73429e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2 # MNIST dataset.\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "\n",
        "# Indicates if the batch normalization should be using the mini-batch's mean\n",
        "# or the mean of the entire training set (same with standard deviation).\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logts = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
        "                                      momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-8427158e4514>:15: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbzsx0D4M1Kq",
        "colab_type": "text"
      },
      "source": [
        "The BN algorithm uses _exponential decay_ to compute a running average, which is why it requires the _momentum_ parameter. Given a new value, $v$, it updates the running average $\\hat{v}$ given by\n",
        "\n",
        "$$ \\hat{v} \\leftarrow \\hat{v} \\times \\text{momentum} + v \\times (1 - \\text{momentum}) $$\n",
        "\n",
        "Momentum values should be typically close to 1, e.g. 0.9, 0.99, or 0.999.\n",
        "\n",
        "Below is an example of using _partial application_ using the `functools` library in order to make the code less repetitive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3ESzVlLNmbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "\n",
        "# Indicates if the batch normalization should be using the mini-batch's mean\n",
        "# or the mean of the entire training set (same with standard deviation).\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saqpC1nRRtEb",
        "colab_type": "code",
        "outputId": "fa4a18cc-e54d-4a78-c698-fb7626ade2d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Setting up the rest of the graph for training.\n",
        "\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxI0G_EvSbV3",
        "colab_type": "code",
        "outputId": "6f33a6b8-2344-40b3-9a8c-91a2577e6750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Downloading the MNIST dataset.\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
        "\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "  rnd_idx = np.random.permutation(len(X))\n",
        "  n_batches = len(X) // batch_size\n",
        "  for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "    X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "    yield X_batch, y_batch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szFxyIuFTSXD",
        "colab_type": "code",
        "outputId": "0ce2c18c-5bce-4c82-f394-0d7d1b23ef8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Training the neural network using Batch Normalization.\n",
        "# In just 20 training iterations it achieves 97% accuracy on the\n",
        "# validation set.\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "# These extra ops are for training the scaling and offset parameters\n",
        "# in batch normalization.\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run([training_op, extra_update_ops],\n",
        "               feed_dict={training: True, X: X_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Validation Accuracy: 0.8841999769210815\n",
            "Epoch: 1 Validation Accuracy: 0.9100000262260437\n",
            "Epoch: 2 Validation Accuracy: 0.9211999773979187\n",
            "Epoch: 3 Validation Accuracy: 0.932200014591217\n",
            "Epoch: 4 Validation Accuracy: 0.9377999901771545\n",
            "Epoch: 5 Validation Accuracy: 0.9444000124931335\n",
            "Epoch: 6 Validation Accuracy: 0.9480000138282776\n",
            "Epoch: 7 Validation Accuracy: 0.9509999752044678\n",
            "Epoch: 8 Validation Accuracy: 0.954200029373169\n",
            "Epoch: 9 Validation Accuracy: 0.9575999975204468\n",
            "Epoch: 10 Validation Accuracy: 0.9588000178337097\n",
            "Epoch: 11 Validation Accuracy: 0.9603999853134155\n",
            "Epoch: 12 Validation Accuracy: 0.9634000062942505\n",
            "Epoch: 13 Validation Accuracy: 0.9639999866485596\n",
            "Epoch: 14 Validation Accuracy: 0.9661999940872192\n",
            "Epoch: 15 Validation Accuracy: 0.9679999947547913\n",
            "Epoch: 16 Validation Accuracy: 0.9678000211715698\n",
            "Epoch: 17 Validation Accuracy: 0.9685999751091003\n",
            "Epoch: 18 Validation Accuracy: 0.968999981880188\n",
            "Epoch: 19 Validation Accuracy: 0.9696000218391418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K51njD5WZmm",
        "colab_type": "text"
      },
      "source": [
        "An alternate syntax to training a model this way is to define the `training_op` the following way:\n",
        "\n",
        "```python\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(extra_update_ops):\n",
        "        training_op = optimizer.minimize(loss)\n",
        "```\n",
        "\n",
        "this lets you train the model using the more simple syntax:\n",
        "\n",
        "```python\n",
        "sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "```\n",
        "\n",
        "### Gradient Clipping\n",
        "\n",
        "One way to solve the exploding gradients problem is to clip the gradients' values to a defined range. This technique is called [_Gradient Clipping_](http://proceedings.mlr.press/v28/pascanu13.pdf). Though in general people prefer Batch Normalization. Below is an example of Gradient Clipping using TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-JHcp_LXL3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example of gradient clipping.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "hidden1_act = tf.nn.elu(hidden1)\n",
        "\n",
        "hidden2 = tf.layers.dense(hidden1_act, n_hidden2, name='hidden2')\n",
        "hidden2_act = tf.nn.elu(hidden2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(hidden2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "# Gradient clipping is here.\n",
        "threshold = 1.0\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "grads_and_vars = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
        "              for grad, var in grads_and_vars]\n",
        "training_op = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Z-m3dzZ5Sm",
        "colab_type": "text"
      },
      "source": [
        "## Reusing Pretrained Layers\n",
        "\n",
        "Instead of training a large DNN from scratch, it is generally better to reuse an existing neural network used for a similar task, then reuse the lower layers of this network. This technique is called _transfer learning_.\n",
        "\n",
        "### Reusing a TensorFlow model\n",
        "\n",
        "Below is an example of saving and restoring a TensorFlow model using the `tr.train.import_meta_graph()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mswAchPZdh6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the graph and saving it.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# New code here!\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  saver.save(sess, './my_model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3qPPnLIfYii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting nodes in the previous graph for reuse.\n",
        "\n",
        "X = tf.get_default_graph().get_tensor_by_name('X:0')\n",
        "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
        "accuracy = tf.get_default_graph().get_tensor_by_name('accuracy:0')\n",
        "training_op = tf.get_default_graph().get_operation_by_name('GradientDescent')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJTAb84Bf2Pp",
        "colab_type": "code",
        "outputId": "450a3fb2-260c-45d1-bef4-b8a1d6a597ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Listing the operations in the predefined graph, truncated for readability.\n",
        "\n",
        "for op in tf.get_default_graph().get_operations()[:20]:\n",
        "  print(op.name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X\n",
            "y\n",
            "training/input\n",
            "training\n",
            "hidden1/kernel/Initializer/random_uniform/shape\n",
            "hidden1/kernel/Initializer/random_uniform/min\n",
            "hidden1/kernel/Initializer/random_uniform/max\n",
            "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden1/kernel/Initializer/random_uniform/sub\n",
            "hidden1/kernel/Initializer/random_uniform/mul\n",
            "hidden1/kernel/Initializer/random_uniform\n",
            "hidden1/kernel\n",
            "hidden1/kernel/Assign\n",
            "hidden1/kernel/read\n",
            "hidden1/bias/Initializer/zeros\n",
            "hidden1/bias\n",
            "hidden1/bias/Assign\n",
            "hidden1/bias/read\n",
            "hidden1/MatMul\n",
            "hidden1/BiasAdd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRkkdLD-jU1v",
        "colab_type": "text"
      },
      "source": [
        "Below is an example of creating a collection of important operations. This is often helpful if the graph is large and you only want to reuse certain operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bc_2l9ggN-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the original graph.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                           training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
        "bn1 = batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
        "bn2 = batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
        "logits = batch_norm_layer(logits_before_bn)\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                           labels=y)\n",
        "loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# New code here!\n",
        "for op in (X, y, accuracy, training_op):\n",
        "  tf.add_to_collection('my_important_ops', op)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  saver.save(sess, './my_model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGuXXeZ9jmrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Restoring the graph and getting the operations from the collection.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "saver = tf.train.import_meta_graph('./my_model.ckpt.meta')\n",
        "\n",
        "X, y, accuracy, training_op = tf.get_collection('my_important_ops')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjw5pBVmqC5Q",
        "colab_type": "text"
      },
      "source": [
        "You can also define a restore Saver which will only restore specified variables. This is useful if you only want to restore the lower layers of a neural network. Below is an example of restoring only the lower layers of a neural network using a saver which only restores specified variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kqrygEnlNDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a graph with 5 hidden layers. First implementing gradient clipping.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None,), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(None))\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  layer = X\n",
        "  for i, n_hidden in enumerate((300, 100, 50, 20)):\n",
        "    hidden = tf.layers.dense(layer, n_hidden1,\n",
        "                             name='hidden{}'.format(i+1))\n",
        "    layer = tf.nn.relu(hidden, name='relu{}'.format(i+1))\n",
        "  logits = tf.layers.dense(layer, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                             labels=y)\n",
        "  loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iUbtgyCqapj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the savers and running the training.\n",
        "\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]') # Regex string\n",
        "reuse_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwbNBPC4CxFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the model one time to train the lower layers of the neural network.\n",
        "\n",
        "import os\n",
        "\n",
        "model_path = './my_model.ckpt'\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbbnS6dcLPys",
        "colab_type": "code",
        "outputId": "7616ef06-1c56-47d1-bd97-40e1999743cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# Training the model again, this time we restore the lower layers\n",
        "\n",
        "new_model_path = './my_model_new.ckpt'\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, new_model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./my_model.ckpt\n",
            "Epoch: 0 Validation set accuracy: 0.9498000144958496\n",
            "Epoch: 1 Validation set accuracy: 0.9549999833106995\n",
            "Epoch: 2 Validation set accuracy: 0.9552000164985657\n",
            "Epoch: 3 Validation set accuracy: 0.9559999704360962\n",
            "Epoch: 4 Validation set accuracy: 0.9585999846458435\n",
            "Epoch: 5 Validation set accuracy: 0.9606000185012817\n",
            "Epoch: 6 Validation set accuracy: 0.9599999785423279\n",
            "Epoch: 7 Validation set accuracy: 0.9617999792098999\n",
            "Epoch: 8 Validation set accuracy: 0.9629999995231628\n",
            "Epoch: 9 Validation set accuracy: 0.9628000259399414\n",
            "Epoch: 10 Validation set accuracy: 0.9635999798774719\n",
            "Epoch: 11 Validation set accuracy: 0.9652000069618225\n",
            "Epoch: 12 Validation set accuracy: 0.9664000272750854\n",
            "Epoch: 13 Validation set accuracy: 0.9666000008583069\n",
            "Epoch: 14 Validation set accuracy: 0.9678000211715698\n",
            "Epoch: 15 Validation set accuracy: 0.9674000144004822\n",
            "Epoch: 16 Validation set accuracy: 0.967199981212616\n",
            "Epoch: 17 Validation set accuracy: 0.9685999751091003\n",
            "Epoch: 18 Validation set accuracy: 0.968999981880188\n",
            "Epoch: 19 Validation set accuracy: 0.9692000150680542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81TV-KeqS3dx",
        "colab_type": "text"
      },
      "source": [
        "### Reusing Models From another Frameworks\n",
        "\n",
        "Below is an example to illustrate how to import models from other frameworks into TensorFlow. This code sets the kernel and bias of a hidden layer at the start of the TensorFlow session using the initializer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH543xCWTFtS",
        "colab_type": "code",
        "outputId": "8e53dd2f-5af7-4c3a-cc98-537127550ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]]\n",
        "original_b = [7., 8., 9.]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, name='hidden')\n",
        "\n",
        "graph = tf.get_default_graph()\n",
        "assign_kernel = graph.get_operation_by_name('hidden/kernel/Assign')\n",
        "assign_bias = graph.get_operation_by_name('hidden/bias/Assign')\n",
        "\n",
        "init_kernel = assign_kernel.inputs[1]\n",
        "init_bias = assign_bias.inputs[1]\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init, feed_dict={\n",
        "    init_kernel: original_w,\n",
        "    init_bias: original_b,\n",
        "  })\n",
        "  print(hidden.eval(feed_dict={X: [[10., 11.]]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHrxoDnfXgYF",
        "colab_type": "text"
      },
      "source": [
        "Another way is to make dedicated nodes for assigning the hidden layer's kernel and bias, then set them at any point using placeholders. This is more verbose but allows more control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzUkDhs7YGEX",
        "colab_type": "code",
        "outputId": "947e30a6-1151-4be2-89f9-b2a7e0aff64a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]]\n",
        "original_b = [7., 8., 9.]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.relu, name='hidden')\n",
        "\n",
        "with tf.variable_scope('', default_name='', reuse=True):\n",
        "  hidden_weights = tf.get_variable('hidden/kernel')\n",
        "  hidden_bias = tf.get_variable('hidden/bias')\n",
        "  \n",
        "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden))\n",
        "original_bias = tf.placeholder(tf.float32, shape=(n_hidden))\n",
        "\n",
        "assign_hidden_weights = tf.assign(hidden_weights, original_weights)\n",
        "assign_hidden_bias = tf.assign(hidden_bias, original_bias)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  sess.run(assign_hidden_weights, feed_dict={original_weights: original_w})\n",
        "  sess.run(assign_hidden_bias, feed_dict={original_bias: original_b})\n",
        "  print(hidden.eval(feed_dict={X: [[10., 11.]]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCgFlOhUtNZO",
        "colab_type": "text"
      },
      "source": [
        "### Freezing Lower Layers\n",
        "\n",
        "Since it is likely that the lower layers of the DNN have learned to detect the lower level patterns in the training set, you can reuse the layers as they are by \"freezing\" their weights. As a result, the higher level layers will be easier to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tfRdZ2IvJ80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a new graph which will use the lower level\n",
        "# layers from the section on using pre-trained layers.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None,), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(None))\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  layer = X\n",
        "  for i, n_hidden in enumerate((300, 100, 50, 20)):\n",
        "    hidden = tf.layers.dense(layer, n_hidden1,\n",
        "                             name='hidden{}'.format(i+1))\n",
        "    layer = tf.nn.relu(hidden, name='relu{}'.format(i+1))\n",
        "  logits = tf.layers.dense(layer, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                             labels=y)\n",
        "  loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "# New code here!\n",
        "with tf.name_scope('train'):\n",
        "  training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                    scope='hidden[34]|outputs')\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "  training_op = optimizer.minimize(loss, var_list=training_vars)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]') # Regex string\n",
        "reuse_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGxpZs2ev3XV",
        "colab_type": "code",
        "outputId": "51f42593-2ef2-4eeb-8727-09fbf17dd8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Training the model with the frozen layers.\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, new_model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model.ckpt\n",
            "Epoch: 0 Validation set accuracy: 0.9531999826431274\n",
            "Epoch: 1 Validation set accuracy: 0.9539999961853027\n",
            "Epoch: 2 Validation set accuracy: 0.9553999900817871\n",
            "Epoch: 3 Validation set accuracy: 0.9552000164985657\n",
            "Epoch: 4 Validation set accuracy: 0.9567999839782715\n",
            "Epoch: 5 Validation set accuracy: 0.9563999772071838\n",
            "Epoch: 6 Validation set accuracy: 0.9580000042915344\n",
            "Epoch: 7 Validation set accuracy: 0.9577999711036682\n",
            "Epoch: 8 Validation set accuracy: 0.9581999778747559\n",
            "Epoch: 9 Validation set accuracy: 0.9584000110626221\n",
            "Epoch: 10 Validation set accuracy: 0.9584000110626221\n",
            "Epoch: 11 Validation set accuracy: 0.9589999914169312\n",
            "Epoch: 12 Validation set accuracy: 0.9577999711036682\n",
            "Epoch: 13 Validation set accuracy: 0.9595999717712402\n",
            "Epoch: 14 Validation set accuracy: 0.9606000185012817\n",
            "Epoch: 15 Validation set accuracy: 0.9599999785423279\n",
            "Epoch: 16 Validation set accuracy: 0.9599999785423279\n",
            "Epoch: 17 Validation set accuracy: 0.9592000246047974\n",
            "Epoch: 18 Validation set accuracy: 0.9599999785423279\n",
            "Epoch: 19 Validation set accuracy: 0.9603999853134155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-t_5pzMwK8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Another way to freeze the lower layers is to add a stop_gradient()\n",
        "# layer in the graph. Any layer below it will be frozen.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None,), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(None))\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  layer = X\n",
        "  for i, n_hidden in enumerate((300, 100, 50, 20)):\n",
        "    hidden = tf.layers.dense(layer, n_hidden1,\n",
        "                             name='hidden{}'.format(i+1))\n",
        "    layer = tf.nn.relu(hidden, name='relu{}'.format(i+1))\n",
        "    # New code here!\n",
        "    if i == 1:\n",
        "      hidden2 = layer\n",
        "      layer = tf.stop_gradient(layer)\n",
        "  logits = tf.layers.dense(layer, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                             labels=y)\n",
        "  loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden[123]') # Regex string\n",
        "reuse_saver = tf.train.Saver(reuse_vars)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiZrIkI-w3mi",
        "colab_type": "code",
        "outputId": "adb8eb89-584d-4279-915f-a8fdd073f930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, new_model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model.ckpt\n",
            "Epoch: 0 Validation set accuracy: 0.9383999705314636\n",
            "Epoch: 1 Validation set accuracy: 0.9441999793052673\n",
            "Epoch: 2 Validation set accuracy: 0.9485999941825867\n",
            "Epoch: 3 Validation set accuracy: 0.9509999752044678\n",
            "Epoch: 4 Validation set accuracy: 0.9527999758720398\n",
            "Epoch: 5 Validation set accuracy: 0.951200008392334\n",
            "Epoch: 6 Validation set accuracy: 0.954200029373169\n",
            "Epoch: 7 Validation set accuracy: 0.9559999704360962\n",
            "Epoch: 8 Validation set accuracy: 0.9559999704360962\n",
            "Epoch: 9 Validation set accuracy: 0.9562000036239624\n",
            "Epoch: 10 Validation set accuracy: 0.9559999704360962\n",
            "Epoch: 11 Validation set accuracy: 0.9562000036239624\n",
            "Epoch: 12 Validation set accuracy: 0.9556000232696533\n",
            "Epoch: 13 Validation set accuracy: 0.9575999975204468\n",
            "Epoch: 14 Validation set accuracy: 0.9580000042915344\n",
            "Epoch: 15 Validation set accuracy: 0.9577999711036682\n",
            "Epoch: 16 Validation set accuracy: 0.9580000042915344\n",
            "Epoch: 17 Validation set accuracy: 0.9595999717712402\n",
            "Epoch: 18 Validation set accuracy: 0.9598000049591064\n",
            "Epoch: 19 Validation set accuracy: 0.9589999914169312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C02iirCBxHuc",
        "colab_type": "text"
      },
      "source": [
        "### Caching Frozen Layers\n",
        "\n",
        "One way to improve the speed of training when you have frozen the lower layers is to run the training set through the frozen layers at the start of training. The code below shows an example of training a model this way, reusing the TensorFlow graph defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZaxwDuAzZJ-",
        "colab_type": "code",
        "outputId": "9b5f0328-ec9a-4cd9-eedf-43e2122251ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "  h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
        "  for epoch in range(n_epochs):\n",
        "    for h2_batch, y_batch in shuffle_batch(h2_cache, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={hidden2: h2_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Validation set accuracy: {}'.format(epoch, accuracy_val))\n",
        "  saver.save(sess, new_model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model.ckpt\n",
            "Epoch: 0 Validation set accuracy: 0.9369999766349792\n",
            "Epoch: 1 Validation set accuracy: 0.9467999935150146\n",
            "Epoch: 2 Validation set accuracy: 0.9506000280380249\n",
            "Epoch: 3 Validation set accuracy: 0.9521999955177307\n",
            "Epoch: 4 Validation set accuracy: 0.9526000022888184\n",
            "Epoch: 5 Validation set accuracy: 0.9531999826431274\n",
            "Epoch: 6 Validation set accuracy: 0.9544000029563904\n",
            "Epoch: 7 Validation set accuracy: 0.954200029373169\n",
            "Epoch: 8 Validation set accuracy: 0.9545999765396118\n",
            "Epoch: 9 Validation set accuracy: 0.9559999704360962\n",
            "Epoch: 10 Validation set accuracy: 0.9559999704360962\n",
            "Epoch: 11 Validation set accuracy: 0.9570000171661377\n",
            "Epoch: 12 Validation set accuracy: 0.9575999975204468\n",
            "Epoch: 13 Validation set accuracy: 0.9563999772071838\n",
            "Epoch: 14 Validation set accuracy: 0.9567999839782715\n",
            "Epoch: 15 Validation set accuracy: 0.9575999975204468\n",
            "Epoch: 16 Validation set accuracy: 0.9581999778747559\n",
            "Epoch: 17 Validation set accuracy: 0.9588000178337097\n",
            "Epoch: 18 Validation set accuracy: 0.9581999778747559\n",
            "Epoch: 19 Validation set accuracy: 0.9595999717712402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4WhKgIR2AmA",
        "colab_type": "text"
      },
      "source": [
        "### Tweaking, Dropping, or Replacing the Upper Layers\n",
        "\n",
        "The higher the layer is in the previously trained neural network, the less likely it will be useful in training a new network for different tasks. The output layer is generally always replaced, in many cases the old output layer may be a different shape than the output for the new layer.\n",
        "\n",
        "One way to try to determine how many layers to freeze is to try freezing all of the hidden layers first, then training the neural network again after unfreezing one or two of the top layers and seeing if performance improves. The more training data, the more layers you can unfreeze.\n",
        "\n",
        "If you still cannot get good performance with little training data, you can try dropping the top hidden layers and freezing the lower ones. You can keep trying until you find the right number of layers to reuse. If you have a lot of training data, you can replace the top layers instead of dropping them or add more layers.\n",
        "\n",
        "### Model Zoos\n",
        "\n",
        "A _model zoo_ is a collection of machine learning models that other people have trained for different machine learning tasks. TensorFlow has its own [model zoo](https://github.com/tensorflow/models). Another popular model zoo is the [Caffe Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo). Saumitro Dasgupta wrote a [converter](https://github.com/ethereon/caffe-tensorflow) to convert Caffe models to TensorFlow.\n",
        "\n",
        "### Unsupervised Training\n",
        "\n",
        "If you do not have a large labeled training set and there is not a previously trained model for a similar task, but you do have a large unlabled training set, you can use an _unsupervised pretraining_ algorithms such as _Restricted Boltzmann Machines_ (RBMs) or autoencoders to train successlive DNN layers to find low level features in the training set. Afterwards you can tune the model using supervised learning and backpropagation.\n",
        "\n",
        "### Pretraining an Auxilary Task\n",
        "\n",
        "One way to train a DNN if you have limited labeled training data is to train a neural network for a similar task then reuse the lower layers to train a new DNN for the actual task.\n",
        "\n",
        "Another strategy is to take unlabeled training data and take some data and modify it. You label the unmodified data as \"good\" and the modified data as \"bad\" so that you can train a DNN classifier using a supervised algorithm to get lower layers which recognize lower level features for the actual task.\n",
        "\n",
        "## Faster Optimizers\n",
        "\n",
        "In this section we will examine optimizers which are faster than plain Gradient Descent which can help speed up training DNNs.\n",
        "\n",
        "### Momentum Optimizers\n",
        "\n",
        "Recall that Gradient Descent updates the weight vector, $\\theta$ by subtracting the weight of the gradient of the cost function, $\\nabla_\\theta J(\\theta)$ multiplied by the learning rate, $\\eta$, i.e.\n",
        "\n",
        "$$ \\theta \\leftarrow \\theta - \\eta\\nabla_\\theta J(\\theta) $$\n",
        "\n",
        "[Momentum optimization](https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods), proposed by Boris Polyak in 1964 subtracts the local gradient from a momentum vector, $\\mathbf{m}$, multiplied by the learning rate $\\eta$ and it updates the weights by adding the momentum vector to the weight vector, $\\theta$. To prevent the momentum from growing too large, the algorithm introduces a hyperparameter, $\\beta$, called the _momentum_, which is between 0 and 1 (typically 0.9). The algorithm can be written in two stages:\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "1. && \\mathbf{m} \\leftarrow \\beta\\,\\mathbf{m} - \\eta\\nabla_\\theta J(\\theta) \\\\\n",
        "2. && \\theta \\leftarrow \\theta + \\mathbf{m}\n",
        "\\end{matrix}  $$\n",
        "\n",
        "It follows that if the gradient remains constant, the terminal velocity (the maximum size of the weight updates) is given by the learning rate, $\\eta$, multiplied by $\\frac{1}{1\\,-\\,\\beta}$. If $\\beta = 0.9$ then Momentum optimization converges 10 times as quickly as Gradient Descent. Due to the larger steps, Momentum optimization can escape local optima much more quickly than Gradient Descent. Below is an example of implementing Momentum optimization in TensorFlow:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0OFkaNcLfMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rjSAkPILoF4",
        "colab_type": "text"
      },
      "source": [
        "The main drawback of Momentum optimization is it adds another hyperparameter to tune, but generally 0.9 works in practice.\n",
        "\n",
        "### Nesterov Accelerated Gradient\n",
        "\n",
        "One improvement to Momentum optimization proposed by Yuli Nesterov in 1983 is called [Nesterov Momentum Optimization](https://scholar.google.com/citations?view_op=view_citation&citation_for_view=DJ8Ep8YAAAAJ:hkOj_22Ku90C) or _Nesterov Accelerated Gradient_ (NAG). The algorithm works in the following steps:\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "1. && \\mathbf{m} \\leftarrow \\beta\\,\\mathbf{m} - \\eta\\nabla_\\theta J(\\theta + \\beta\\,\\mathbf{m}) \\\\\n",
        "2. && \\theta \\leftarrow \\theta + \\mathbf{m}\n",
        "\\end{matrix} $$\n",
        "\n",
        "The only difference between NAG and Momentum optimization is that it computes the gradient of the cost function at $\\theta + \\beta\\,\\mathbf{m}$ instead of at the current value of $\\theta$. This improves the algorithm since the momentum vector is usually pointing in the direction of the optimal value. Below is an example of NAG using TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1VNy0SfQAth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9,\n",
        "                                       use_nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh7oCaBhSbpE",
        "colab_type": "text"
      },
      "source": [
        "### AdaGrad Optimizer\n",
        "\n",
        "The [AdaGrad algorithm](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) is an algorithm designed to decay the learning rate, doing so faster for steeper gradients, in order to converge more directly towards the global optimum. This is called an _adaptive learning rate_. The algorithm works in two stages:\n",
        "\n",
        "The first stage computes a vector, $\\mathbf{s}$, given by\n",
        "\n",
        "$$ \\mathbf{s} \\leftarrow \\mathbf{s} + \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta) $$\n",
        "\n",
        "where $\\otimes$ denotes component-wise multiplication. This is a vectorized form of the following operation:\n",
        "\n",
        "$$ s_i \\leftarrow s_i + \\left( \\frac{\\partial J(\\theta)}{\\partial \\theta_i} \\right)^2 $$\n",
        "\n",
        "where $s_i$ denotes a component of $\\mathbf{s}$.The vector $\\mathbf{s}$ accumulates the squares of each component of the gradient, and it grows larger when the gradient is steeper.\n",
        "\n",
        "The second step is similar to Gradient Descent but with a modification. It is given by\n",
        "\n",
        "$$ \\theta \\leftarrow \\theta - \\eta\\nabla_\\theta J(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$\n",
        "\n",
        "where $\\oslash$ denotes component-wise division and $\\epsilon$ is a smoothing term to avoid division by zero (it is typically 10<sup>-10</sup>). This operation is a vectorized form of the following operation:\n",
        "\n",
        "$$ \\theta_i \\leftarrow \\theta_i - \\eta \\left( \\frac{\\partial J(\\theta)}{\\partial \\theta_i} \\right) \\left( s_i + \\epsilon \\right)^{-1/2} $$\n",
        "\n",
        "where $\\theta_i$ is each component of the vector $\\theta$.\n",
        "\n",
        "AdaGrad performs well for simple quadratic problems, but often stops too early when training DNNs because the learning rate degrades to zero. Below is an example of an AdaGrad optimizer in TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi8CR9TDWrSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdagradOptimizer(learning_rate=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmQg86NtYSZD",
        "colab_type": "text"
      },
      "source": [
        "### RMSProp\n",
        "\n",
        "Since AdaGrad can decay the learning rate too quickly, [RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) slows the rate of decay of the learning rate by accumulating only the most recent iterations using exponential decay. The algorithm works as follows:\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "1. && \\mathbf{s} \\leftarrow \\beta\\,\\mathbf{s} + (1 - \\beta\\, ) \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta) \\\\\n",
        "2. && \\theta \\leftarrow \\theta - \\eta\\nabla_\\theta J(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon}\n",
        "\\end{matrix} $$\n",
        "\n",
        "where $\\beta$ is the decay rate and is typically set to 0.9. This value typically works in practice so you do not have to tune it. Except for simple problems, RMSProp typically performs better than AdaGrad. Below is an example of RMSProp with TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THExdj-mbBgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.1, momentum=0.9,\n",
        "                                      decay=0.9, epsilon=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0M0N3OBdJoS",
        "colab_type": "text"
      },
      "source": [
        "### Adam Optimizer\n",
        "\n",
        "[Adam](https://arxiv.org/pdf/1412.6980v8.pdf) which stands for _adaptive moment estimation_ combines Momentum optimizers and RMSProp, keeping track of an exponentially decaying average of past gradients and past squared gradients. The algorithm works as follows:\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "1. && \\mathbf{m} \\leftarrow \\beta_1\\mathbf{m} - (1 - \\beta_1) \\nabla_\\theta J(\\theta) \\\\\n",
        "2. && \\mathbf{s} \\leftarrow \\beta_2\\mathbf{s} + (1 - \\beta_2) \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta) \\\\\n",
        "3. && \\mathbf{m} \\leftarrow (1 - \\beta_1)^{-\\,t}\\,\\mathbf{m} \\\\\n",
        "4. && \\mathbf{s} \\leftarrow (1 - \\beta_2)^{-\\,t} \\,\\mathbf{s} \\\\\n",
        "5. && \\theta \\leftarrow \\theta + \\eta \\, \\mathbf{m} \\oslash \\sqrt{\\mathbf{s} + \\epsilon}\n",
        "\\end{matrix}$$\n",
        "\n",
        "where $t$ is the training iteration number, $\\beta_1$ is the momentum decay rate, and $\\beta_2$ is the scaling decay rate. Steps 1, 2, and 5 resemble both Momentum optimization and RMSProp. Steps 3 and 4 account for the fact that $\\mathbf{m}$ and $\\mathbf{s}$ are initialized to zero vectors, so these steps prevent the algorithm from being biased towards zero at the beginning.\n",
        "\n",
        "The momentum decay rate, $\\beta_1$, is typically initialized to 0.9. The scaling decay rate, $\\beta_2$ is initialized to 0.99. These values perform well in practice so its rare you have to tune them. The smoothing parameter, $\\epsilon$, is typically initialized to 10<sup>-10</sup>. Since the model's learning rate is adaptive, it is not as necessary to tune the learning rate, $\\eta$.\n",
        "\n",
        "Below is an example of an Adam optimizer in TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmFQP723jBDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1ZduGrOKgwo",
        "colab_type": "text"
      },
      "source": [
        "### Learning Rate Scheduling\n",
        "\n",
        "Finding a good learning rate can be difficult. If the learning rate is too high the algorithm can diverge. If it is too low the algorithm will take too long to train. If it is slightly too high the algorithm may dance around the optimum and not converge unless you are using an apadtive learning rate algorithm like AdaGrad, RMSProp, or Adam.\n",
        "\n",
        "Below are some strategies for _learning rate scheduling_, training methods which start with a high learning rate and gradually reduce it as you get closer to the optimum:\n",
        "\n",
        "#### Predetermined piecewise constant learning rate\n",
        "\n",
        "Setting the learning rate to a high value at the start of training, e.g. $\\eta_0 = 0.01$ then reducing it to a lower rate, e.g. $\\eta_1 = 0.001$ after a constant number of training iterations. This performs well but requires tuning to find which training epoch is the right one to reduce the learning rate at.\n",
        "\n",
        "#### Performance scheduling\n",
        "\n",
        "Measure the validation error every $N$ steps and reduce the learning rate by some constant factor, $\\lambda$, when the error starts increasing.\n",
        "\n",
        "#### Exponential scheduling\n",
        "\n",
        "The learning rate is an exponential function of the iteration number, i.e.\n",
        "\n",
        "$$ \\eta(t) = \\eta_0^{\\;-t/r} $$\n",
        "\n",
        "This requires tuning of the initial learning rate, $\\eta_0$, and the rate of decay, $r$.\n",
        "\n",
        "#### Power scheduling\n",
        "\n",
        "Set the learning rate to the exponential function\n",
        "\n",
        "$$ \\eta(t) = \\eta_0 (1 + t/r)^{-c} $$\n",
        "\n",
        "where $c$ is typically set to 1. This also requires tuning like exponential scheduling, but in this case the learning rate drops much more slowly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfyjXLyFNW2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example of implementing a learning schedule with TensorFlow.\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "decay_steps = 10000\n",
        "decay_rate = 0.1\n",
        "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
        "                                           decay_steps, decay_rate)\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
        "training_op = optimizer.minimize(loss, global_step=global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iU1wF0DOETu",
        "colab_type": "text"
      },
      "source": [
        "For adaptive learning rate optimization methods like AdaGrad, RMSProp or Adam, learning rate scheduling is not necessary.\n",
        "\n",
        "## Avoiding Overfitting Through Regularization\n",
        "\n",
        "Since neural networks have tens of thousands of parameters (sometimes millions) they are prone to overfitting the data. The following section goes over the most common ways of introducing regularization into the model to prevent overfitting.\n",
        "\n",
        "### Early Stopping\n",
        "\n",
        "One way to prevent overfitting is to implement early stopping (introduced in chapter 4). After a certain number of training iterations (e.g. every 50 iterations), you measure the model's error on the validation set. If after a certain number of training iterations, the error does not decrease, stop training the model. Early stopping typically works best when combined with another regularization technique.\n",
        "\n",
        "### $\\ell_1$ and $\\ell_2$ Regularization\n",
        "\n",
        "You can add $\\ell_1$ or $\\ell_2$ regularization to neural networks' weights (typically not the biases) just like linear models in chapter 4. One way to do this with TensorFlow is to simply add the regularization to the cost function. Below is an example of adding $\\ell_1$ regularization to a neural network with one hidden layer using TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzggR7xMcJix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 300\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                            name='hidden1')\n",
        "  logits = tf.layers.dense(hidden1, n_outputs, name='outputs')\n",
        "\n",
        "W1 = tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')\n",
        "W2 = tf.get_default_graph().get_tensor_by_name('outputs/kernel:0')\n",
        "\n",
        "# New code here!\n",
        "scale = 0.001 # L1 regularization parameter\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                             logits=logits)\n",
        "  base_loss = tf.reduce_mean(x_entropy, name='avg_x_entropy')\n",
        "  reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
        "  loss = tf.add(base_loss, scale * reg_losses, name='loss')\n",
        "  \n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "  \n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVUQTJDefzXX",
        "colab_type": "code",
        "outputId": "673832db-895d-4867-92ba-0aed8da7fc14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Training the model and printing the validation error.\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Accuracy: 0.8309999704360962\n",
            "Epoch: 1 Accuracy: 0.8709999918937683\n",
            "Epoch: 2 Accuracy: 0.8838000297546387\n",
            "Epoch: 3 Accuracy: 0.8934000134468079\n",
            "Epoch: 4 Accuracy: 0.8966000080108643\n",
            "Epoch: 5 Accuracy: 0.8988000154495239\n",
            "Epoch: 6 Accuracy: 0.9016000032424927\n",
            "Epoch: 7 Accuracy: 0.9043999910354614\n",
            "Epoch: 8 Accuracy: 0.9057999849319458\n",
            "Epoch: 9 Accuracy: 0.906000018119812\n",
            "Epoch: 10 Accuracy: 0.9067999720573425\n",
            "Epoch: 11 Accuracy: 0.9053999781608582\n",
            "Epoch: 12 Accuracy: 0.9070000052452087\n",
            "Epoch: 13 Accuracy: 0.9083999991416931\n",
            "Epoch: 14 Accuracy: 0.9088000059127808\n",
            "Epoch: 15 Accuracy: 0.9064000248908997\n",
            "Epoch: 16 Accuracy: 0.9065999984741211\n",
            "Epoch: 17 Accuracy: 0.9065999984741211\n",
            "Epoch: 18 Accuracy: 0.9065999984741211\n",
            "Epoch: 19 Accuracy: 0.9052000045776367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32qtj6LxhsQY",
        "colab_type": "text"
      },
      "source": [
        "Below is an alternative way of adding $\\ell_1$ regularization using `tf.layers.dense()`. You can use `l1_regularizer()`, `l2_regularizer()`, or `l1_l2_regularizer()` functions to add regularization to each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVjMhRq8hrmJ",
        "colab_type": "code",
        "outputId": "4d731dd7-abc6-4d73-a67c-04abf04024a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "scale = 0.001\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "regularized_dense_layer = partial(\n",
        "    tf.layers.dense, activation=tf.nn.relu,\n",
        "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  hidden1 = regularized_dense_layer(X, n_hidden1, name='hidden1')\n",
        "  hidden2 = regularized_dense_layer(hidden1, n_hidden2, name='hidden2')\n",
        "  logits = regularized_dense_layer(hidden2, n_outputs, name='outputs')\n",
        "  \n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                             logits=logits)\n",
        "  base_loss = tf.reduce_mean(x_entropy, name='avg_x_entropy')\n",
        "  reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "  loss = tf.add_n([base_loss] + reg_losses, name='loss')\n",
        "  \n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "  \n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOByHPZSkmad",
        "colab_type": "code",
        "outputId": "48aaf21e-87e4-45f9-be76-f979dc07abc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Accuracy: 0.8176000118255615\n",
            "Epoch: 1 Accuracy: 0.8751999735832214\n",
            "Epoch: 2 Accuracy: 0.8913999795913696\n",
            "Epoch: 3 Accuracy: 0.9020000100135803\n",
            "Epoch: 4 Accuracy: 0.9064000248908997\n",
            "Epoch: 5 Accuracy: 0.9082000255584717\n",
            "Epoch: 6 Accuracy: 0.9121999740600586\n",
            "Epoch: 7 Accuracy: 0.9146000146865845\n",
            "Epoch: 8 Accuracy: 0.9150000214576721\n",
            "Epoch: 9 Accuracy: 0.9187999963760376\n",
            "Epoch: 10 Accuracy: 0.9179999828338623\n",
            "Epoch: 11 Accuracy: 0.9196000099182129\n",
            "Epoch: 12 Accuracy: 0.9179999828338623\n",
            "Epoch: 13 Accuracy: 0.9192000031471252\n",
            "Epoch: 14 Accuracy: 0.9196000099182129\n",
            "Epoch: 15 Accuracy: 0.9179999828338623\n",
            "Epoch: 16 Accuracy: 0.9186000227928162\n",
            "Epoch: 17 Accuracy: 0.920199990272522\n",
            "Epoch: 18 Accuracy: 0.9186000227928162\n",
            "Epoch: 19 Accuracy: 0.9174000024795532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS1AkDfNlABy",
        "colab_type": "text"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "The most popular regularization technique for DNNs is [_dropout_](https://arxiv.org/pdf/1207.0580.pdf) proposed by G. E. Hinton in 2012 and in this [paper](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) by Nitish Srivastava et al. which has been shown to improve DNN accuracy by 1-2%.\n",
        "\n",
        "The algoritm is simple, at every training step each neuron has a probability, $p$, of being temporarily excluded during that round of training. That hyperparameter, $p$, is called the _dropout rate_. Dropout improves the performance of DNNs because it lets you train the DNN as if it were an ensemble of $2^N$ possible DNNs (where $N$ is the number of neurons) so it prevents overfitting. After training, you need to multiply each connection by $(1 - p)$ or the _keep rate_ to make up for the fact that each neuron on average had fewer connections during training. Alternatively you can divide each connection by $(1-p)$ during training, which as a similar effect but is not exactly equivalent.\n",
        "\n",
        "Below is an example of implementing dropout using TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymCfxQYpOsE",
        "colab_type": "code",
        "outputId": "97fe9f9f-1694-4538-9530-2a408d03f0ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "dropout_rate = 0.5 # == 1 - keep_rate\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
        "                            name='hidden1')\n",
        "  hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
        "  hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
        "                            name='hidden2')\n",
        "  hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
        "  logits = tf.layers.dense(hidden2_drop, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                             logits=logits)\n",
        "  loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "  \n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "  \n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-40-979f70084efb>:14: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ahRQjGirye1",
        "colab_type": "code",
        "outputId": "6244945e-3de6-4157-dc9c-da71ce8d3626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Accuracy: 0.9021999835968018\n",
            "Epoch: 1 Accuracy: 0.9240000247955322\n",
            "Epoch: 2 Accuracy: 0.9326000213623047\n",
            "Epoch: 3 Accuracy: 0.9387999773025513\n",
            "Epoch: 4 Accuracy: 0.9431999921798706\n",
            "Epoch: 5 Accuracy: 0.9480000138282776\n",
            "Epoch: 6 Accuracy: 0.9521999955177307\n",
            "Epoch: 7 Accuracy: 0.9552000164985657\n",
            "Epoch: 8 Accuracy: 0.9584000110626221\n",
            "Epoch: 9 Accuracy: 0.9598000049591064\n",
            "Epoch: 10 Accuracy: 0.9616000056266785\n",
            "Epoch: 11 Accuracy: 0.9634000062942505\n",
            "Epoch: 12 Accuracy: 0.9661999940872192\n",
            "Epoch: 13 Accuracy: 0.9674000144004822\n",
            "Epoch: 14 Accuracy: 0.9682000279426575\n",
            "Epoch: 15 Accuracy: 0.9703999757766724\n",
            "Epoch: 16 Accuracy: 0.9711999893188477\n",
            "Epoch: 17 Accuracy: 0.9711999893188477\n",
            "Epoch: 18 Accuracy: 0.9742000102996826\n",
            "Epoch: 19 Accuracy: 0.9732000231742859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBjEMWl-sXkk",
        "colab_type": "text"
      },
      "source": [
        "### Training Sparse Models\n",
        "\n",
        "All of the optimization techniques presented produce _dense_ models, meaning all or most parameters will be nonzero. If you need a faster model or a model that runs very fast, you may need a sparse model instead.\n",
        "\n",
        "One way to achieve this is to set all small weights to zero, another option is to apply strong $\\ell_1$ regularization. One last option is to apply _Dual Averaging_, also known as [_Follow The Regularized Leader_](https://scholar.google.fr/citations?view_op=view_citation&citation_for_view=DJ8Ep8YAAAAJ:Tyk-4Ss8FVUC) (FTRL), proposed by Yurii Nesterov. TensorFlow implements a variant of FTRL called [_FTRL-Proximal_](https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf) in the `FTRLOptimizer` class.\n",
        "\n",
        "### Max-Norm Regularization\n",
        "\n",
        "Another regularization technique is called _max-norm regularization_ for the weights, $\\mathbf{w}$, of each hidden layer where it applys the following constraint\n",
        "\n",
        "$$ ||\\,\\mathbf{w}\\,||_{\\,2} \\leq r $$\n",
        "\n",
        "where $||\\cdot||_{\\,2}$ is the $\\ell_2$ norm and $r$ is the max-norm hyperparameter. The algorithm works by computing\n",
        "\n",
        "$$ \\lambda = \\max\\left( \\frac{r}{||\\,\\mathbf{w}\\,||_{\\,2}}, \\, 1 \\right) $$\n",
        "\n",
        "and then updates each hidden layer's weight vector\n",
        "\n",
        "$$ \\mathbf{w} \\leftarrow \\lambda\\,\\mathbf{w} $$\n",
        "\n",
        "TensorFlow does not have max-norm regularization built in but it is possible to implement it using the `clip_by_norm()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN8VMvMmZAZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                            name='hidden1')\n",
        "  hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
        "                            name='hidden2')\n",
        "  logits = tf.layers.dense(hidden2, n_outputs, name='outputs')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                            logits=logits)\n",
        "  loss = tf.reduce_mean(xentropy, name='loss')\n",
        "  \n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "  \n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "# New code here below\n",
        "threshold = 1.0\n",
        "\n",
        "weights1 = tf.get_default_graph().get_tensor_by_name('hidden1/kernel:0')\n",
        "clipped_weights1 = tf.clip_by_norm(weights1, clip_norm=threshold, axes=1)\n",
        "clip_weights1 = tf.assign(weights1, clipped_weights1)\n",
        "\n",
        "weights2 = tf.get_default_graph().get_tensor_by_name('hidden2/kernel:0')\n",
        "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
        "clip_weights2 = tf.assign(weights2, clipped_weights2)\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu7nztpfeWRR",
        "colab_type": "code",
        "outputId": "daa16404-8e6b-487e-8cad-361f6b5bb04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "      clip_weights1.eval()\n",
        "      clip_weights2.eval()\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Accuracy: 0.9567999839782715\n",
            "Epoch: 1 Accuracy: 0.9696000218391418\n",
            "Epoch: 2 Accuracy: 0.9715999960899353\n",
            "Epoch: 3 Accuracy: 0.9771999716758728\n",
            "Epoch: 4 Accuracy: 0.9771999716758728\n",
            "Epoch: 5 Accuracy: 0.977400004863739\n",
            "Epoch: 6 Accuracy: 0.982200026512146\n",
            "Epoch: 7 Accuracy: 0.9810000061988831\n",
            "Epoch: 8 Accuracy: 0.9800000190734863\n",
            "Epoch: 9 Accuracy: 0.9824000000953674\n",
            "Epoch: 10 Accuracy: 0.982200026512146\n",
            "Epoch: 11 Accuracy: 0.9851999878883362\n",
            "Epoch: 12 Accuracy: 0.9824000000953674\n",
            "Epoch: 13 Accuracy: 0.984000027179718\n",
            "Epoch: 14 Accuracy: 0.9842000007629395\n",
            "Epoch: 15 Accuracy: 0.9842000007629395\n",
            "Epoch: 16 Accuracy: 0.984000027179718\n",
            "Epoch: 17 Accuracy: 0.9833999872207642\n",
            "Epoch: 18 Accuracy: 0.9842000007629395\n",
            "Epoch: 19 Accuracy: 0.9843999743461609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cIQMvqhjPGP",
        "colab_type": "text"
      },
      "source": [
        "The above implementation works, but it is verbose and not reuseable. Below is a different implementation which defines a regularizer similar to the `l1_regularizer()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue4KKHSIi_AX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_norm_regularizer(threshold, axes=1, name='max_norm',\n",
        "                         collection='max_norm'):\n",
        "  def max_norm(weights):\n",
        "    clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
        "    clip_weights = tf.assign(weights, clipped)\n",
        "    tf.add_to_collection(collection, clip_weights)\n",
        "    return None\n",
        "  return max_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ-_eFmdkdBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "  hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                            kernel_regularizer=max_norm_reg, name='hidden1')\n",
        "  hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
        "                            kernel_regularizer=max_norm_reg, name='hidden2')\n",
        "  logits = tf.layers.dense(hidden2, n_outputs, name='outputs')\n",
        "  \n",
        "with tf.name_scope('loss'):\n",
        "  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                            logits=logits)\n",
        "  loss = tf.reduce_mean(xentropy, name='loss')\n",
        "  \n",
        "with tf.name_scope('train'):\n",
        "  optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "  \n",
        "with tf.name_scope('eval'):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "  \n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUcVOIgpmdOM",
        "colab_type": "code",
        "outputId": "78b00f29-572e-4049-a26b-59017e44c30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "clip_all_weights = tf.get_collection('max_norms')\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "      sess.run(clip_all_weights)\n",
        "    accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "    print('Epoch: {} Accuracy: {}'.format(epoch, accuracy_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Accuracy: 0.9562000036239624\n",
            "Epoch: 1 Accuracy: 0.9710000157356262\n",
            "Epoch: 2 Accuracy: 0.9733999967575073\n",
            "Epoch: 3 Accuracy: 0.9753999710083008\n",
            "Epoch: 4 Accuracy: 0.9746000170707703\n",
            "Epoch: 5 Accuracy: 0.9783999919891357\n",
            "Epoch: 6 Accuracy: 0.9800000190734863\n",
            "Epoch: 7 Accuracy: 0.980400025844574\n",
            "Epoch: 8 Accuracy: 0.9810000061988831\n",
            "Epoch: 9 Accuracy: 0.9824000000953674\n",
            "Epoch: 10 Accuracy: 0.9815999865531921\n",
            "Epoch: 11 Accuracy: 0.9815999865531921\n",
            "Epoch: 12 Accuracy: 0.9807999730110168\n",
            "Epoch: 13 Accuracy: 0.9818000197410583\n",
            "Epoch: 14 Accuracy: 0.9819999933242798\n",
            "Epoch: 15 Accuracy: 0.9810000061988831\n",
            "Epoch: 16 Accuracy: 0.9807999730110168\n",
            "Epoch: 17 Accuracy: 0.982200026512146\n",
            "Epoch: 18 Accuracy: 0.9811999797821045\n",
            "Epoch: 19 Accuracy: 0.982200026512146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im7NsuyfnUhU",
        "colab_type": "text"
      },
      "source": [
        "### Data Augmentation\n",
        "\n",
        "One last regularization technique is data augmentation. You can augment your training instances in such a way that a human would not be able to tell the difference, for example if the task is image classification you can try slightly rotating the images or changing the lighting conditions slightly. This will make the model more tolerant to minor changes and prevent overfitting.\n",
        "\n",
        "## Practical Guidelines\n",
        "\n",
        "Below are good default settings for a DNN:\n",
        "\n",
        "<table>\n",
        "  <tr><td><b>Initialization</b></td><td></td><td>He Initialization</td></tr>\n",
        "  <tr><td><b>Activation function</b></td><td></td><td>ELU</td></tr>\n",
        "  <tr><td><b>Normalization</b></td><td></td><td>Batch Normalization</td></tr>\n",
        "  <tr><td><b>Regularization</b></td><td></td><td>Dropout</td></tr>\n",
        "  <tr><td><b>Optimizer</b></td><td></td><td>Nesterov Accelerated Gradient</td></tr>\n",
        "  <tr><td><b>Learning rate schedule</b></td><td></td><td>None</td></tr>\n",
        "</table>\n",
        "\n",
        "Here is how the default configuration can be tweaked:\n",
        "\n",
        "- You can add learning rate scheduling if you are unable to find a good learning rate.\n",
        "\n",
        "- If your training set is too small, you can use data augmentation to add more training instances.\n",
        "\n",
        "- If you need a sparse model, you can add $\\ell_1$ regularization or you can use FTRL optimization as well.\n",
        "\n",
        "- If you want a fast model at runtime, you can drop Batch Normalization or replace the ELU activation function with ReLU.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "### 1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
        "\n",
        "No, the point of using He initialization is so that the variance of the outputs is the same as the variance of the inputs. Initializing all of the weights to the same value will have the same affect as having one neuron per layer, and it will not be possible to converge to a good solution during training.\n",
        "\n",
        "### 2. Is it okay to initialize the bias terms to 0?\n",
        "\n",
        "Yes, it is fine to initialize the bias terms to zero at first. You can even initialize them randomly like the weights.\n",
        "\n",
        "### 3. Name three advantages of the ELU activation function over ReLU.\n",
        "\n",
        "1. ELU can have negative values, which means that the mean of the output values will be closer to zero, which can help solve the vanishing gradients problem.\n",
        "\n",
        "2. Since ELU outputs negative values instead of zero when the output of the layer before activation is negative and they also have a nonzero gradient, which prevents neurons from going \"dead.\"\n",
        "\n",
        "3. ELU is differentiable for all possible input values, which means that Gradient Descent will converge faster.\n",
        "\n",
        "### 4. In which case would you want to use each of the following activation functions:\n",
        "\n",
        "#### ELU\n",
        "\n",
        "You can use ELU to help training converge faster or if the neural network's performance is degrading because some neurons are going \"dead.\"\n",
        "\n",
        "#### Leaky ReLU\n",
        "\n",
        "You can use leaky ReLU to prevent neurons from going \"dead\" and if you want prediction to be more performant than ELU, even if it means training may take longer.\n",
        "\n",
        "#### ReLU\n",
        "\n",
        "You can use ReLU as a default activation function for the hidden layers in a DNN. It generally performs well and is very fast to compute. Although leaky ReLU and ELU outperform ReLU, people use ReLU for its simplicity.\n",
        "\n",
        "#### tanh\n",
        "\n",
        "You can use the tanh activation function as an alternative for the output layer if the DNN is for a regression task whose outputs are between -1 and 1.\n",
        "\n",
        "#### logistic\n",
        "\n",
        "Similar to tanh, you can use the logistic activation function as an activation function for an output layer which needs to output probabilities that an instance belongs to a particular class.\n",
        "\n",
        "#### softmax\n",
        "\n",
        "Softmax activation function should be used for the output layer of the DNN to make predictions for a classifying instances into mutually exclusive classes.\n",
        "\n",
        "### 5. What may happen if you set the `momentum` hyperparamter to close to 1 (e.g. 0.999)?\n",
        "\n",
        "If the `momentum` hyperparameter is too close to one, then the momentum vector, $\\mathbf{m}$, will increase where the gradient is steep and not decrease when the gradient is less steep. This will cause the optimization algorithm to bounce around the optimum and not converge.\n",
        "\n",
        "### 6. Name three ways you can produce a sparse model.\n",
        "\n",
        "1. You can set all of the weights that are very small to zero.\n",
        "\n",
        "2. You can apply strong $\\ell_1$ regularization to the model.\n",
        "\n",
        "3. You can use FTRL instead of Adam optimization.\n",
        "\n",
        "### 7. Does dropout slow down training? Does it slow down inference (i.e. making predictions on new instances)?\n",
        "\n",
        "Dropout slows training typically by a factor of two since you need to train the DNN for more iterations on average. Dropout also requires TensorFlow to apply additional operations during training which increases training time. Dropout does affect the performance of inference.\n",
        "\n",
        "### 8. Deep Learning\n",
        "\n",
        "#### a. Build a DNN with 5 hidden layers of 100 neurons each, He initialization, and the ELU activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvRaW0lKZC7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function which builds the DNN with the specified settings.\n",
        "\n",
        "def build_dnn(X, n_outputs):\n",
        "  he_init = tf.variance_scaling_initializer()\n",
        "  n_hidden = 100\n",
        "  with tf.name_scope('dnn'):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden, kernel_initializer=he_init,\n",
        "                              activation=tf.nn.elu, name='hidden1')\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden, kernel_initializer=he_init,\n",
        "                              activation=tf.nn.elu, name='hidden2')\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden, kernel_initializer=he_init,\n",
        "                              activation=tf.nn.elu, name='hidden3')\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden, kernel_initializer=he_init,\n",
        "                              activation=tf.nn.elu, name='hidden4')\n",
        "    hidden5 = tf.layers.dense(hidden4, n_hidden, kernel_initializer=he_init,\n",
        "                              activation=tf.nn.elu, name='hidden5')\n",
        "    logits = tf.layers.dense(hidden5, n_outputs, name='outputs')\n",
        "  return hidden1, hidden2, hidden3, hidden4, hidden5, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugOt_-Yaj4bK",
        "colab_type": "text"
      },
      "source": [
        "#### b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4. You will need a softmax output layer with five neurons, and make sure to save checkpoints at regular intervals and save the final model so you can use it later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUMCyQr7tGzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the digits from MNIST that are in the range of 0 to 4\n",
        "\n",
        "X_train_01234 = X_train[y_train < 5]\n",
        "y_train_01234 = y_train[y_train < 5]\n",
        "\n",
        "X_valid_01234 = X_valid[y_valid < 5]\n",
        "y_valid_01234 = y_valid[y_valid < 5]\n",
        "\n",
        "X_test_01234 = X_test[y_test < 5]\n",
        "y_test_01234 = y_test[y_test < 5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmoBXNi1upEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function for building the TensorFlow graph for the classification\n",
        "# task\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_outputs = 5\n",
        "root_logdir = 'logs'\n",
        "\n",
        "def build_graph(X, y, learning_rate=0.001, beta1=0.9, beta2=0.999):\n",
        "  hidden1, hidden2, hidden3, hidden4, hidden5, logits = build_dnn(X, n_outputs)\n",
        "\n",
        "  with tf.name_scope('loss'):\n",
        "    x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                             logits=logits)\n",
        "    loss = tf.reduce_mean(x_entropy, name='loss')\n",
        "\n",
        "  with tf.name_scope('train'):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1,\n",
        "                                       beta2=beta2)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "    \n",
        "  with tf.name_scope('eval'):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "  now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
        "  logdir = '{}/run-{}/'.format(root_logdir, now)\n",
        "  with tf.name_scope('saver'):\n",
        "    saver = tf.train.Saver()\n",
        "    loss_summary = tf.summary.scalar('Loss', loss)\n",
        "    accuracy_summary = tf.summary.scalar('Accuracy', accuracy)\n",
        "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
        "    \n",
        "  with tf.name_scope('init'):\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "  return init, saver, training_op, loss, accuracy, loss_summary, \\\n",
        "    accuracy_summary, file_writer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNwgtDor3wgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function for training the DNN.\n",
        "\n",
        "import os\n",
        "\n",
        "model_path = 'mnist_model_01234.ckpt'\n",
        "n_epochs = 500\n",
        "\n",
        "def train_dnn(X, y, init, saver, training_op, loss, accuracy, loss_summary,\n",
        "              accuracy_summary, file_writer, n_epochs=100, batch_size=50):\n",
        "  with tf.Session() as sess:\n",
        "    if os.path.isfile(model_path):\n",
        "      saver.restore(sess, model_path)\n",
        "      with open('{}.epoch'.format(model_path)) as f:\n",
        "        start_epoch = int(f.read())\n",
        "    else:\n",
        "      sess.run(init)\n",
        "      start_epoch = 0\n",
        "      \n",
        "    best_loss = None\n",
        "    rounds_since_best_loss = 0\n",
        "    \n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "      if epoch % 10 == 0:\n",
        "        saver.save(sess, model_path)\n",
        "        with open('{}.epoch'.format(model_path), 'w') as f:\n",
        "          f.write(str(epoch))\n",
        "          f.close()\n",
        "          \n",
        "      for X_batch, y_batch in shuffle_batch(X_train_01234, y_train_01234,\n",
        "                                            batch_size):\n",
        "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        \n",
        "      loss_summary_str = loss_summary.eval(\n",
        "          feed_dict={X: X_valid_01234, y: y_valid_01234})\n",
        "      acc_summary_str = accuracy_summary.eval(\n",
        "          feed_dict={X: X_valid_01234, y: y_valid_01234})\n",
        "      file_writer.add_summary(loss_summary_str, epoch)\n",
        "      file_writer.add_summary(acc_summary_str, epoch)\n",
        "      \n",
        "      if epoch == 0:\n",
        "        best_loss = loss.eval(feed_dict={X: X_train_01234, y: y_train_01234})\n",
        "      elif epoch % 5 == 0:\n",
        "        loss_val = loss.eval(feed_dict={X: X_train_01234, y: y_train_01234})\n",
        "        if loss_val < best_loss:\n",
        "          best_loss = loss_val\n",
        "          rounds_since_best_loss = 0\n",
        "        else:\n",
        "          rounds_since_best_loss += 1\n",
        "          if rounds_since_best_loss == 4:\n",
        "            break\n",
        "            \n",
        "      if epoch % 10 == 0:\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid_01234,\n",
        "                                                y: y_valid_01234})\n",
        "        print('Epoch: {} Accuracy: {}'.format(epoch, accuracy_val))\n",
        "        \n",
        "    saver.save(sess, model_path)\n",
        "    with open('{}.epoch'.format(model_path), 'w') as f:\n",
        "      f.write(str(epoch))\n",
        "      f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojgf4HluosHm",
        "colab_type": "code",
        "outputId": "1319a479-4f65-44e8-a1b1-971956c32d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Training the model\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "init, saver, training_op, loss, accuracy, loss_summary, accuracy_summary, \\\n",
        "  file_writer = build_graph(X, y)\n",
        "\n",
        "train_dnn(X, y, init, saver, training_op, loss, accuracy, loss_summary,\n",
        "          accuracy_summary, file_writer, n_epochs=500, batch_size=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Accuracy: 0.9757623076438904\n",
            "Epoch: 10 Accuracy: 0.989835798740387\n",
            "Epoch: 20 Accuracy: 0.9917904734611511\n",
            "Epoch: 30 Accuracy: 0.9937450885772705\n",
            "Epoch: 40 Accuracy: 0.9906176924705505\n",
            "Epoch: 50 Accuracy: 0.9929632544517517\n",
            "Epoch: 60 Accuracy: 0.9937450885772705\n",
            "Epoch: 70 Accuracy: 0.9941360354423523\n",
            "Epoch: 80 Accuracy: 0.9941360354423523\n",
            "Epoch: 90 Accuracy: 0.9941360354423523\n",
            "Epoch: 100 Accuracy: 0.9941360354423523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz1K4e0SCO8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XacsvgqaCRkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw(\n",
        "  'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(root_logdir))\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa-ddAPNCTsf",
        "colab_type": "code",
        "outputId": "ad48bd85-35c7-4506-818c-24529225e1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "  \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://f2880a28.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXeBU27Fc6Z4",
        "colab_type": "text"
      },
      "source": [
        "#### c. Tune the hyperparameters with cross-validation and see what precision you can achieve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W6vfEX84M3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Refactoring into the DNN defined above as an Scikit-Learn Estimator\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.exceptions import NotFittedError\n",
        "\n",
        "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
        "  def __init__(self, n_hidden=5, n_neurons=100, learning_rate=0.001,\n",
        "           beta1=0.9, beta2=0.999, batch_size=50,\n",
        "           batch_normalization_momentum=None, dropout_rate=None,\n",
        "           activation=tf.nn.elu, n_epochs=500, initializer=he_init):\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_neurons = n_neurons\n",
        "    self.learning_rate = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.batch_size = batch_size\n",
        "    self.batch_normalization_momentum = batch_normalization_momentum\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.activation = activation\n",
        "    self.n_epochs = n_epochs\n",
        "    self.initializer = initializer\n",
        "    self._sess = None\n",
        "    \n",
        "  def __del__(self):\n",
        "    if self._sess is not None:\n",
        "      self._sess.close()\n",
        "      \n",
        "  def _dnn(self, inputs):\n",
        "    for i in range(self.n_hidden):\n",
        "      if self.dropout_rate:\n",
        "        inputs = tf.layers.dropout(inputs, rate=self.dropout_rate,\n",
        "                                   training=self._training,\n",
        "                                   name='dropout{}'.format(i))\n",
        "      inputs = tf.layers.dense(inputs, self.n_neurons,\n",
        "                               name='hidden{}'.format(i),\n",
        "                               kernel_initializer=self.initializer)\n",
        "      if self.batch_normalization_momentum:\n",
        "        inputs = tf.layers.batch_normalization(\n",
        "            inputs, training=self._training,\n",
        "            momentum=self.batch_normalization_momentum,\n",
        "            name='batch_normal{}'.format(i))\n",
        "      inputs = self.activation(inputs)\n",
        "    return inputs\n",
        "    \n",
        "  def _build_graph(self, n_inputs, n_outputs):\n",
        "    reset_graph()\n",
        "    self._X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "    self._y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "    \n",
        "    if self.batch_normalization_momentum or self.dropout_rate:\n",
        "      self._training = tf.placeholder_with_default(False, shape=(),\n",
        "                                                   name='training')\n",
        "    else:\n",
        "      self._training = None\n",
        "      \n",
        "    dnn_outputs = self._dnn(self._X)\n",
        "    logits = tf.layers.dense(dnn_outputs, n_outputs,\n",
        "                             kernel_initializer=self.initializer,\n",
        "                             name='logits')\n",
        "    \n",
        "    self._y_proba = tf.nn.softmax(logits, name='y_proba')\n",
        "    \n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self._y,\n",
        "                                                              logits=logits)\n",
        "    self._loss = tf.reduce_mean(xentropy, name='loss')\n",
        "    \n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate,\n",
        "                                       beta1=self.beta1, beta2=self.beta2)\n",
        "    self._training_op = optimizer.minimize(self._loss)\n",
        "    self._extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    \n",
        "    correct = tf.nn.in_top_k(logits, self._y, 1)\n",
        "    self._accuracy = tf.reduce_mean(tf.cast(correct, tf.float32),\n",
        "                                    name=\"accuracy\")\n",
        "    \n",
        "    self._saver = tf.train.Saver()\n",
        "    self._loss_summary = tf.summary.scalar('Loss', self._loss)\n",
        "    self._accuracy_summary = tf.summary.scalar('Accuracy', self._accuracy)\n",
        "    now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
        "    logdir = '{}/run-{}/'.format(root_logdir, now)\n",
        "    self._file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
        "    \n",
        "    self._init = tf.global_variables_initializer()\n",
        "    \n",
        "    self._graph = tf.get_default_graph()\n",
        "    \n",
        "  def _get_model_params(self):\n",
        "    with self._graph.as_default():\n",
        "      gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: val\n",
        "            for (gvar, val) in zip(gvars, self._sess.run(gvars))}\n",
        "  \n",
        "  def _restore_model_params(self, model_params):\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {\n",
        "        gvar_name: self._graph.get_operation_by_name(\n",
        "            '{}/Assign'.format(gvar_name))\n",
        "        for gvar_name in gvar_names\n",
        "    }\n",
        "    init_values = {gvar_name: assign_op.inputs[1]\n",
        "                   for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name]\n",
        "                 for gvar_name in gvar_names}\n",
        "    self._sess.run(assign_ops, feed_dict=feed_dict)\n",
        "    \n",
        "  def _train(self, X_train, y_train):\n",
        "    self._sess.run(self._init)\n",
        "    \n",
        "    best_loss = None\n",
        "    rounds_since_best_loss = 0\n",
        "    best_params = None\n",
        "    \n",
        "    for epoch in range(self.n_epochs):\n",
        "      for X_batch, y_batch in shuffle_batch(X_train, y_train, self.batch_size):\n",
        "        feed_dict = {self._X: X_batch, self._y: y_batch}\n",
        "        if self._training is not None:\n",
        "          feed_dict[self._training] = True\n",
        "        self._sess.run(self._training_op, feed_dict=feed_dict)\n",
        "        if self._extra_update_ops:\n",
        "          self._sess.run(self._extra_update_ops, feed_dict=feed_dict)\n",
        "      \n",
        "      loss_summary_str = self._loss_summary.eval(\n",
        "          session=self._sess, feed_dict={self._X: X_train, self._y: y_train})\n",
        "      acc_summary_str = self._accuracy_summary.eval(\n",
        "          session=self._sess, feed_dict={self._X: X_train, self._y: y_train})\n",
        "      self._file_writer.add_summary(loss_summary_str, epoch)\n",
        "      self._file_writer.add_summary(acc_summary_str, epoch)\n",
        "      \n",
        "      if epoch == 0:\n",
        "        best_loss = self._loss.eval(\n",
        "            session=self._sess, feed_dict={self._X: X_train, self._y: y_train})\n",
        "        best_params = self._get_model_params()\n",
        "      elif epoch % 5 == 0:\n",
        "        loss_val = self._loss.eval(\n",
        "            session=self._sess, feed_dict={self._X: X_train, self._y: y_train})\n",
        "        if loss_val < best_loss:\n",
        "          best_loss = loss_val\n",
        "          rounds_since_best_loss = 0\n",
        "          best_params = self._get_model_params()\n",
        "        else:\n",
        "          rounds_since_best_loss += 1\n",
        "          if rounds_since_best_loss == 5:\n",
        "            self._restore_model_params(best_params)\n",
        "            break\n",
        "      \n",
        "  def save_model(self, model_path):\n",
        "    if self._sess is None:\n",
        "      raise NotFittedError()\n",
        "    self._saver.save(self._sess, model_path)\n",
        "    \n",
        "  def restore_model(self, model_path, n_inputs, n_outputs):\n",
        "    if self._sess is not None:\n",
        "      self._sess.close()\n",
        "    self._build_graph(n_inputs, n_outputs)\n",
        "    self._sess = tf.Session()\n",
        "    self._saver.restore(self._sess, model_path)\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "    if self._sess is None:\n",
        "      self._build_graph(X.shape[1], len(set(y)))\n",
        "      self._sess = tf.Session()\n",
        "    self._train(X, y)\n",
        "    return self\n",
        "    \n",
        "  def predict_proba(X, y=None):\n",
        "    if self._sess is None:\n",
        "      raise NotFittedError()\n",
        "    return self._y_proba.eval(session=self._sess,\n",
        "                              feed_dict={self._X: X, self._y: y})\n",
        "  \n",
        "  def predict(self, X, y=None):\n",
        "    y_proba = self.predict_proba(X, y)\n",
        "    return np.argmax(y_proba, axis=1)\n",
        "  \n",
        "  def score(self, X, y):\n",
        "    if self._sess is None:\n",
        "      raise NotFittedError()\n",
        "    return self._accuracy.eval(session=self._sess,\n",
        "                               feed_dict={self._X: X, self._y: y})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dqBXrmFLQNr",
        "colab_type": "code",
        "outputId": "559ccdfa-7397-4dde-af13-f62155a246fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'activation': [tf.nn.relu, tf.nn.elu, leaky_relu, tf.nn.tanh],\n",
        "    'learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
        "    'beta1': [0.9, 0.99, 0.999],\n",
        "    'beta2': [0.9, 0.99, 0.999],\n",
        "    'n_neurons': [70, 80, 90, 100, 110, 120, 130, 140, 150],\n",
        "    'batch_size': [60, 80, 100, 120, 140, 160, 180, 200],\n",
        "}\n",
        "\n",
        "rnd_search = RandomizedSearchCV(DNNClassifier(), param_grid, n_iter=40, cv=3)\n",
        "rnd_search.fit(X_train_01234, y_train_01234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "          estimator=DNNClassifier(activation=<function elu at 0x7f7baa3e6488>,\n",
              "       batch_normalization_momentum=None, batch_size=50, beta1=0.9,\n",
              "       beta2=0.999, dropout_rate=None,\n",
              "       initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x7f7b93c00ef0>,\n",
              "       learning_rate=0.001, n_epochs=500, n_hidden=5, n_neurons=100),\n",
              "          fit_params=None, iid='warn', n_iter=40, n_jobs=None,\n",
              "          param_distributions={'activation': [<function relu at 0x7f7baa3989d8>, <function elu at 0x7f7baa3e6488>, <function leaky_relu at 0x7f7b9f805d08>, <function tanh at 0x7f7baa56ba60>], 'learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01], 'beta1': [0.9, 0.99, 0.999], 'beta2': [0.9, 0.99, 0.999], 'n_neurons': [70, 80, 90, 100, 110, 120, 130, 140, 150], 'batch_size': [60, 80, 100, 120, 140, 160, 180, 200]},\n",
              "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "          return_train_score='warn', scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llsgDeV98qlI",
        "colab_type": "code",
        "outputId": "32d74c5d-eb8d-4de8-b7a1-22dff92e6cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print('Best score:', rnd_search.best_score_)\n",
        "print('Best params:', rnd_search.best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.9901205472628479\n",
            "Best params: {'n_neurons': 110, 'learning_rate': 0.001, 'beta2': 0.99, 'beta1': 0.9, 'batch_size': 140, 'activation': <function leaky_relu at 0x7f7b9f805d08>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMjE_dqIsao0",
        "colab_type": "code",
        "outputId": "0cda5932-03a5-4b7f-f506-ed8ce78eaae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dnn_clf = rnd_search.best_estimator_\n",
        "dnn_clf.score(X_test_01234, y_test_01234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99494064"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptN6frZtsxDm",
        "colab_type": "text"
      },
      "source": [
        "#### d. Now add batch normalization and compare the learning curves. Is it converging faster than before?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyV33qfZt6Xt",
        "colab_type": "code",
        "outputId": "6df064d8-1d9c-498f-9621-bd49dbb016fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "dnn_clf = DNNClassifier(batch_normalization_momentum=0.95, n_neurons=110,\n",
        "                        learning_rate=0.001, beta2=0.99, beta1=0.9,\n",
        "                        batch_size=140, activation=leaky_relu)\n",
        "dnn_clf.fit(X_train_01234, y_train_01234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DNNClassifier(activation=<function leaky_relu at 0x7f7b9f805d08>,\n",
              "       batch_normalization_momentum=0.95, batch_size=140, beta1=0.9,\n",
              "       beta2=0.99, dropout_rate=None,\n",
              "       initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x7f7b93c00ef0>,\n",
              "       learning_rate=0.001, n_epochs=500, n_hidden=5, n_neurons=110)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-s_fgPGyU9O",
        "colab_type": "code",
        "outputId": "34e69f83-f534-43be-f884-0befdd7ece80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dnn_clf.score(X_test_01234, y_test_01234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.995719"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDptOKQeuWsE",
        "colab_type": "code",
        "outputId": "06c1fd6e-cc0e-4897-9eec-b045cc9a7c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dnn_clf.score(X_train_01234, y_train_01234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLWB_XKKwDY7",
        "colab_type": "text"
      },
      "source": [
        "Examining TensorBoard shows that the model did converge faster. It also appears that the model is overfitting the training set now.\n",
        "\n",
        "#### e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
        "\n",
        "The model is overfitting. Below is code which retrains the model using dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scotZ3TuCBgN",
        "colab_type": "code",
        "outputId": "b4e68a2f-bc8c-4614-e3d1-89ad1861e2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "dnn_clf = DNNClassifier(batch_normalization_momentum=0.95, n_neurons=110,\n",
        "                        learning_rate=0.001, beta2=0.99, beta1=0.9,\n",
        "                        batch_size=140, activation=leaky_relu, dropout_rate=0.5)\n",
        "dnn_clf.fit(X_train_01234, y_train_01234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DNNClassifier(activation=<function leaky_relu at 0x7f7b9f805d08>,\n",
              "       batch_normalization_momentum=0.95, batch_size=140, beta1=0.9,\n",
              "       beta2=0.99, dropout_rate=0.5,\n",
              "       initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x7f7b93c00ef0>,\n",
              "       learning_rate=0.001, n_epochs=500, n_hidden=5, n_neurons=110)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8UtNIQIG9Sv",
        "colab_type": "code",
        "outputId": "24f90c36-2dc0-42ec-925f-ee5d81c4d1fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dnn_clf.score(X_test_01234, y_test_01234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9939677"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogjOkVO6HAMl",
        "colab_type": "code",
        "outputId": "9e237a55-b6ab-4569-aad3-96ed670acb74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dnn_clf.score(X_train_01234, y_train_01234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99703974"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnmUck15HH8G",
        "colab_type": "text"
      },
      "source": [
        "The model is overfitting less, but it is also performing worse than without dropout. I could run another parameter search using `RandomizedSearchCV` to tune the hyperparameters. In the interest of time I will omit that and use the model without dropout. Below I will retrain the model without dropout and then save it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lGUIHr3IGlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = './my_mnist_model_01234.ckpt'\n",
        "\n",
        "dnn_clf = DNNClassifier(n_neurons=110, learning_rate=0.001, beta2=0.99,\n",
        "                        beta1=0.9, batch_size=140, activation=leaky_relu)\n",
        "dnn_clf.fit(X_train_01234, y_train_01234)\n",
        "dnn_clf.save_model(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXu15dAaJ80X",
        "colab_type": "text"
      },
      "source": [
        "### 9. Transfer learning.\n",
        "\n",
        "#### a. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with the new one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPx7-aOubqtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Refactoring the class from the previous exercise to have another\n",
        "# hyperparameter.\n",
        "\n",
        "class TransferDNNClassifier(DNNClassifier):\n",
        "  def __init__(self, n_frozen=0, **kwargs):\n",
        "    DNNClassifier.__init__(self, **kwargs)\n",
        "    self.n_frozen = n_frozen\n",
        "    \n",
        "  def _dnn(self, inputs):\n",
        "    for i in range(self.n_hidden):\n",
        "      if self.dropout_rate:\n",
        "        inputs = tf.layers.dropout(inputs, rate=self.dropout_rate,\n",
        "                                   training=self._training,\n",
        "                                   name='dropout{}'.format(i))\n",
        "      inputs = tf.layers.dense(inputs, self.n_neurons,\n",
        "                               name='hidden{}'.format(i),\n",
        "                               kernel_initializer=self.initializer)\n",
        "      inputs = self.activation(inputs)\n",
        "      if i + 1 == self.n_frozen:\n",
        "        inputs = tf.stop_gradient(inputs)\n",
        "    return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noOZfJCWiaBf",
        "colab_type": "code",
        "outputId": "7793bef8-d126-49a6-9605-b2cba0b10acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tdnn_clf = TransferDNNClassifier(n_neurons=110, learning_rate=0.001,\n",
        "                                 beta2=0.99, beta1=0.9, batch_size=140,\n",
        "                                 activation=leaky_relu, n_frozen=5)\n",
        "tdnn_clf.restore_model(model_path, 28 * 28, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_mnist_model_01234.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GOdQJQyipX0",
        "colab_type": "text"
      },
      "source": [
        "#### b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZu3Bf9aj_tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clock class for timing training.\n",
        "\n",
        "import time\n",
        "\n",
        "class Clock:\n",
        "  def __init__(self):\n",
        "    self.start_time = None\n",
        "  def start(self):\n",
        "    self.start_time = time.time()\n",
        "    return self\n",
        "  def stop(self):\n",
        "    dt = time.time() - self.start_time\n",
        "    self.start_time = None\n",
        "    h, m, s = int(dt // 3600), int(dt % 3600) // 60, dt % 60\n",
        "    return '{}h {}m {:.3f}s'.format(h, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVz4M8LmkEK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the MNIST data by first getting all of the instances\n",
        "# which are 5 through 9.\n",
        "\n",
        "X_train_56789 = X_train[y_train >= 5]\n",
        "y_train_56789 = y_train[y_train >= 5] - 5\n",
        "\n",
        "X_test_56789 = X_test[y_test >= 5]\n",
        "y_test_56789 = y_test[y_test >= 5] - 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCJ2p-RrlrUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting a training set with 100 instances from each class.\n",
        "\n",
        "X_train_transfer = []\n",
        "y_train_transfer = []\n",
        "\n",
        "counts = {i: 0 for i in range(5)}\n",
        "for data, label in zip(X_train_56789, y_train_56789):\n",
        "  counts[label] += 1\n",
        "  if counts[label] <= 100:\n",
        "    X_train_transfer.append(data)\n",
        "    y_train_transfer.append(label)\n",
        "\n",
        "X_train_transfer = np.array(X_train_transfer, dtype=np.float32)\n",
        "y_train_transfer = np.array(y_train_transfer, dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWwGsSkMnH27",
        "colab_type": "code",
        "outputId": "de14763a-ee3a-48be-aff0-cea6d474e8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training the model and timing how long it takes to train it.\n",
        "\n",
        "clock = Clock().start()\n",
        "tdnn_clf.fit(X_train_transfer, y_train_transfer)\n",
        "clock.stop()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0h 0m 8.735s'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5qHr3sdnksE",
        "colab_type": "code",
        "outputId": "bca4e6b4-1596-47d4-8135-751a0ab4fb1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The accuracy is not that great, but this is expected given we\n",
        "# are only retraining the output layer.\n",
        "\n",
        "tdnn_clf.score(X_test_56789, y_test_56789)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.62353426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cuGF9wBqAJD",
        "colab_type": "text"
      },
      "source": [
        "#### c. Try caching the frozen layers instead and train the model again: how much faster is it now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIuej6kYp-80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Refactoring the DNNClassiifer to cache the frozen layers during traning\n",
        "\n",
        "class CachedDNNClassifier(DNNClassifier):\n",
        "  def __init__(self, n_frozen=0, **kwargs):\n",
        "    DNNClassifier.__init__(self, **kwargs)\n",
        "    self.n_frozen = n_frozen\n",
        "    \n",
        "  def _dnn(self, inputs):\n",
        "    self._cached_layer = None\n",
        "    for i in range(self.n_hidden):\n",
        "      if self.dropout_rate:\n",
        "        inputs = tf.layers.dropout(inputs, rate=self.dropout_rate,\n",
        "                                   training=self._training,\n",
        "                                   name='dropout{}'.format(i))\n",
        "      inputs = tf.layers.dense(inputs, self.n_neurons,\n",
        "                               name='hidden{}'.format(i),\n",
        "                               kernel_initializer=self.initializer)\n",
        "      inputs = self.activation(inputs)\n",
        "      if i + 1 == self.n_frozen:\n",
        "        self._cached_layer = inputs\n",
        "        inputs = tf.stop_gradient(inputs)\n",
        "    return inputs\n",
        "  \n",
        "  def _train(self, X_train, y_train):\n",
        "    self._sess.run(self._init)\n",
        "    \n",
        "    best_loss = None\n",
        "    rounds_since_best_loss = 0\n",
        "    best_params = None\n",
        "    \n",
        "    X_input = X_train\n",
        "    if self._cached_layer is not None:\n",
        "      X_input = self._sess.run(self._cached_layer, feed_dict={self._X: X_train,\n",
        "                                                              self._y: y_train})\n",
        "    \n",
        "    for epoch in range(self.n_epochs):\n",
        "      for X_batch, y_batch in shuffle_batch(X_input, y_train, self.batch_size):\n",
        "        inputs = self._X if self._cached_layer is None else self._cached_layer\n",
        "        feed_dict = {inputs: X_batch, self._y: y_batch}\n",
        "        if self._training is not None:\n",
        "          feed_dict[self._training] = True\n",
        "        self._sess.run(self._training_op, feed_dict=feed_dict)\n",
        "      \n",
        "      loss_summary_str = self._loss_summary.eval(\n",
        "          session=self._sess, feed_dict={self._X: X_train, self._y: y_train})\n",
        "      acc_summary_str = self._accuracy_summary.eval(\n",
        "          session=self._sess, feed_dict={self._X: X_train, self._y: y_train})\n",
        "      self._file_writer.add_summary(loss_summary_str, epoch)\n",
        "      self._file_writer.add_summary(acc_summary_str, epoch)\n",
        "      \n",
        "      if epoch == 0:\n",
        "        best_loss = self._loss.eval(\n",
        "            session=self._sess, feed_dict={self._X: X_train, self._y: y_train})\n",
        "        best_params = self._get_model_params()\n",
        "      elif epoch % 5 == 0:\n",
        "        loss_val = self._loss.eval(\n",
        "            session=self._sess, feed_dict={self._X: X_train, self._y: y_train})\n",
        "        if loss_val < best_loss:\n",
        "          best_loss = loss_val\n",
        "          rounds_since_best_loss = 0\n",
        "          best_params = self._get_model_params()\n",
        "        else:\n",
        "          rounds_since_best_loss += 1\n",
        "          if rounds_since_best_loss == 5:\n",
        "            self._restore_model_params(best_params)\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhOUXv6FsxXt",
        "colab_type": "code",
        "outputId": "e854479f-7970-4496-8adb-7277971ee548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cdnn_clf = CachedDNNClassifier(n_neurons=110, learning_rate=0.01,\n",
        "                               beta2=0.99, beta1=0.9, batch_size=140,\n",
        "                               activation=leaky_relu, n_frozen=5)\n",
        "cdnn_clf.restore_model(model_path, 28 * 28, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_mnist_model_01234.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJRqbg4Ds4-i",
        "colab_type": "code",
        "outputId": "39a4e91a-3a9a-45c6-e928-bba18d51cc68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training the model and timing how long it takes to train it. The model\n",
        "# takes less time to train with caching.\n",
        "\n",
        "clock = Clock().start()\n",
        "cdnn_clf.fit(X_train_transfer, y_train_transfer)\n",
        "clock.stop()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0h 0m 6.521s'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t26b91WsGJCy",
        "colab_type": "code",
        "outputId": "969527f4-7e4e-43a1-b051-b7ac4701d5b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cdnn_clf.score(X_test_56789, y_test_56789)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6877186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7H5HCQeF3vA",
        "colab_type": "text"
      },
      "source": [
        "#### d. Try again reusing four hidden layers instead of five. Can you achieve a higher precision?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWNvc-F3GATj",
        "colab_type": "code",
        "outputId": "acda8844-1d28-48d4-94d6-dc04b1db8e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cdnn_clf = CachedDNNClassifier(n_neurons=110, learning_rate=0.01,\n",
        "                               beta2=0.99, beta1=0.9, batch_size=140,\n",
        "                               activation=leaky_relu, n_frozen=4)\n",
        "cdnn_clf.restore_model(model_path, 28 * 28, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_mnist_model_01234.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4drqEbaGEoV",
        "colab_type": "code",
        "outputId": "c239f12f-e693-4cf2-af24-ef1b4d9a6c60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cdnn_clf.fit(X_train_transfer, y_train_transfer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CachedDNNClassifier(n_frozen=4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slyaGtNPGTE_",
        "colab_type": "code",
        "outputId": "236523da-0f13-4b82-aeb4-2da74e6fe525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cdnn_clf.score(X_test_56789, y_test_56789)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7228965"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjQ1EbIrGbzR",
        "colab_type": "text"
      },
      "source": [
        "As we can see, reusing only four out of five hidden layers increased accuracy by 10%.\n",
        "\n",
        "#### e. Now unfreeze two hidden layers and continue training, can you get the model to perform even better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kmxjFjPGprM",
        "colab_type": "code",
        "outputId": "cc9aa14b-9fd9-405e-fadb-d1c8025879c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cdnn_clf = CachedDNNClassifier(n_neurons=110, learning_rate=0.01,\n",
        "                               beta2=0.99, beta1=0.9, batch_size=50,\n",
        "                               activation=tf.nn.elu, n_frozen=3)\n",
        "cdnn_clf.restore_model(model_path, 28 * 28, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_mnist_model_01234.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_uF7QGQGtuo",
        "colab_type": "code",
        "outputId": "eaecff0b-b7be-4714-e96b-43f3281c5056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cdnn_clf.fit(X_train_transfer, y_train_transfer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CachedDNNClassifier(n_frozen=3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8VrKffyGxE6",
        "colab_type": "code",
        "outputId": "da6b66c2-f581-4144-82b2-0e923c0ad241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cdnn_clf.score(X_test_56789, y_test_56789)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8514709"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i538tZx9Gz7R",
        "colab_type": "text"
      },
      "source": [
        "The model does perform slightly better after unfreezing two hidden layers. Increasing the learning rate also helped the model converge faster. ELU also helped the model achieve some better performance.\n",
        "\n",
        "### 10. Pretraining on an auxilary task.\n",
        "\n",
        "#### a. Build two DNNs (let's call them DNN A and DNN B), both similar to the one you built earlier but without the output layer: each DNN has 5 hidden layers of 100 neurons each, He initialization, and ELU activation. Next add one more hidden layer with 10 units on top of both DNNs using TensorFlow's `concat()` function, then add an output layer with a single neuron using the logistic activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqSm6M2vFQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "12262dfe-82fa-4ba4-9c9a-c51a7472dffb"
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "\n",
        "X_a = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X_a')\n",
        "X_b = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X_b')\n",
        "\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "def hidden_layer(inputs, name, n_neurons=100):\n",
        "  return tf.layers.dense(inputs, n_neurons, kernel_initializer=he_init,\n",
        "                         activation=tf.nn.elu, name=name)\n",
        "\n",
        "hidden_1a = hidden_layer(X_a, name='hidden_1a')\n",
        "hidden_2a = hidden_layer(hidden_1a, name='hidden_2a')\n",
        "hidden_3a = hidden_layer(hidden_2a, name='hidden_3a')\n",
        "hidden_4a = hidden_layer(hidden_3a, name='hidden_4a')\n",
        "hidden_5a = hidden_layer(hidden_4a, name='hidden_5a')\n",
        "\n",
        "hidden_1b = hidden_layer(X_b, name='hidden_1b')\n",
        "hidden_2b = hidden_layer(hidden_1b, name='hidden_2b')\n",
        "hidden_3b = hidden_layer(hidden_2b, name='hidden_3b')\n",
        "hidden_4b = hidden_layer(hidden_3b, name='hidden_4b')\n",
        "hidden_5b = hidden_layer(hidden_4b, name='hidden_5b')\n",
        "\n",
        "concat = tf.concat([hidden_5a, hidden_5b], axis=1, name='concat')\n",
        "\n",
        "hidden_merged = hidden_layer(concat, n_neurons=10, name='hidden_merged')\n",
        "logits = tf.layers.dense(hidden_merged, 1, kernel_initializer=he_init)\n",
        "\n",
        "y_float = tf.cast(y, tf.float32)\n",
        "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_float,\n",
        "                                                   logits=logits)\n",
        "loss = tf.reduce_mean(xentropy, name='loss')\n",
        "\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)\n",
        "correct = tf.equal(y_pred, y)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbKh59Jq2ZXU",
        "colab_type": "text"
      },
      "source": [
        "#### b. Split the MNIST training set into two sets: split #1 should contain 55,000 images, and split #2 should contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CreSQdb23N8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading MNIST again.\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY84rhKI4ang",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the training set into split #1 and split #2.\n",
        "\n",
        "X_train1 = []\n",
        "y_train1 = []\n",
        "\n",
        "X_train2 = []\n",
        "y_train2 = []\n",
        "\n",
        "counts = {i: 0 for i in range(10)}\n",
        "\n",
        "for data, label in zip(X_train, y_train):\n",
        "  if counts[label] < 500:\n",
        "    counts[label] += 1\n",
        "    X_train2.append(data)\n",
        "    y_train2.append(label)\n",
        "  else:\n",
        "    X_train1.append(data)\n",
        "    y_train1.append(label)\n",
        "    \n",
        "X_train1 = np.array(X_train1, dtype=np.float32)\n",
        "y_train1 = np.array(y_train1, dtype=np.int32)\n",
        "\n",
        "X_train2 = np.array(X_train2, dtype=np.float32)\n",
        "y_train2 = np.array(y_train2, dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0L65H-a7drV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a function for producing batches from split #1 where each\n",
        "# instance is a pair of digits in X_train1. Each batch has an equal number\n",
        "# of pairs that are the same digit and pairs that are different digits.\n",
        "\n",
        "def generate_batch(X, y, batch_size):\n",
        "  rand_idx1 = np.random.permutation(len(X))\n",
        "  rand_idx2 = np.random.permutation(len(X))\n",
        "  \n",
        "  X_batch1 = []\n",
        "  X_batch2 = []\n",
        "  y_batch = []\n",
        "  \n",
        "  same_classes = 0\n",
        "  diff_classes = 0\n",
        "  \n",
        "  for i, j in zip(rand_idx1, rand_idx2):\n",
        "    data1, label1 = X[i], y[i]\n",
        "    data2, label2 = X[j], y[j]\n",
        "    \n",
        "    if label1 == label2:\n",
        "      if same_classes < batch_size / 2:\n",
        "        same_classes += 1\n",
        "        X_batch1.append(data1)\n",
        "        X_batch2.append(data2)\n",
        "        y_batch.append([0])\n",
        "    else:\n",
        "      if diff_classes < batch_size / 2:\n",
        "        diff_classes += 1\n",
        "        X_batch1.append(data1)\n",
        "        X_batch2.append(data2)\n",
        "        y_batch.append([1])\n",
        "    \n",
        "    if len(X_batch1) == batch_size:\n",
        "      break\n",
        "  rand_idx = np.random.permutation(len(X_batch1))\n",
        "  return np.array(X_batch1)[rand_idx], \\\n",
        "         np.array(X_batch2)[rand_idx], \\\n",
        "         np.array(y_batch)[rand_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jguMzL4pAt2I",
        "colab_type": "text"
      },
      "source": [
        "#### c. Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR5jQpgOBqFx",
        "colab_type": "code",
        "outputId": "9ef5d050-a66a-41c6-94c4-f48ccf7f38d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "# Training the DNNs using early stopping.\n",
        "\n",
        "model_path = 'dual_dnn_model'\n",
        "n_epochs = 500\n",
        "batch_size = 500\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  \n",
        "  best_loss = None\n",
        "  rounds_since_best_loss = 0\n",
        "  \n",
        "  for epoch in range(n_epochs):\n",
        "    for _ in range(len(X_train1) // batch_size):\n",
        "      X_batch1, X_batch2, y_batch = generate_batch(X_train1, y_train1,\n",
        "                                                   batch_size)\n",
        "      sess.run(training_op,\n",
        "               feed_dict={X_a: X_batch1, X_b: X_batch2, y: y_batch})\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "      acc_val = accuracy.eval(feed_dict={X_a: X_batch1, X_b: X_batch2,\n",
        "                                         y: y_batch})\n",
        "      loss_val = loss.eval(feed_dict={X_a: X_batch1, X_b: X_batch2,\n",
        "                                      y: y_batch})\n",
        "      print('Epoch: {} Loss: {} Accuracy: {}'.format(epoch, loss_val, acc_val))\n",
        "      if epoch == 0:\n",
        "        best_loss = loss_val\n",
        "      elif loss_val < best_loss:\n",
        "        best_loss = loss_val\n",
        "        rounds_since_best_loss = 0\n",
        "        saver.save(sess, model_path)\n",
        "      else:\n",
        "        rounds_since_best_loss += 1\n",
        "        if rounds_since_best_loss == 5:\n",
        "          break\n",
        "  else:\n",
        "    saver.save(sess, model_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 0.6925538778305054 Accuracy: 0.5099999904632568\n",
            "Epoch: 5 Loss: 0.42443645000457764 Accuracy: 0.8299999833106995\n",
            "Epoch: 10 Loss: 0.32438501715660095 Accuracy: 0.8479999899864197\n",
            "Epoch: 15 Loss: 0.30453580617904663 Accuracy: 0.8700000047683716\n",
            "Epoch: 20 Loss: 0.20559610426425934 Accuracy: 0.9240000247955322\n",
            "Epoch: 25 Loss: 0.2321556955575943 Accuracy: 0.9160000085830688\n",
            "Epoch: 30 Loss: 0.21768558025360107 Accuracy: 0.906000018119812\n",
            "Epoch: 35 Loss: 0.14439666271209717 Accuracy: 0.9480000138282776\n",
            "Epoch: 40 Loss: 0.17752383649349213 Accuracy: 0.9279999732971191\n",
            "Epoch: 45 Loss: 0.12383560091257095 Accuracy: 0.9419999718666077\n",
            "Epoch: 50 Loss: 0.1052306592464447 Accuracy: 0.9599999785423279\n",
            "Epoch: 55 Loss: 0.101117342710495 Accuracy: 0.9639999866485596\n",
            "Epoch: 60 Loss: 0.10225614160299301 Accuracy: 0.9639999866485596\n",
            "Epoch: 65 Loss: 0.07068835198879242 Accuracy: 0.9639999866485596\n",
            "Epoch: 70 Loss: 0.06899896264076233 Accuracy: 0.9779999852180481\n",
            "Epoch: 75 Loss: 0.07025179266929626 Accuracy: 0.9679999947547913\n",
            "Epoch: 80 Loss: 0.08179041743278503 Accuracy: 0.9679999947547913\n",
            "Epoch: 85 Loss: 0.08949308097362518 Accuracy: 0.9679999947547913\n",
            "Epoch: 90 Loss: 0.03269192576408386 Accuracy: 0.9919999837875366\n",
            "Epoch: 95 Loss: 0.04489409923553467 Accuracy: 0.9860000014305115\n",
            "Epoch: 100 Loss: 0.04439261183142662 Accuracy: 0.984000027179718\n",
            "Epoch: 105 Loss: 0.06805920600891113 Accuracy: 0.9760000109672546\n",
            "Epoch: 110 Loss: 0.04596066102385521 Accuracy: 0.9860000014305115\n",
            "Epoch: 115 Loss: 0.04562963545322418 Accuracy: 0.9879999756813049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nITgWVnjcMC1",
        "colab_type": "code",
        "outputId": "797bef1d-df29-4e94-b61e-60efebd820e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Restoring the model and calculating the test set accuracy.\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, model_path)\n",
        "  X_batch1, X_batch2, y_batch = generate_batch(X_train, y_train, len(X_train))\n",
        "  train_acc_val = accuracy.eval(feed_dict={X_a: X_batch1, X_b: X_batch2,\n",
        "                                           y: y_batch})\n",
        "  X_batch1, X_batch2, y_batch = generate_batch(X_test, y_test, len(X_test))\n",
        "  test_acc_val = accuracy.eval(feed_dict={X_a: X_batch1, X_b: X_batch2,\n",
        "                                          y: y_batch})\n",
        "  print('Training set accuracy:', train_acc_val)\n",
        "  print('Test set accuracy:', test_acc_val)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from dual_dnn_model\n",
            "Training set accuracy: 0.97814053\n",
            "Test set accuracy: 0.97619045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVUSMSP7fUKl",
        "colab_type": "text"
      },
      "source": [
        "#### d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network with split #2 and see if you can achieve high performance despite having only 500 images per class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnsjwm0YfTRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "def hidden_layer(inputs, name, n_neurons=100):\n",
        "  return tf.layers.dense(inputs, n_neurons, kernel_initializer=he_init,\n",
        "                         activation=tf.nn.elu, name=name)\n",
        "\n",
        "hidden_1a = hidden_layer(X, name='hidden_1a')\n",
        "hidden_2a = hidden_layer(hidden_1a, name='hidden_2a')\n",
        "hidden_3a = hidden_layer(hidden_2a, name='hidden_3a')\n",
        "hidden_4a = hidden_layer(hidden_3a, name='hidden_4a')\n",
        "hidden_5a = hidden_layer(hidden_4a, name='hidden_5a')\n",
        "stop_grad = tf.stop_gradient(hidden_5a)\n",
        "\n",
        "logits = tf.layers.dense(stop_grad, 10, kernel_initializer=he_init)\n",
        "\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                          logits=logits)\n",
        "loss = tf.reduce_mean(xentropy, name='loss')\n",
        "\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope='hidden_[12345]a')\n",
        "reuse_saver = tf.train.Saver(reuse_vars)\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z82NXv01d3T0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "outputId": "94b62b35-7a67-4a1d-9367-547cb98c3d64"
      },
      "source": [
        "# Training DNN A after restoring the hidden layers to classify digits in the\n",
        "# MNIST dataset.\n",
        "\n",
        "batch_size = 100\n",
        "new_model_path = 'dual_dnn_retrained_model'\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  reuse_saver.restore(sess, model_path)\n",
        "\n",
        "  best_loss = None\n",
        "  rounds_since_best_loss = 0\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(X_train2, y_train2, batch_size):\n",
        "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "  \n",
        "    if epoch % 5 == 0:\n",
        "      loss_val = loss.eval(feed_dict={X: X_train2, y: y_train2})\n",
        "      if epoch % 10 == 0:\n",
        "        acc_val = accuracy.eval(feed_dict={X: X_train2, y: y_train2})\n",
        "        print('Epoch: {} Loss: {} Accuracy: {}'.format(epoch, loss_val,\n",
        "                                                       acc_val))\n",
        "      if epoch == 0:\n",
        "        best_loss = loss_val\n",
        "      elif loss_val < best_loss:\n",
        "        best_loss = loss_val\n",
        "        rounds_since_best_loss = 0\n",
        "        saver.save(sess, new_model_path)\n",
        "      else:\n",
        "        rounds_since_best_loss += 1\n",
        "        if rounds_since_best_loss == 5:\n",
        "          break\n",
        "  else:\n",
        "    saver.save(sess, new_model_path)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from dual_dnn_model\n",
            "Epoch: 0 Loss: 0.14949673414230347 Accuracy: 0.9602000117301941\n",
            "Epoch: 10 Loss: 0.10927245765924454 Accuracy: 0.968999981880188\n",
            "Epoch: 20 Loss: 0.09988775849342346 Accuracy: 0.9696000218391418\n",
            "Epoch: 30 Loss: 0.09442849457263947 Accuracy: 0.9718000292778015\n",
            "Epoch: 40 Loss: 0.09076770395040512 Accuracy: 0.9724000096321106\n",
            "Epoch: 50 Loss: 0.0879199281334877 Accuracy: 0.9735999703407288\n",
            "Epoch: 60 Loss: 0.0857444629073143 Accuracy: 0.973800003528595\n",
            "Epoch: 70 Loss: 0.0838107168674469 Accuracy: 0.974399983882904\n",
            "Epoch: 80 Loss: 0.08218634873628616 Accuracy: 0.9753999710083008\n",
            "Epoch: 90 Loss: 0.08075331151485443 Accuracy: 0.9757999777793884\n",
            "Epoch: 100 Loss: 0.07950214296579361 Accuracy: 0.9765999913215637\n",
            "Epoch: 110 Loss: 0.07833652198314667 Accuracy: 0.9765999913215637\n",
            "Epoch: 120 Loss: 0.07726486027240753 Accuracy: 0.9768000245094299\n",
            "Epoch: 130 Loss: 0.0763075053691864 Accuracy: 0.9771999716758728\n",
            "Epoch: 140 Loss: 0.07542519271373749 Accuracy: 0.9769999980926514\n",
            "Epoch: 150 Loss: 0.07451355457305908 Accuracy: 0.977400004863739\n",
            "Epoch: 160 Loss: 0.07370317727327347 Accuracy: 0.9775999784469604\n",
            "Epoch: 170 Loss: 0.07295164465904236 Accuracy: 0.977400004863739\n",
            "Epoch: 180 Loss: 0.07222463935613632 Accuracy: 0.9775999784469604\n",
            "Epoch: 190 Loss: 0.07153211534023285 Accuracy: 0.9782000184059143\n",
            "Epoch: 200 Loss: 0.07087071239948273 Accuracy: 0.9782000184059143\n",
            "Epoch: 210 Loss: 0.07025063782930374 Accuracy: 0.9782000184059143\n",
            "Epoch: 220 Loss: 0.06963296979665756 Accuracy: 0.9783999919891357\n",
            "Epoch: 230 Loss: 0.06905782222747803 Accuracy: 0.9787999987602234\n",
            "Epoch: 240 Loss: 0.0685039684176445 Accuracy: 0.9787999987602234\n",
            "Epoch: 250 Loss: 0.06792901456356049 Accuracy: 0.978600025177002\n",
            "Epoch: 260 Loss: 0.06741203367710114 Accuracy: 0.978600025177002\n",
            "Epoch: 270 Loss: 0.06687253713607788 Accuracy: 0.9787999987602234\n",
            "Epoch: 280 Loss: 0.06638097018003464 Accuracy: 0.978600025177002\n",
            "Epoch: 290 Loss: 0.06588944792747498 Accuracy: 0.9787999987602234\n",
            "Epoch: 300 Loss: 0.0654216855764389 Accuracy: 0.9787999987602234\n",
            "Epoch: 310 Loss: 0.06500695645809174 Accuracy: 0.9793999791145325\n",
            "Epoch: 320 Loss: 0.06451179832220078 Accuracy: 0.9797999858856201\n",
            "Epoch: 330 Loss: 0.0641169548034668 Accuracy: 0.9800000190734863\n",
            "Epoch: 340 Loss: 0.06366235017776489 Accuracy: 0.9800000190734863\n",
            "Epoch: 350 Loss: 0.06324537098407745 Accuracy: 0.9800000190734863\n",
            "Epoch: 360 Loss: 0.06284990161657333 Accuracy: 0.9801999926567078\n",
            "Epoch: 370 Loss: 0.06245168298482895 Accuracy: 0.9797999858856201\n",
            "Epoch: 380 Loss: 0.06211354210972786 Accuracy: 0.9805999994277954\n",
            "Epoch: 390 Loss: 0.061685241758823395 Accuracy: 0.9800000190734863\n",
            "Epoch: 400 Loss: 0.06136669963598251 Accuracy: 0.9805999994277954\n",
            "Epoch: 410 Loss: 0.06096336618065834 Accuracy: 0.9805999994277954\n",
            "Epoch: 420 Loss: 0.06060868501663208 Accuracy: 0.9815999865531921\n",
            "Epoch: 430 Loss: 0.060247767716646194 Accuracy: 0.9815999865531921\n",
            "Epoch: 440 Loss: 0.059913914650678635 Accuracy: 0.9815999865531921\n",
            "Epoch: 450 Loss: 0.05959947407245636 Accuracy: 0.9818000197410583\n",
            "Epoch: 460 Loss: 0.05923864245414734 Accuracy: 0.9815999865531921\n",
            "Epoch: 470 Loss: 0.05891764536499977 Accuracy: 0.9818000197410583\n",
            "Epoch: 480 Loss: 0.05861738324165344 Accuracy: 0.9818000197410583\n",
            "Epoch: 490 Loss: 0.05828869715332985 Accuracy: 0.9818000197410583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYrA5Fi1gQ65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "93536743-f27b-471f-94e6-24ddf72e129d"
      },
      "source": [
        "# Restoring the model and testing the performance. The model is slightly\n",
        "# overfitting, but considering it only was given 500 instances of each digit,\n",
        "# 96.7% accuracy is very good!\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, new_model_path)\n",
        "  train_acc_val = accuracy.eval(feed_dict={X: X_train2, y: y_train2})\n",
        "  test_acc_val = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "  print('Train set accuracy:', train_acc_val)\n",
        "  print('Test set accuracy:', test_acc_val)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from dual_dnn_retrained_model\n",
            "Train set accuracy: 0.9818\n",
            "Test set accuracy: 0.9667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}