{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercises.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ilct8w92RGg",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 14: Recurrent Neural Networks\n",
        "\n",
        "## Exercises\n",
        "\n",
        "### 1. Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?\n",
        "\n",
        "A sequence-to-sequence RNN is generally used for a model for predicting the future behavior of some input sequence. This can be used to create a predictive model for determining what the next word you are about to type might be.\n",
        "\n",
        "A sequence-to-vector RNN is good for classifying sequences, such as sentiment analysis. Also a sequence-to-vector RNN is used for finding the embeddings of a vocabulary of words in a denser, smaller vector space.\n",
        "\n",
        "In machine translation, vector-to-sequence RNN is good for decoding embeddings from input in one language into words in the target language. You can also use vector-to-sequence RNNs to generate captions for images.\n",
        "\n",
        "### 2. Why do people use encoder-decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
        "\n",
        "Most models for automatic translation encode vocabularies as a vector space where each word is a perpendicular unit vector. For a vocabulary of 50,000 words, this means the input sequences are vectors in a 50,000-dimensional space. Training a sequence-to-sequence RNN for machine translation with large vocabularies would take a large amount of memory, making it inefficient.\n",
        "\n",
        "Using an Encoder-Decoder model allows you to train the encoder to find a denser representation of the words, making training more efficient. Also training the model to find an embedding also helps the model learn what words are closely related to one another.\n",
        "\n",
        "### 3. How could you combine a convolutional neural network and an RNN to classify videos?\n",
        "\n",
        "Since a video is a sequence of images, you could create a convolutional neural network where each cell is a convolutional layers which learn feature maps for the images in each frame of the video. You could have it learn one set of feature maps for the input and another feature map for the previous output.\n",
        "\n",
        "### 4. What are the advantages of building an RNN using `dynamic_rnn()` rather than `static_rnn()`?\n",
        "\n",
        "The `static_rnn()` function creates new graph nodes for each time step in the sequence. This means that if you are processing a sequence with a large number of steps, you risk getting an OOM error when building your TensorFlow graph. The `dynamic_rnn()` function uses a while loop to perform multiple operations using the same nodes. The `dyanmic_rnn()` function also allows you to swap memory between the GPU and the CPU using the `swap_memory` parameter. It also accepts a single tensor as an input instead of a list of tensors for each time step in the sequence.\n",
        "\n",
        "### 5. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
        "\n",
        "The `dynamic_rnn()` function takes a `sequence_length` parameter which is a 1D tensor of integers which represent the sequence length of each of the inputs. Input sequences that are less than the maximum length sequence are padded with zeros.\n",
        "\n",
        "For variable-length output sequences, since it is not possible to determine how long each output will be prior to training, so each output sequence ends with an end of sequence (EOS) character to delimit the end.\n",
        "\n",
        "### 6. What is a common way to distribute training and execution of a deep RNN across multiple GPUs?\n",
        "\n",
        "In order to distribute an RNN across devices you cannot just simply call the `tf.device()` function. This is because TensorFlow's built in RNN cell classes like `BasicRNNCell` do not create the graph ncdes themselves, rather they are cell factories.\n",
        "\n",
        "In order to distribute an RNN across devices, you must define a new cell factory which actually creates each cell on a separate device. For an example, see `DeviceCellWrapper` in `RecurrentNeuralNetworks.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d643pw4y8dsb",
        "colab_type": "text"
      },
      "source": [
        "### 7. _Embedded Reber grammars_ are artificial grammars used to produce strings. Train an RNN to identify whether or not a string represents the grammar discussed in [Jenny Orr's introduction](http://www.willamette.edu/~gorr/classes/cs449/reber.html) or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar and 50% that do not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QDrhajr9aXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to generate Rebber grammars. Each key in the dict\n",
        "# is a node in the graph. Each element in the list is an adjacent node\n",
        "# in the graph.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Adjacency list for the graph.\n",
        "reber_grammar_graph = [\n",
        "  [('B', 1)],\n",
        "  [('T', 2), ('P', 3)],\n",
        "  [('S', 2), ('X', 4)],\n",
        "  [('T', 3), ('V', 5)],\n",
        "  [('X', 3), ('S', 6)],\n",
        "  [('P', 4), ('V', 6)],\n",
        "  [('E', None)],\n",
        "]\n",
        "\n",
        "def generate_reber_grammar():\n",
        "  idx = 0\n",
        "  result = ''\n",
        "  while idx is not None:\n",
        "    chars = reber_grammar_graph[idx]\n",
        "    c, idx = chars[np.random.randint(0, len(chars))]\n",
        "    result += c\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvdMLgsbJFUH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "828d0efc-e4ff-4fd7-9f2c-87d8ec72dc3b"
      },
      "source": [
        "generate_reber_grammar()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BTSXXTTTTTTTTTTTVVE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBTaT1UvKSQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function for generating embedded Reber grammar.\n",
        "\n",
        "REBER_GRAPH = 'reber_graph'\n",
        "\n",
        "embedded_reber_grammar_graph = [\n",
        "  [('B', 1)],\n",
        "  [('T', 2), ('P', 3)],\n",
        "  [(REBER_GRAPH, 4)],\n",
        "  [(REBER_GRAPH, 5)],\n",
        "  [('T', 6)],\n",
        "  [('P', 6)],\n",
        "  [('E', None)]\n",
        "]\n",
        "\n",
        "def generate_embedded_reber_grammar():\n",
        "  idx = 0\n",
        "  result = ''\n",
        "  while idx is not None:\n",
        "    chars = embedded_reber_grammar_graph[idx]\n",
        "    c, idx = chars[np.random.randint(0, len(chars))]\n",
        "    result += c if c != REBER_GRAPH else generate_reber_grammar()\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgjM18o06Z57",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc113445-f616-40e3-bf66-24f3f18001aa"
      },
      "source": [
        "generate_embedded_reber_grammar()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BPBPVPXVVEPE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt5SLRKu7kJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a corrupted string by creating an embedded Reber grammar\n",
        "# and then change a single character.\n",
        "\n",
        "def generate_corrupted_string():\n",
        "  erg_string = generate_embedded_reber_grammar()\n",
        "  chars = set(erg_string)\n",
        "  idx = np.random.randint(0, len(erg_string))\n",
        "  bad_char = np.random.choice(list(chars - set(erg_string[idx])))\n",
        "  return '{}{}{}'.format(erg_string[:idx], bad_char, erg_string[idx+1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VotOIMgi8O_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9693ce9-11e6-485d-e145-da77f1d2d9cb"
      },
      "source": [
        "generate_corrupted_string()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BPBPPVVEPE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5acQZo9H87nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One-hot encoding each string.\n",
        "\n",
        "char_to_idx_map = {c: i for i, c in enumerate('BEPSTVX')}\n",
        "\n",
        "def char_to_vector(c):\n",
        "  data = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "  data[char_to_idx_map[c]] = 1.0\n",
        "  return data\n",
        "\n",
        "def one_hot_encode(erg_str):\n",
        "  return np.array([char_to_vector(c) for c in erg_str], dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp_INdvO_f8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "d34ca675-7252-48e8-d476-c64c856a5c18"
      },
      "source": [
        "one_hot_encode(generate_embedded_reber_grammar())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCaCnB9aBOia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad a one-hot encoded embedded Reber grammar string.\n",
        "def pad_zeros(ohe_erg_str, length):\n",
        "  str_length = len(ohe_erg_str)\n",
        "  if str_length > length:\n",
        "    raise Exception(\n",
        "        'the 2nd argument of pad_zeros must be gte the length of the first '\n",
        "        'argument')\n",
        "  for i in range(length - str_length):\n",
        "    ohe_erg_str = \\\n",
        "        np.concatenate((ohe_erg_str, [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]))\n",
        "  return ohe_erg_str"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD-oVACqCTfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "e3e56063-6fa5-459e-ae54-23dd3fd7f11e"
      },
      "source": [
        "pad_zeros(one_hot_encode(generate_embedded_reber_grammar())[:10], 15)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yraMzDHEDVtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a single training batch.\n",
        "\n",
        "def generate_batch(batch_size, max_seq_length):\n",
        "  good_batch = []\n",
        "  bad_batch = []\n",
        "  while len(good_batch) < batch_size / 2:\n",
        "    seq = one_hot_encode(generate_embedded_reber_grammar())\n",
        "    if len(seq) > max_seq_length:\n",
        "      continue\n",
        "    good_batch.append((seq, True))\n",
        "  while len(bad_batch) < batch_size / 2:\n",
        "    seq = one_hot_encode(generate_corrupted_string())\n",
        "    if len(seq) > max_seq_length:\n",
        "      continue\n",
        "    bad_batch.append((seq, False))\n",
        "  batch = []\n",
        "  seq_lengths = []\n",
        "  labels = []\n",
        "  all_seqs = good_batch + bad_batch\n",
        "  np.random.shuffle(all_seqs)\n",
        "  for seq, is_valid in all_seqs:\n",
        "    batch.append(pad_zeros(seq, max_seq_length))\n",
        "    seq_lengths.append(len(seq))\n",
        "    labels.append([int(is_valid)])\n",
        "  return np.array(batch, dtype=np.float32), \\\n",
        "      np.array(labels, dtype=np.int32), \\\n",
        "      np.array(seq_lengths, dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-pZQDr6MoSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the TensorFlow graph\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "n_chars = 7\n",
        "max_sequence_length = 20\n",
        "n_outputs = 1\n",
        "n_neurons = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  X = tf.placeholder(tf.float32, (None, max_sequence_length, n_chars))\n",
        "  y = tf.placeholder(tf.float32, (None, 1))\n",
        "  seq_length = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "  cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)\n",
        "  _, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32,\n",
        "                                sequence_length=seq_length)\n",
        "  logits = tf.layers.dense(states, n_outputs)\n",
        "\n",
        "  xentropy = \\\n",
        "      tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y, tf.float32),\n",
        "                                              logits=logits)\n",
        "  loss = tf.reduce_mean(xentropy)\n",
        "  opt = tf.train.AdamOptimizer(learning_rate)\n",
        "  training_op = opt.minimize(loss)\n",
        "\n",
        "  y_pred = tf.cast(tf.greater(logits, 0.0), tf.float32)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(y, y_pred), tf.float32))\n",
        "\n",
        "  init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOlKHTavSdBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "3fe2e828-6adf-42c4-dbc9-2814a8350713"
      },
      "source": [
        "# Training a model to recognize embedded Reber grammars.\n",
        "\n",
        "n_epochs = 50\n",
        "n_batches = 25\n",
        "batch_size = 100\n",
        "validation_set_size = 200\n",
        "\n",
        "with graph.as_default():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    X_valid, y_valid, seq_len_valid = \\\n",
        "        generate_batch(validation_set_size, max_sequence_length)\n",
        "    valid_feed_dict = {\n",
        "        X: X_valid,\n",
        "        y: y_valid,\n",
        "        seq_length: seq_len_valid,\n",
        "    }\n",
        "    for epoch in range(n_epochs):\n",
        "      for _ in range(n_batches):\n",
        "        X_batch, y_batch, seq_len_batch = \\\n",
        "            generate_batch(batch_size, max_sequence_length)\n",
        "        sess.run(training_op, feed_dict={\n",
        "            X: X_batch,\n",
        "            y: y_batch,\n",
        "            seq_length: seq_len_batch,\n",
        "        })\n",
        "      if epoch % 5 == 0:\n",
        "        loss_val = loss.eval(feed_dict=valid_feed_dict)\n",
        "        acc_val = accuracy.eval(feed_dict=valid_feed_dict)\n",
        "        print('Epoch: {}\\tLoss: {}\\tAccuracy: {}'.format(epoch, loss_val,\n",
        "                                                         acc_val))"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tLoss: 0.6584188938140869\tAccuracy: 0.6449999809265137\n",
            "Epoch: 5\tLoss: 0.6347793340682983\tAccuracy: 0.4300000071525574\n",
            "Epoch: 10\tLoss: 0.13499197363853455\tAccuracy: 0.9599999785423279\n",
            "Epoch: 15\tLoss: 0.08909790217876434\tAccuracy: 0.9750000238418579\n",
            "Epoch: 20\tLoss: 0.04386292025446892\tAccuracy: 0.9900000095367432\n",
            "Epoch: 25\tLoss: 0.0029373581055551767\tAccuracy: 1.0\n",
            "Epoch: 30\tLoss: 0.001209043781273067\tAccuracy: 1.0\n",
            "Epoch: 35\tLoss: 0.0013871783157810569\tAccuracy: 1.0\n",
            "Epoch: 40\tLoss: 0.00021878271945752203\tAccuracy: 1.0\n",
            "Epoch: 45\tLoss: 0.0001390562829328701\tAccuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-G-VZIjd_Bx",
        "colab_type": "text"
      },
      "source": [
        "### 8. Tacle the [\"How much did it rain? II\" Kaggle competition](https://www.kaggle.com/c/how-much-did-it-rain-ii), this is a time series prediction task.\n",
        "\n",
        "[Luis Andre Dutra e Silva's interview](http://blog.kaggle.com/2015/12/17/how-much-did-it-rain-ii-2nd-place-luis-andre-dutra-e-silva/) shows some insights that he used to reach second place in the competition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRWSmOooeqOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}