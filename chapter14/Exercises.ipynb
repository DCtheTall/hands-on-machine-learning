{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercises.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ilct8w92RGg",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 14: Recurrent Neural Networks\n",
        "\n",
        "## Exercises\n",
        "\n",
        "### 1. Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?\n",
        "\n",
        "A sequence-to-sequence RNN is generally used for a model for predicting the future behavior of some input sequence. This can be used to create a predictive model for determining what the next word you are about to type might be.\n",
        "\n",
        "A sequence-to-vector RNN is good for classifying sequences, such as sentiment analysis. Also a sequence-to-vector RNN is used for finding the embeddings of a vocabulary of words in a denser, smaller vector space.\n",
        "\n",
        "In machine translation, vector-to-sequence RNN is good for decoding embeddings from input in one language into words in the target language. You can also use vector-to-sequence RNNs to generate captions for images.\n",
        "\n",
        "### 2. Why do people use encoder-decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
        "\n",
        "Most models for automatic translation encode vocabularies as a vector space where each word is a perpendicular unit vector. For a vocabulary of 50,000 words, this means the input sequences are vectors in a 50,000-dimensional space. Training a sequence-to-sequence RNN for machine translation with large vocabularies would take a large amount of memory, making it inefficient.\n",
        "\n",
        "Using an Encoder-Decoder model allows you to train the encoder to find a denser representation of the words, making training more efficient. Also training the model to find an embedding also helps the model learn what words are closely related to one another.\n",
        "\n",
        "### 3. How could you combine a convolutional neural network and an RNN to classify videos?\n",
        "\n",
        "Since a video is a sequence of images, you could create a convolutional neural network where each cell is a convolutional layers which learn feature maps for the images in each frame of the video. You could have it learn one set of feature maps for the input and another feature map for the previous output.\n",
        "\n",
        "### 4. What are the advantages of building an RNN using `dynamic_rnn()` rather than `static_rnn()`?\n",
        "\n",
        "The `static_rnn()` function creates new graph nodes for each time step in the sequence. This means that if you are processing a sequence with a large number of steps, you risk getting an OOM error when building your TensorFlow graph. The `dynamic_rnn()` function uses a while loop to perform multiple operations using the same nodes. The `dyanmic_rnn()` function also allows you to swap memory between the GPU and the CPU using the `swap_memory` parameter. It also accepts a single tensor as an input instead of a list of tensors for each time step in the sequence.\n",
        "\n",
        "### 5. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
        "\n",
        "The `dynamic_rnn()` function takes a `sequence_length` parameter which is a 1D tensor of integers which represent the sequence length of each of the inputs. Input sequences that are less than the maximum length sequence are padded with zeros.\n",
        "\n",
        "For variable-length output sequences, since it is not possible to determine how long each output will be prior to training, so each output sequence ends with an end of sequence (EOS) character to delimit the end.\n",
        "\n",
        "### 6. What is a common way to distribute training and execution of a deep RNN across multiple GPUs?\n",
        "\n",
        "In order to distribute an RNN across devices you cannot just simply call the `tf.device()` function. This is because TensorFlow's built in RNN cell classes like `BasicRNNCell` do not create the graph ncdes themselves, rather they are cell factories.\n",
        "\n",
        "In order to distribute an RNN across devices, you must define a new cell factory which actually creates each cell on a separate device. For an example, see `DeviceCellWrapper` in `RecurrentNeuralNetworks.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d643pw4y8dsb",
        "colab_type": "text"
      },
      "source": [
        "### 7. _Embedded Reber grammars_ are artificial grammars used to produce strings. Train an RNN to identify whether or not a string represents the grammar discussed in [Jenny Orr's introduction](http://www.willamette.edu/~gorr/classes/cs449/reber.html) or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar and 50% that do not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QDrhajr9aXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to generate Rebber grammars. Each key in the dict\n",
        "# is a node in the graph. Each element in the list is an adjacent node\n",
        "# in the graph.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Adjacency list for the graph.\n",
        "reber_grammar_graph = [\n",
        "  [('B', 1)],\n",
        "  [('T', 2), ('P', 3)],\n",
        "  [('S', 2), ('X', 4)],\n",
        "  [('T', 3), ('V', 5)],\n",
        "  [('X', 3), ('S', 6)],\n",
        "  [('P', 4), ('V', 6)],\n",
        "  [('E', None)],\n",
        "]\n",
        "\n",
        "def generate_reber_grammar():\n",
        "  idx = 0\n",
        "  result = ''\n",
        "  while idx is not None:\n",
        "    chars = reber_grammar_graph[idx]\n",
        "    c, idx = chars[np.random.randint(0, len(chars))]\n",
        "    result += c\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvdMLgsbJFUH",
        "colab_type": "code",
        "outputId": "828d0efc-e4ff-4fd7-9f2c-87d8ec72dc3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_reber_grammar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BTSXXTTTTTTTTTTTVVE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBTaT1UvKSQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function for generating embedded Reber grammar.\n",
        "\n",
        "REBER_GRAPH = 'reber_graph'\n",
        "\n",
        "embedded_reber_grammar_graph = [\n",
        "  [('B', 1)],\n",
        "  [('T', 2), ('P', 3)],\n",
        "  [(REBER_GRAPH, 4)],\n",
        "  [(REBER_GRAPH, 5)],\n",
        "  [('T', 6)],\n",
        "  [('P', 6)],\n",
        "  [('E', None)]\n",
        "]\n",
        "\n",
        "def generate_embedded_reber_grammar():\n",
        "  idx = 0\n",
        "  result = ''\n",
        "  while idx is not None:\n",
        "    chars = embedded_reber_grammar_graph[idx]\n",
        "    c, idx = chars[np.random.randint(0, len(chars))]\n",
        "    result += c if c != REBER_GRAPH else generate_reber_grammar()\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgjM18o06Z57",
        "colab_type": "code",
        "outputId": "cc113445-f616-40e3-bf66-24f3f18001aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_embedded_reber_grammar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BPBPVPXVVEPE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt5SLRKu7kJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a corrupted string by creating an embedded Reber grammar\n",
        "# and then change a single character.\n",
        "\n",
        "def generate_corrupted_string():\n",
        "  erg_string = generate_embedded_reber_grammar()\n",
        "  chars = set(erg_string)\n",
        "  idx = np.random.randint(0, len(erg_string))\n",
        "  bad_char = np.random.choice(list(chars - set(erg_string[idx])))\n",
        "  return '{}{}{}'.format(erg_string[:idx], bad_char, erg_string[idx+1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VotOIMgi8O_7",
        "colab_type": "code",
        "outputId": "a9693ce9-11e6-485d-e145-da77f1d2d9cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_corrupted_string()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BPBPPVVEPE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5acQZo9H87nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One-hot encoding each string.\n",
        "\n",
        "char_to_idx_map = {c: i for i, c in enumerate('BEPSTVX')}\n",
        "\n",
        "def char_to_vector(c):\n",
        "  data = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "  data[char_to_idx_map[c]] = 1.0\n",
        "  return data\n",
        "\n",
        "def one_hot_encode(erg_str):\n",
        "  return np.array([char_to_vector(c) for c in erg_str], dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp_INdvO_f8j",
        "colab_type": "code",
        "outputId": "d34ca675-7252-48e8-d476-c64c856a5c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "one_hot_encode(generate_embedded_reber_grammar())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCaCnB9aBOia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad a one-hot encoded embedded Reber grammar string.\n",
        "def pad_zeros(ohe_erg_str, length):\n",
        "  str_length = len(ohe_erg_str)\n",
        "  if str_length > length:\n",
        "    raise Exception(\n",
        "        'the 2nd argument of pad_zeros must be gte the length of the first '\n",
        "        'argument')\n",
        "  for i in range(length - str_length):\n",
        "    ohe_erg_str = \\\n",
        "        np.concatenate((ohe_erg_str, [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]))\n",
        "  return ohe_erg_str"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD-oVACqCTfh",
        "colab_type": "code",
        "outputId": "e3e56063-6fa5-459e-ae54-23dd3fd7f11e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "pad_zeros(one_hot_encode(generate_embedded_reber_grammar())[:10], 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yraMzDHEDVtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a single training batch.\n",
        "\n",
        "def generate_batch(batch_size, max_seq_length):\n",
        "  good_batch = []\n",
        "  bad_batch = []\n",
        "  while len(good_batch) < batch_size / 2:\n",
        "    seq = one_hot_encode(generate_embedded_reber_grammar())\n",
        "    if len(seq) > max_seq_length:\n",
        "      continue\n",
        "    good_batch.append((seq, True))\n",
        "  while len(bad_batch) < batch_size / 2:\n",
        "    seq = one_hot_encode(generate_corrupted_string())\n",
        "    if len(seq) > max_seq_length:\n",
        "      continue\n",
        "    bad_batch.append((seq, False))\n",
        "  batch = []\n",
        "  seq_lengths = []\n",
        "  labels = []\n",
        "  all_seqs = good_batch + bad_batch\n",
        "  np.random.shuffle(all_seqs)\n",
        "  for seq, is_valid in all_seqs:\n",
        "    batch.append(pad_zeros(seq, max_seq_length))\n",
        "    seq_lengths.append(len(seq))\n",
        "    labels.append([int(is_valid)])\n",
        "  return np.array(batch, dtype=np.float32), \\\n",
        "      np.array(labels, dtype=np.int32), \\\n",
        "      np.array(seq_lengths, dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-pZQDr6MoSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the TensorFlow graph\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "n_chars = 7\n",
        "max_sequence_length = 20\n",
        "n_outputs = 1\n",
        "n_neurons = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  X = tf.placeholder(tf.float32, (None, max_sequence_length, n_chars))\n",
        "  y = tf.placeholder(tf.float32, (None, 1))\n",
        "  seq_length = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "  cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)\n",
        "  _, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32,\n",
        "                                sequence_length=seq_length)\n",
        "  logits = tf.layers.dense(states, n_outputs)\n",
        "\n",
        "  xentropy = \\\n",
        "      tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y, tf.float32),\n",
        "                                              logits=logits)\n",
        "  loss = tf.reduce_mean(xentropy)\n",
        "  opt = tf.train.AdamOptimizer(learning_rate)\n",
        "  training_op = opt.minimize(loss)\n",
        "\n",
        "  y_pred = tf.cast(tf.greater(logits, 0.0), tf.float32)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(y, y_pred), tf.float32))\n",
        "\n",
        "  init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOlKHTavSdBZ",
        "colab_type": "code",
        "outputId": "3fe2e828-6adf-42c4-dbc9-2814a8350713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# Training a model to recognize embedded Reber grammars.\n",
        "\n",
        "n_epochs = 50\n",
        "n_batches = 25\n",
        "batch_size = 100\n",
        "validation_set_size = 200\n",
        "\n",
        "with graph.as_default():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    X_valid, y_valid, seq_len_valid = \\\n",
        "        generate_batch(validation_set_size, max_sequence_length)\n",
        "    valid_feed_dict = {\n",
        "        X: X_valid,\n",
        "        y: y_valid,\n",
        "        seq_length: seq_len_valid,\n",
        "    }\n",
        "    for epoch in range(n_epochs):\n",
        "      for _ in range(n_batches):\n",
        "        X_batch, y_batch, seq_len_batch = \\\n",
        "            generate_batch(batch_size, max_sequence_length)\n",
        "        sess.run(training_op, feed_dict={\n",
        "            X: X_batch,\n",
        "            y: y_batch,\n",
        "            seq_length: seq_len_batch,\n",
        "        })\n",
        "      if epoch % 5 == 0:\n",
        "        loss_val = loss.eval(feed_dict=valid_feed_dict)\n",
        "        acc_val = accuracy.eval(feed_dict=valid_feed_dict)\n",
        "        print('Epoch: {}\\tLoss: {}\\tAccuracy: {}'.format(epoch, loss_val,\n",
        "                                                         acc_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tLoss: 0.6584188938140869\tAccuracy: 0.6449999809265137\n",
            "Epoch: 5\tLoss: 0.6347793340682983\tAccuracy: 0.4300000071525574\n",
            "Epoch: 10\tLoss: 0.13499197363853455\tAccuracy: 0.9599999785423279\n",
            "Epoch: 15\tLoss: 0.08909790217876434\tAccuracy: 0.9750000238418579\n",
            "Epoch: 20\tLoss: 0.04386292025446892\tAccuracy: 0.9900000095367432\n",
            "Epoch: 25\tLoss: 0.0029373581055551767\tAccuracy: 1.0\n",
            "Epoch: 30\tLoss: 0.001209043781273067\tAccuracy: 1.0\n",
            "Epoch: 35\tLoss: 0.0013871783157810569\tAccuracy: 1.0\n",
            "Epoch: 40\tLoss: 0.00021878271945752203\tAccuracy: 1.0\n",
            "Epoch: 45\tLoss: 0.0001390562829328701\tAccuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-G-VZIjd_Bx",
        "colab_type": "text"
      },
      "source": [
        "### 8. Tacle the [\"How much did it rain? II\" Kaggle competition](https://www.kaggle.com/c/how-much-did-it-rain-ii), this is a time series prediction task.\n",
        "\n",
        "[Luis Andre Dutra e Silva's interview](http://blog.kaggle.com/2015/12/17/how-much-did-it-rain-ii-2nd-place-luis-andre-dutra-e-silva/) shows some insights that he used to reach second place in the competition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRWSmOooeqOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First install kaggle API and get the data. The kaggle JSON is stored\n",
        "# as a local file in the Colab kernel to avoid revealing PII.\n",
        "\n",
        "!pip install kaggle\n",
        "!mkdir -p /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "!kaggle config path -p .\n",
        "!kaggle competitions download -c how-much-did-it-rain-ii"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwUkzi_G0eRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "for f in os.listdir():\n",
        "  if f[-4:] == '.zip':\n",
        "    zip_ref = zipfile.ZipFile(f, mode='r')\n",
        "    zip_ref.extractall()\n",
        "    zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn2-CMhL1oBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload the training data.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "training_df = pd.read_csv('train.csv')\n",
        "training_df = training_df.dropna().reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNJR4jen92uo",
        "colab_type": "code",
        "outputId": "51483a94-f466-4fff-c9d3-db09b98fb819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/81868506e94e6988/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 7.5,\n            'f': \"7.5\",\n        },\n{\n            'v': 10.5,\n            'f': \"10.5\",\n        },\n{\n            'v': 15,\n            'f': \"15\",\n        },\n{\n            'v': 10.5,\n            'f': \"10.5\",\n        },\n{\n            'v': 16.5,\n            'f': \"16.5\",\n        },\n{\n            'v': 23.5,\n            'f': \"23.5\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.375,\n            'f': \"0.375\",\n        },\n{\n            'v': -0.125,\n            'f': \"-0.125\",\n        },\n{\n            'v': 0.3125,\n            'f': \"0.3125\",\n        },\n{\n            'v': 0.875,\n            'f': \"0.875\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 16,\n            'f': \"16\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 18,\n            'f': \"18\",\n        },\n{\n            'v': 14,\n            'f': \"14\",\n        },\n{\n            'v': 17.5,\n            'f': \"17.5\",\n        },\n{\n            'v': 21,\n            'f': \"21\",\n        },\n{\n            'v': 20.5,\n            'f': \"20.5\",\n        },\n{\n            'v': 18,\n            'f': \"18\",\n        },\n{\n            'v': 20.5,\n            'f': \"20.5\",\n        },\n{\n            'v': 23,\n            'f': \"23\",\n        },\n{\n            'v': 0.995,\n            'f': \"0.995\",\n        },\n{\n            'v': 0.995,\n            'f': \"0.995\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 1.0016667,\n            'f': \"1.0016667\",\n        },\n{\n            'v': 0.25,\n            'f': \"0.25\",\n        },\n{\n            'v': 0.125,\n            'f': \"0.125\",\n        },\n{\n            'v': 0.375,\n            'f': \"0.375\",\n        },\n{\n            'v': 0.6875,\n            'f': \"0.6875\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 21,\n            'f': \"21\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 24.5,\n            'f': \"24.5\",\n        },\n{\n            'v': 16.5,\n            'f': \"16.5\",\n        },\n{\n            'v': 21,\n            'f': \"21\",\n        },\n{\n            'v': 24.5,\n            'f': \"24.5\",\n        },\n{\n            'v': 24.5,\n            'f': \"24.5\",\n        },\n{\n            'v': 21,\n            'f': \"21\",\n        },\n{\n            'v': 24,\n            'f': \"24\",\n        },\n{\n            'v': 28,\n            'f': \"28\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.995,\n            'f': \"0.995\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.25,\n            'f': \"0.25\",\n        },\n{\n            'v': 0.0625,\n            'f': \"0.0625\",\n        },\n{\n            'v': 0.1875,\n            'f': \"0.1875\",\n        },\n{\n            'v': 0.5625,\n            'f': \"0.5625\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 11,\n            'f': \"11\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 26,\n            'f': \"26\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 12,\n            'f': \"12\",\n        },\n{\n            'v': 12,\n            'f': \"12\",\n        },\n{\n            'v': 16,\n            'f': \"16\",\n        },\n{\n            'v': 20,\n            'f': \"20\",\n        },\n{\n            'v': 16.5,\n            'f': \"16.5\",\n        },\n{\n            'v': 17,\n            'f': \"17\",\n        },\n{\n            'v': 19,\n            'f': \"19\",\n        },\n{\n            'v': 21,\n            'f': \"21\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.995,\n            'f': \"0.995\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.5625,\n            'f': \"0.5625\",\n        },\n{\n            'v': 0.25,\n            'f': \"0.25\",\n        },\n{\n            'v': 0.4375,\n            'f': \"0.4375\",\n        },\n{\n            'v': 0.6875,\n            'f': \"0.6875\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 12,\n            'f': \"12\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 31,\n            'f': \"31\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 22.5,\n            'f': \"22.5\",\n        },\n{\n            'v': 19,\n            'f': \"19\",\n        },\n{\n            'v': 22,\n            'f': \"22\",\n        },\n{\n            'v': 25,\n            'f': \"25\",\n        },\n{\n            'v': 26,\n            'f': \"26\",\n        },\n{\n            'v': 23.5,\n            'f': \"23.5\",\n        },\n{\n            'v': 25.5,\n            'f': \"25.5\",\n        },\n{\n            'v': 27.5,\n            'f': \"27.5\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 0.995,\n            'f': \"0.995\",\n        },\n{\n            'v': 0.99833333,\n            'f': \"0.99833333\",\n        },\n{\n            'v': 1.0016667,\n            'f': \"1.0016667\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': -0.1875,\n            'f': \"-0.1875\",\n        },\n{\n            'v': 0.25,\n            'f': \"0.25\",\n        },\n{\n            'v': 0.625,\n            'f': \"0.625\",\n        }]],\n        columns: [[\"number\", \"level_0\"], [\"number\", \"index\"], [\"number\", \"Id\"], [\"number\", \"minutes_past\"], [\"number\", \"radardist_km\"], [\"number\", \"Ref\"], [\"number\", \"Ref_5x5_10th\"], [\"number\", \"Ref_5x5_50th\"], [\"number\", \"Ref_5x5_90th\"], [\"number\", \"RefComposite\"], [\"number\", \"RefComposite_5x5_10th\"], [\"number\", \"RefComposite_5x5_50th\"], [\"number\", \"RefComposite_5x5_90th\"], [\"number\", \"RhoHV\"], [\"number\", \"RhoHV_5x5_10th\"], [\"number\", \"RhoHV_5x5_50th\"], [\"number\", \"RhoHV_5x5_90th\"], [\"number\", \"Zdr\"], [\"number\", \"Zdr_5x5_10th\"], [\"number\", \"Zdr_5x5_50th\"], [\"number\", \"Zdr_5x5_90th\"]],\n        rowsPerPage: 25,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Id</th>\n",
              "      <th>minutes_past</th>\n",
              "      <th>radardist_km</th>\n",
              "      <th>Ref</th>\n",
              "      <th>Ref_5x5_10th</th>\n",
              "      <th>Ref_5x5_50th</th>\n",
              "      <th>Ref_5x5_90th</th>\n",
              "      <th>RefComposite</th>\n",
              "      <th>RefComposite_5x5_10th</th>\n",
              "      <th>RefComposite_5x5_50th</th>\n",
              "      <th>RefComposite_5x5_90th</th>\n",
              "      <th>RhoHV</th>\n",
              "      <th>RhoHV_5x5_10th</th>\n",
              "      <th>RhoHV_5x5_50th</th>\n",
              "      <th>RhoHV_5x5_90th</th>\n",
              "      <th>Zdr</th>\n",
              "      <th>Zdr_5x5_10th</th>\n",
              "      <th>Zdr_5x5_50th</th>\n",
              "      <th>Zdr_5x5_90th</th>\n",
              "      <th>Kdp</th>\n",
              "      <th>Kdp_5x5_10th</th>\n",
              "      <th>Kdp_5x5_50th</th>\n",
              "      <th>Kdp_5x5_90th</th>\n",
              "      <th>Expected</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.5</td>\n",
              "      <td>10.5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>16.5</td>\n",
              "      <td>23.5</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.3750</td>\n",
              "      <td>-0.1250</td>\n",
              "      <td>0.3125</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>1.059998</td>\n",
              "      <td>-1.410004</td>\n",
              "      <td>-0.350006</td>\n",
              "      <td>1.059998</td>\n",
              "      <td>1.016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>2.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17.5</td>\n",
              "      <td>21.0</td>\n",
              "      <td>20.5</td>\n",
              "      <td>18.0</td>\n",
              "      <td>20.5</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>1.001667</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>0.3750</td>\n",
              "      <td>0.6875</td>\n",
              "      <td>0.349991</td>\n",
              "      <td>-1.059998</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.059998</td>\n",
              "      <td>1.016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>2.0</td>\n",
              "      <td>24.5</td>\n",
              "      <td>16.5</td>\n",
              "      <td>21.0</td>\n",
              "      <td>24.5</td>\n",
              "      <td>24.5</td>\n",
              "      <td>21.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>0.5625</td>\n",
              "      <td>-0.350006</td>\n",
              "      <td>-1.059998</td>\n",
              "      <td>-0.350006</td>\n",
              "      <td>1.759994</td>\n",
              "      <td>1.016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>26</td>\n",
              "      <td>2.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.5</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.5625</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.6875</td>\n",
              "      <td>-1.760010</td>\n",
              "      <td>-1.760010</td>\n",
              "      <td>-0.350006</td>\n",
              "      <td>0.709991</td>\n",
              "      <td>1.016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "      <td>2.0</td>\n",
              "      <td>22.5</td>\n",
              "      <td>19.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>23.5</td>\n",
              "      <td>25.5</td>\n",
              "      <td>27.5</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.998333</td>\n",
              "      <td>1.001667</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.1875</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.6250</td>\n",
              "      <td>-1.059998</td>\n",
              "      <td>-2.120010</td>\n",
              "      <td>-0.710007</td>\n",
              "      <td>0.349991</td>\n",
              "      <td>1.016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  Id  minutes_past  ...  Kdp_5x5_50th  Kdp_5x5_90th  Expected\n",
              "0      6   2             1  ...     -0.350006      1.059998     1.016\n",
              "1      9   2            16  ...      0.000000      1.059998     1.016\n",
              "2     10   2            21  ...     -0.350006      1.759994     1.016\n",
              "3     11   2            26  ...     -0.350006      0.709991     1.016\n",
              "4     12   2            31  ...     -0.710007      0.349991     1.016\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvR01QIS5hN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function to prepare the data.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "features = [\n",
        "  'minutes_past',\n",
        "  'radardist_km',\n",
        "  'Ref',\n",
        "  'Ref_5x5_10th',\n",
        "  'Ref_5x5_50th',\n",
        "  'Ref_5x5_90th',\n",
        "  'RefComposite',\n",
        "  'RefComposite_5x5_10th',\n",
        "  'RefComposite_5x5_50th',\n",
        "  'RefComposite_5x5_90th',\n",
        "  'RhoHV',\n",
        "  'RhoHV_5x5_10th',\n",
        "  'RhoHV_5x5_50th',\n",
        "  'RhoHV_5x5_90th',\n",
        "  'Zdr',\n",
        "  'Zdr_5x5_10th',\n",
        "  'Zdr_5x5_50th',\n",
        "  'Zdr_5x5_90th',\n",
        "  'Kdp',\n",
        "  'Kdp_5x5_10th',\n",
        "  'Kdp_5x5_50th',\n",
        "  'Kdp_5x5_90th',\n",
        "]\n",
        "\n",
        "def prepare_data_for_model(df):\n",
        "  # Going to organize the data into sequences sorted by minutes_after by id.\n",
        "  sequences = dict()\n",
        "  max_len = 1\n",
        "  for i, row in df.iterrows():\n",
        "    entry = [row[k] for k in features]\n",
        "    try:\n",
        "      sequences[row['Id']].append((entry, row['Expected']))\n",
        "      if len(sequences[row['Id']]) > max_len:\n",
        "        max_len = len(sequences[row['Id']])\n",
        "    except:\n",
        "      sequences[row['Id']] = [(entry, row['Expected'])]\n",
        "  data, outputs, seq_lengths = [], [], []\n",
        "  for i in sequences:\n",
        "    outputs.append([sequences[i][0][1]])\n",
        "    seq_lengths.append(len(sequences[i]))\n",
        "    S = sorted(sequences[i], key=lambda r: r[0][0])\n",
        "    data_entry = []\n",
        "    for entry, _ in S:\n",
        "      data_entry.append(entry)\n",
        "    for _ in range(max_len - len(S)):\n",
        "      data_entry.append([0.0] * len(features))\n",
        "    data.append(data_entry)\n",
        "  return np.array(data, dtype=np.float32), \\\n",
        "      np.array(outputs, dtype=np.float32), \\\n",
        "      np.array(seq_lengths, dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww9pfvyY-XmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_valid, y_train_valid, seq_length_train_valid = \\\n",
        "    prepare_data_for_model(training_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vS8-q_0N7md",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid, seq_length_train, seq_length_valid = \\\n",
        "    train_test_split(X_train_valid, y_train_valid, seq_length_train_valid,\n",
        "                     test_size=2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adCei_67Nib2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the model graph using 2 layers of LSTM cells.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "n_steps = max(seq_length_train)\n",
        "n_features = len(features)\n",
        "n_neurons = 200\n",
        "n_layers = 2\n",
        "n_outputs = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  X = tf.placeholder(tf.float32, (None, n_steps, n_features))\n",
        "  y = tf.placeholder(tf.float32, (None, 1))\n",
        "  seq_length = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "  lstm_cells = [tf.nn.rnn_cell.BasicLSTMCell(num_units=n_neurons)\n",
        "                for _ in range(n_layers)]\n",
        "  multi_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)\n",
        "  outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
        "  top_layer_h_states = states[-1][1]\n",
        "  logits = tf.layers.dense(top_layer_h_states, n_outputs)\n",
        "\n",
        "  loss = tf.reduce_mean(tf.abs(logits - y))\n",
        "  opt = tf.train.AdamOptimizer(learning_rate)\n",
        "  training_op = opt.minimize(loss)\n",
        "\n",
        "  saver = tf.train.Saver()\n",
        "  init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qy7Jq4EQle8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for generating random batches.\n",
        "\n",
        "def shuffle_batch(X, y, seq_length, batch_size):\n",
        "  rnd_idx = np.random.permutation(len(X))\n",
        "  n_batches = len(X) // batch_size\n",
        "  for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "    X_batch, y_batch, seq_length_batch = \\\n",
        "        X[batch_idx], y[batch_idx], seq_length[batch_idx]\n",
        "    yield X_batch, y_batch, seq_length_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQhq34eVTCN_",
        "colab_type": "code",
        "outputId": "ff0da40f-b6ed-4e91-da7f-05dbd8f360a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Training the model and evaluating against a validation set.\n",
        "\n",
        "n_epochs = 10\n",
        "batch_size = 256\n",
        "n_batches = len(X_train) // batch_size\n",
        "max_rounds_since_best_loss = 100\n",
        "\n",
        "with graph.as_default():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    best_loss = float('inf')\n",
        "    rounds_since_best_loss = 0\n",
        "    for epoch in range(n_epochs):\n",
        "      print('Epoch:', epoch)\n",
        "      mean_train_loss = 0.0\n",
        "      for X_batch, y_batch, seq_length_batch in \\\n",
        "          shuffle_batch(X_train, y_train, seq_length_train, batch_size):\n",
        "        _, loss_train_val = sess.run([training_op, loss], feed_dict={\n",
        "            X: X_batch,\n",
        "            y: y_batch,\n",
        "            seq_length: seq_length_batch,\n",
        "        })\n",
        "        mean_train_loss += loss_train_val\n",
        "        if loss_train_val < best_loss:\n",
        "          best_loss = loss_train_val\n",
        "          rounds_since_best_loss = 0\n",
        "          saver.save(sess, 'rainfall_model.cpkt')\n",
        "        else:\n",
        "          rounds_since_best_loss += 1\n",
        "      loss_valid_val = loss.eval(feed_dict={\n",
        "          X: X_valid,\n",
        "          y: y_valid,\n",
        "          seq_length: seq_length_valid,\n",
        "      })\n",
        "      mean_train_loss /= n_batches\n",
        "      print(\n",
        "          'Validation loss: {:.4f}\\nMean training loss: {:.4f}' \\\n",
        "              .format(loss_valid_val, mean_train_loss))\n",
        "      print('======')\n",
        "      if rounds_since_best_loss >= max_rounds_since_best_loss:\n",
        "        print('Early stopping.')\n",
        "        break\n",
        "    else:\n",
        "      saver.save(sess, 'rainfall_model.cpkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Validation loss: 12.8471\n",
            "Mean training loss: 12.6746\n",
            "======\n",
            "Early stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tWWODlQBOO3",
        "colab_type": "code",
        "outputId": "ae9b42fb-3606-4f91-d84a-7c898d3a3c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Getting the loss on the entire training set.\n",
        "\n",
        "with graph.as_default():\n",
        "  with tf.Session() as sess:\n",
        "    saver.restore(sess, 'rainfall_model.cpkt')\n",
        "    loss_val = loss.eval(feed_dict={\n",
        "      X: X_train_valid,\n",
        "      y: y_train_valid,\n",
        "      seq_length: seq_length_train_valid,\n",
        "    })\n",
        "    print('Total loss:', loss_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total loss: 12.640512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsx2xts5uMn8",
        "colab_type": "text"
      },
      "source": [
        "### 9. Go through [TensorFlow's Word2Vec tutoral](https://www.tensorflow.org/tutorials/representation/word2vec) to create word embeddings, and then go through the NMT tutorial to train a Vietnamese-to-English translation system.\n",
        "\n",
        "See the **Embeddings** section of `RecurrentNeuralNetworks.ipynb` in this section in order to see an implementation of word embeddings based on TensorFlow's Word2Vec tutorial.\n",
        "\n",
        "The book uses a link to a tutorial that I could not find on TensorFlow's site, so instead I will be going through [this tutorial](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention) on translating Viatnamese to English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSZedhnE1e0i",
        "colab_type": "code",
        "outputId": "ae3b0f27-c229-45a2-9c49-82b2d6f93dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 348.9MB 72kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 40.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 54.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV59H9zji5AE",
        "colab_type": "code",
        "outputId": "847967cc-8e34-42cf-e81b-05629e4f8e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Download the file contaning training data.\n",
        "\n",
        "origin = \\\n",
        "    'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin=origin,\n",
        "                                      extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip) + '/spa-eng/spa.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK-ye7hIow9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing the data for training.\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "  \n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "  # Adds a space between a word and punctuation following it.\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  # Removing special characters and numbers.\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "  w = w.rstrip().strip()\n",
        "  # Add start and end tokens.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFqE5MytUAaF",
        "colab_type": "code",
        "outputId": "fed1062c-f94c-4a1c-f4c5-a86f1ad3a29e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYCZbRaB7Ig_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the dataset from the downloaded data.\n",
        "\n",
        "def create_dataset(path, n_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]\n",
        "                for l in lines[:n_examples]]\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y66q02ev8EnR",
        "colab_type": "code",
        "outputId": "7fcded5c-adab-4e94-9769-a1348384a387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Testing generate the datasets.\n",
        "\n",
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPwWij8g8ghJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining some utility functions.\n",
        "\n",
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path, n_examples=None):\n",
        "  targ_lang, inp_lang = create_dataset(path, n_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtfIL1niBBk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the data set.\n",
        "\n",
        "n_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = \\\n",
        "    load_dataset(path_to_file, n_examples)\n",
        "\n",
        "max_length_targ, max_length_inp = \\\n",
        "    max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAa-gdExBiZi",
        "colab_type": "code",
        "outputId": "6b4dde51-8aa0-4db6-db14-361ecb023732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Splitting the data set into a training and a validation set.\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = \\\n",
        "    train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val),\n",
        "      len(target_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoEFk2IocD3q",
        "colab_type": "code",
        "outputId": "711d4392-9305-4d30-f99c-16fa451004b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "8 ----> no\n",
            "23 ----> te\n",
            "126 ----> creo\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> i\n",
            "30 ----> don\n",
            "12 ----> t\n",
            "291 ----> believe\n",
            "6 ----> you\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdjGjTLKcLy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a tf.data dataset.\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzwvK9b3czES",
        "colab_type": "code",
        "outputId": "de857f96-035d-41c3-b9f8-375b15c074ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvJc8aJhdpHK",
        "colab_type": "text"
      },
      "source": [
        "### Writing the encoder and decoder model\n",
        "\n",
        "The following model implements an encoder-decoder model using the more recent TensorFlow 2 API. Below is a graph diagram of a model which implements the [attention equations discussed in Luong's paper](https://arxiv.org/abs/1508.04025v5):\n",
        "\n",
        "<img width=\"500\" src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\">\n",
        "\n",
        "The model implemented below uses [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) for the encoder which computes the attention weights differently from Luong's equations. The input is the output of the encoder, which has the shape (_batch_size_, _max_length_, _hidden_size_), and the hidden state of the encoder, which has the shape (_batch_size_, _hidden_size_). The attention equations are given by the following equations:\n",
        "\n",
        "$$ \\alpha_{ts} = \\frac{\\exp{\\left(\\text{score}\\left(\\mathbf{h}_t, \\bar{\\mathbf{h}}_s \\right)\\right)}}{\\sum\\limits_{s'\\,=\\,1}^S \\exp{\\left(\\text{score}\\left(\\mathbf{h}_t, \\bar{\\mathbf{h}}_{s'} \\right)\\right)}} \\;\\;\\; (\\text{Attention weights}) $$\n",
        "\n",
        "$$ \\mathbf{c}_t = \\sum\\limits_s \\alpha_{ts} \\bar{\\mathbf{h}}_s \\;\\;\\; (\\text{Context vector}) $$\n",
        "\n",
        "$$ \\boldsymbol{\\alpha}_t = \\tanh\\left( \\mathbf{W}_c \\left[ \\mathbf{c}_t ; \\mathbf{h}_t \\right] \\right) \\;\\;\\; (\\text{Attention vector}) $$\n",
        "\n",
        "$$ \\text{score}\\left( \\mathbf{h}_t, \\bar{\\mathbf{h}}_s \\right) = \\left\\{\n",
        "  \\begin{matrix}\n",
        "    \\mathbf{h}_t^{\\,T} \\mathbf{W} \\, \\bar{\\mathbf{h}}_s && (\\text{Luong's multiplicative equation}) \\\\\n",
        "    \\mathbf{v}_a^{\\,T} \\tanh\\left( \\mathbf{W}_1 \\mathbf{h}_t + \\mathbf{W}_2 \\bar{\\mathbf{h}}_s \\right) && (\\text{Bahdanau's additive equation})\n",
        "  \\end{matrix}\n",
        "\\right. $$\n",
        "\n",
        "Below is a TensorFlow implementation of an Encoder-Decoder model which uses an attention vector. It uses the TensorFlow 2 API so it is a bit different from the other code in this repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLJ2ALyhdpBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the encoder, which uses a GRU cell to find \n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "  \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "  \n",
        "  def initialize_hidden(self):\n",
        "    return tf.zeros((self.batch_size, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndEwK8OQfwDJ",
        "colab_type": "code",
        "outputId": "ed084a02-1043-49d9-8599-ae62dff8cb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Testing the encoder by inspecting the shape of the outputs.\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(\n",
        "    sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(\n",
        "    sample_hidden.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yECASmtGiTMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implementing Bahdanau attention.\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    \n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIzn-dEYlDHr",
        "colab_type": "code",
        "outputId": "076e3471-9384-4632-869b-a0d85fdc6249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Instantiating the Bahnadau attention.\n",
        "\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden,\n",
        "                                                      sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(\n",
        "    attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(\n",
        "    attention_weights.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzzK9_vNmezf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder class which decodes the embedding into the target language.\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # output shape == (batch_size, 1, hidden_size)\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2r63Dpk7hXg",
        "colab_type": "code",
        "outputId": "0ae51bfe-ed8e-4adf-8f29-9942f2967b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Testing the Decoder class.\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(\n",
        "    sample_decoder_output.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKCVQ8tj8ZmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the optimizer and loss function.\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrQBO4no9vEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checkpoints for saving during training.\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDy8yRorAsX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the training operation for each iteration.\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE,\n",
        "                               1)\n",
        "    # Teacher forcing: using the target as the input to the next step\n",
        "    for t in range(targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:,t], predictions)\n",
        "      # Teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:,t], 1)\n",
        "    \n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCE3HaGGE7Dv",
        "colab_type": "code",
        "outputId": "5503d45b-2492-4cef-ccca-44af1e962ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training the model.\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  \n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 5.3267\n",
            "Epoch 1 Batch 100 Loss 2.1815\n",
            "Epoch 1 Batch 200 Loss 1.9093\n",
            "Epoch 1 Batch 300 Loss 1.7158\n",
            "Epoch 1 Loss 2.0671\n",
            "Time taken for 1 epoch 74.93305230140686 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.6474\n",
            "Epoch 2 Batch 100 Loss 1.4590\n",
            "Epoch 2 Batch 200 Loss 1.3656\n",
            "Epoch 2 Batch 300 Loss 1.2044\n",
            "Epoch 2 Loss 1.3617\n",
            "Time taken for 1 epoch 45.92219686508179 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.2264\n",
            "Epoch 3 Batch 100 Loss 1.0203\n",
            "Epoch 3 Batch 200 Loss 0.9307\n",
            "Epoch 3 Batch 300 Loss 0.7746\n",
            "Epoch 3 Loss 0.9569\n",
            "Time taken for 1 epoch 45.209630727767944 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.8824\n",
            "Epoch 4 Batch 100 Loss 0.7105\n",
            "Epoch 4 Batch 200 Loss 0.6011\n",
            "Epoch 4 Batch 300 Loss 0.5303\n",
            "Epoch 4 Loss 0.6672\n",
            "Time taken for 1 epoch 45.702401876449585 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6057\n",
            "Epoch 5 Batch 100 Loss 0.4647\n",
            "Epoch 5 Batch 200 Loss 0.3705\n",
            "Epoch 5 Batch 300 Loss 0.3567\n",
            "Epoch 5 Loss 0.4630\n",
            "Time taken for 1 epoch 45.356289863586426 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.4209\n",
            "Epoch 6 Batch 100 Loss 0.3320\n",
            "Epoch 6 Batch 200 Loss 0.2489\n",
            "Epoch 6 Batch 300 Loss 0.2897\n",
            "Epoch 6 Loss 0.3311\n",
            "Time taken for 1 epoch 45.653828620910645 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2965\n",
            "Epoch 7 Batch 100 Loss 0.2379\n",
            "Epoch 7 Batch 200 Loss 0.1764\n",
            "Epoch 7 Batch 300 Loss 0.2036\n",
            "Epoch 7 Loss 0.2463\n",
            "Time taken for 1 epoch 45.199756145477295 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2393\n",
            "Epoch 8 Batch 100 Loss 0.1772\n",
            "Epoch 8 Batch 200 Loss 0.1325\n",
            "Epoch 8 Batch 300 Loss 0.1485\n",
            "Epoch 8 Loss 0.1882\n",
            "Time taken for 1 epoch 45.764466524124146 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2152\n",
            "Epoch 9 Batch 100 Loss 0.1247\n",
            "Epoch 9 Batch 200 Loss 0.1215\n",
            "Epoch 9 Batch 300 Loss 0.1188\n",
            "Epoch 9 Loss 0.1508\n",
            "Time taken for 1 epoch 45.058485984802246 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1716\n",
            "Epoch 10 Batch 100 Loss 0.1075\n",
            "Epoch 10 Batch 200 Loss 0.0932\n",
            "Epoch 10 Batch 300 Loss 0.1006\n",
            "Epoch 10 Loss 0.1233\n",
            "Time taken for 1 epoch 45.842445850372314 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhAJ0Dl9DNPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Translating Spanish to English using the trained model.\n",
        "\n",
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden,\n",
        "                                                         enc_out)\n",
        "    attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0YhcrY1GGhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Translating a sentence.\n",
        "\n",
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "  print('Input: ', sentence)\n",
        "  print('Predicted translation: ', result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV_OGvezGV9_",
        "colab_type": "code",
        "outputId": "5e31bde6-433c-4240-ef15-ec6ebdc2f6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Restoring the model from the last checkpoint.\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f76ea2faeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d3QCeZzGajK",
        "colab_type": "code",
        "outputId": "af642a72-0e82-46dd-f59c-7a76432ec76e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Some translation examples:\n",
        "\n",
        "translate(u'esta es mi vida.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  <start> esta es mi vida . <end>\n",
            "Predicted translation:  <start> this is my life . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CVI3TL8eD3L",
        "colab_type": "code",
        "outputId": "d986783f-c7e1-404d-ada6-4b10f2730e17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  <start> hace mucho frio aqui . <end>\n",
            "Predicted translation:  <start> it s very cold here . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyux5aeifOzL",
        "colab_type": "code",
        "outputId": "b5ad6ed5-1184-412a-abbf-b8bd14038233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "translate(u'¿todavia estan en casa?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  <start> ¿ todavia estan en casa ? <end>\n",
            "Predicted translation:  <start> are you still home ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}