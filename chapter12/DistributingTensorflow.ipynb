{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistributingTensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJTQLhXE1TC2",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 12: Distributing TensorFlow Across Devices and Servers\n",
        "\n",
        "Since training a large DNN for a complex task on a single CPU can take days or even weeks, this chapter discusses distributing TensorFlow across multiple devices on the same machine then multiple devices across multiple machines. TensorFlow has built in support for distributed computing, making it an ideal machine learning framework for this task.\n",
        "\n",
        "## Multiple Devices on a Single Machine\n",
        "\n",
        "You can speed up training a neural network by adding multiple GPUs to your machine. In some cases, it is faster to train a neural network with 8 GPUs on a single machine than 16 GPUs on multiple machines, since network communications can slow down training.\n",
        "\n",
        "### Installation\n",
        "\n",
        "Below is code for installing Nvidia's _Compute Unified Device Architecture_ library (CUDA) in Google Colab. TensorFlow uses CUDA for using the GPU for training DNNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5rTfVZr5bRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxgbjTYNkySz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "38175e60-7633-44bb-9952-08073cca0731"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Wed_Apr_24_19:10:27_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chA96tjx3_QO",
        "colab_type": "text"
      },
      "source": [
        "The following code installs the GPU-enabled version of TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Qm5kLikIXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install --upgrade tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoN_7gmd2L7J",
        "colab_type": "text"
      },
      "source": [
        "### Managin the GPU RAM\n",
        "\n",
        "By default, TensorFlow grabs all the available RAM on GPUs the first time you run a graph. One option is to run each process on different GPU cards. Below is code for doing so:\n",
        "\n",
        "```bash\n",
        "CUDA_VISIBLE_DEVICES=0,1 python3 program1.py\n",
        "CUDA_VISIBLE_DEVICES=2,3 python3 program2.py\n",
        "```\n",
        "\n",
        "Another option is to tell TensorFlow to only use a fraction of the available memory. Code for doing so is below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZT-CIBNlHUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example code telling TensorFlow to grab only 40% of each GPU's memory\n",
        "# so that multiple TensorFlow programs can run.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
        "session = tf.Session(config=config)\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu-zyzx-lVOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatively you can have TensorFlow only grab memory when it needs to.\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.Session(config=config)\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_QDSkly4evG",
        "colab_type": "text"
      },
      "source": [
        "### Placing Operations on Devices\n",
        "\n",
        "The [TensorFlow whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf) presents a _dynamic placer_ algorithm that automatically distributes operations across all devices. This algorithm is internal to Google and is not released in the open source version of TensorFlow. This is due to the fact that in practice, a small set of placement rules specified by the user can perform just as well or better than dynamic placement.\n",
        "\n",
        "Until the dynamic placer is made public, the open source version of TensorFlow relies on the _simple placer_.\n",
        "\n",
        "#### Simple Placer\n",
        "\n",
        "Whenever you run a graph, if a node has not yet been placed, the simple placer will allocate the operation to a device using the following rules:\n",
        "\n",
        "- If a node has already been placed in a previous run of the graph, it is left on that device.\n",
        "\n",
        "- If the user _pinned_ a node to a device (described below) then the placer places it on that device.\n",
        "\n",
        "- Otherwise, it defaults to GPU #0 or the the CPU if there's no GPU.\n",
        "\n",
        "Below is an example of using TensorFlow to _pin_ a node to a device, in this case the code pins the variable `a` and the constant `b` on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsV06UHt84vU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7f51e2c5-dc70-4292-b486-3156f3119ea2"
      },
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  a = tf.Variable(3.0, name='a')\n",
        "  b = tf.constant(4.0, name='b')\n",
        "c = a * b"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HokV65x-7fZ",
        "colab_type": "text"
      },
      "source": [
        "#### Logging Placements\n",
        "\n",
        "Below is code for logging which device each node is pinned to. The code in the book does not work due to [this TenorFlow issue](https://github.com/tensorflow/tensorflow/issues/3047). Below is an example workaround from "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OGu5gNQEx5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wurlitzer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ1yHQmt_BGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3e41f191-42e7-45d6-a8a4-4d76681c38bd"
      },
      "source": [
        "from wurlitzer import pipes\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "with pipes() as (out, err):\n",
        "  print(sess.run(a.initializer))\n",
        "\n",
        "print (out.read())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "a: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "a/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "a/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "a/initial_value: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "b: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwNqFOWxGNKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27c86903-e618-4032-bd8c-ec3efba0ba5a"
      },
      "source": [
        "sess.run(c)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTltMR8HGOeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjoRTtxNGax2",
        "colab_type": "text"
      },
      "source": [
        "#### Dynamic Placement Function\n",
        "\n",
        "When you create a device block, you can also define a function which pins the nodes to devices. You can use this to implement more complex pinning algorithms such as pinning across GPUs in a round-robin fashion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUz-LK8IItNi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "465c1125-fab0-43ae-d8be-da165fcb958c"
      },
      "source": [
        "def variables_on_cpu(op):\n",
        "  if op.type == 'Variable':\n",
        "    return '/cpu:0'\n",
        "  return '/gpu:0'\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(variables_on_cpu):\n",
        "  a = tf.Variable(3.0)\n",
        "  b = tf.constant(4.0)\n",
        "  c = a * b\n",
        "  \n",
        "sess = tf.Session()\n",
        "sess.run(a.initializer)\n",
        "sess.run(c)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_bSrICeHtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaCbqek6KApK",
        "colab_type": "text"
      },
      "source": [
        "#### Operations and Kernels\n",
        "\n",
        "For a TensorFlow variable to run on a device, it needs to have an implementation, or a _kernel_, for that device. Many operations have kernels for GPUs and CPUs. Integer variables, however, do not have a kernel for the GPU. The following code illustrates this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4o3GJias9e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd29b7f7-1bc7-4421-eea4-e6a15fe0965e"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  i = tf.Variable(3)\n",
        "\n",
        "try:\n",
        "  sess = tf.Session()\n",
        "  sess.run(i.initializer)\n",
        "except Exception as ex:\n",
        "  print(type(ex).__name__)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "InvalidArgumentError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyN02Gi-gmtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgmzMXywgRVC",
        "colab_type": "text"
      },
      "source": [
        "#### Soft placement\n",
        "\n",
        "In order to prevent the exception being raised above, you can have TensorFlow fall back on the CPU instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZntUgPagP8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  i = tf.Variable(3)\n",
        "  \n",
        "config = tf.ConfigProto()\n",
        "config.allow_soft_placement = True\n",
        "sess = tf.Session(config=config)\n",
        "sess.run(i.initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLGQJq6Mgwzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NChj2h2eg3UO",
        "colab_type": "text"
      },
      "source": [
        "### Parallel Execution\n",
        "\n",
        "When TensorFlow evaluates a graph, it first evaluates all of the nodes with no dependencies, i.e. the source nodes. Once it evaluates a node which another depends on, the latter node's dependency counter decreases. Once it reaches zero, that node is evaluated. Once all of the nodes TensorFlow needs to evaluate are done, it outputs the result.\n",
        "\n",
        "For nodes evaluated on the CPU, the evaluations are dispatched into a queue in a thread pool called the _inter-op thread pool_. If the CPU has multiple cores, then the operations are executed in parallel. If the operations themselves have multithreaded kernels, then these kernels split their task into sub-operations which are placed in a queue in another thread pool called the _intra-op thread pool_.\n",
        "\n",
        "On the GPU, operations in the queue are evaluated sequentially. Operations which have multithreaded kernels are executed in parallel implemented by CUDA, cuDNN, and other GPU libraries that TensorFlow depends on.\n",
        "\n",
        "### Control Dependencies\n",
        "\n",
        "Sometimes, we do not want to evaluate nodes right when their dependency counter reaches zero. These nodes may take up a lot of compute resources to evaluate, and we may not need their values later. Or alternatively, some nodes rely on a lot of data not localized in the machine, so it may more make sense to evaluate them sequentially instead of in parallel.\n",
        "\n",
        "Below is an example of adding _control dependencies_ in a TensorFlow graph, i.e. nodes which need to wait on the evaluation of other nodes even if they do not directly depend on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr8SaXpkRPvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "a = tf.constant(1.0)\n",
        "b = a + 2.0\n",
        "\n",
        "with tf.control_dependencies([a, b]):\n",
        "  x = tf.constant(3.0)\n",
        "  y = tf.constant(4.0)\n",
        "  \n",
        "z = x + y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj87HhPARfrU",
        "colab_type": "text"
      },
      "source": [
        "Here, the evaluation of `z` depends on the evaluation of `a` and `b` even though `z`'s value does not depend on `a` or `b`. Since `b` depends on `a`, you need only list `b` as a control dependency, but sometimes it is better to be explicit."
      ]
    }
  ]
}