{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistributingTensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJTQLhXE1TC2",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 12: Distributing TensorFlow Across Devices and Servers\n",
        "\n",
        "Since training a large DNN for a complex task on a single CPU can take days or even weeks, this chapter discusses distributing TensorFlow across multiple devices on the same machine then multiple devices across multiple machines. TensorFlow has built in support for distributed computing, making it an ideal machine learning framework for this task.\n",
        "\n",
        "## Multiple Devices on a Single Machine\n",
        "\n",
        "You can speed up training a neural network by adding multiple GPUs to your machine. In some cases, it is faster to train a neural network with 8 GPUs on a single machine than 16 GPUs on multiple machines, since network communications can slow down training.\n",
        "\n",
        "### Installation\n",
        "\n",
        "Below is code for installing Nvidia's _Compute Unified Device Architecture_ library (CUDA) in Google Colab. TensorFlow uses CUDA for using the GPU for training DNNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5rTfVZr5bRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxgbjTYNkySz",
        "colab_type": "code",
        "outputId": "e2bb6a3d-86f4-429f-fd0c-418f285118ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Wed_Apr_24_19:10:27_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chA96tjx3_QO",
        "colab_type": "text"
      },
      "source": [
        "The following code installs the GPU-enabled version of TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Qm5kLikIXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install --upgrade tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoN_7gmd2L7J",
        "colab_type": "text"
      },
      "source": [
        "### Managin the GPU RAM\n",
        "\n",
        "By default, TensorFlow grabs all the available RAM on GPUs the first time you run a graph. One option is to run each process on different GPU cards. Below is code for doing so:\n",
        "\n",
        "```bash\n",
        "CUDA_VISIBLE_DEVICES=0,1 python3 program1.py\n",
        "CUDA_VISIBLE_DEVICES=2,3 python3 program2.py\n",
        "```\n",
        "\n",
        "Another option is to tell TensorFlow to only use a fraction of the available memory. Code for doing so is below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZT-CIBNlHUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example code telling TensorFlow to grab only 40% of each GPU's memory\n",
        "# so that multiple TensorFlow programs can run.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
        "session = tf.Session(config=config)\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu-zyzx-lVOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatively you can have TensorFlow only grab memory when it needs to.\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.Session(config=config)\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_QDSkly4evG",
        "colab_type": "text"
      },
      "source": [
        "### Placing Operations on Devices\n",
        "\n",
        "The [TensorFlow whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf) presents a _dynamic placer_ algorithm that automatically distributes operations across all devices. This algorithm is internal to Google and is not released in the open source version of TensorFlow. This is due to the fact that in practice, a small set of placement rules specified by the user can perform just as well or better than dynamic placement.\n",
        "\n",
        "Until the dynamic placer is made public, the open source version of TensorFlow relies on the _simple placer_.\n",
        "\n",
        "#### Simple Placer\n",
        "\n",
        "Whenever you run a graph, if a node has not yet been placed, the simple placer will allocate the operation to a device using the following rules:\n",
        "\n",
        "- If a node has already been placed in a previous run of the graph, it is left on that device.\n",
        "\n",
        "- If the user _pinned_ a node to a device (described below) then the placer places it on that device.\n",
        "\n",
        "- Otherwise, it defaults to GPU #0 or the the CPU if there's no GPU.\n",
        "\n",
        "Below is an example of using TensorFlow to _pin_ a node to a device, in this case the code pins the variable `a` and the constant `b` on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsV06UHt84vU",
        "colab_type": "code",
        "outputId": "017ede31-6e34-4a44-cd5d-4b4ce49c88d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  a = tf.Variable(3.0, name='a')\n",
        "  b = tf.constant(4.0, name='b')\n",
        "c = a * b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HokV65x-7fZ",
        "colab_type": "text"
      },
      "source": [
        "#### Logging Placements\n",
        "\n",
        "Below is code for logging which device each node is pinned to. The code in the book does not work due to [this TenorFlow issue](https://github.com/tensorflow/tensorflow/issues/3047). Below is an example workaround from "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OGu5gNQEx5E",
        "colab_type": "code",
        "outputId": "55161c90-64cb-4c91-c598-7ea8dab9858a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install wurlitzer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wurlitzer\n",
            "  Downloading https://files.pythonhosted.org/packages/60/17/de2820542c755f4630a58d295daad86bfa981fbf48b48e5f9e1f2ed806cc/wurlitzer-1.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: wurlitzer\n",
            "Successfully installed wurlitzer-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ1yHQmt_BGV",
        "colab_type": "code",
        "outputId": "a798d952-e7e0-40bb-b1d3-32e60004d923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from wurlitzer import pipes\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "with pipes() as (out, err):\n",
        "  print(sess.run(a.initializer))\n",
        "\n",
        "print (out.read())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "a: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "a/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "a/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "a/initial_value: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "b: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwNqFOWxGNKF",
        "colab_type": "code",
        "outputId": "46bbc380-5606-4cd9-b0e3-b50aa6a312ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sess.run(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTltMR8HGOeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjoRTtxNGax2",
        "colab_type": "text"
      },
      "source": [
        "#### Dynamic Placement Function\n",
        "\n",
        "When you create a device block, you can also define a function which pins the nodes to devices. You can use this to implement more complex pinning algorithms such as pinning across GPUs in a round-robin fashion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUz-LK8IItNi",
        "colab_type": "code",
        "outputId": "b8bf2df5-9c81-4275-a53e-9898dc629de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def variables_on_cpu(op):\n",
        "  if op.type == 'Variable':\n",
        "    return '/cpu:0'\n",
        "  return '/gpu:0'\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(variables_on_cpu):\n",
        "  a = tf.Variable(3.0)\n",
        "  b = tf.constant(4.0)\n",
        "  c = a * b\n",
        "  \n",
        "sess = tf.Session()\n",
        "sess.run(a.initializer)\n",
        "sess.run(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_bSrICeHtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaCbqek6KApK",
        "colab_type": "text"
      },
      "source": [
        "#### Operations and Kernels\n",
        "\n",
        "For a TensorFlow variable to run on a device, it needs to have an implementation, or a _kernel_, for that device. Many operations have kernels for GPUs and CPUs. Integer variables, however, do not have a kernel for the GPU. The following code illustrates this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4o3GJias9e",
        "colab_type": "code",
        "outputId": "be2eb431-3227-453a-9008-8add6e31fe6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  i = tf.Variable(3)\n",
        "\n",
        "try:\n",
        "  sess = tf.Session()\n",
        "  sess.run(i.initializer)\n",
        "except Exception as ex:\n",
        "  print(type(ex).__name__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "InvalidArgumentError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyN02Gi-gmtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgmzMXywgRVC",
        "colab_type": "text"
      },
      "source": [
        "#### Soft placement\n",
        "\n",
        "In order to prevent the exception being raised above, you can have TensorFlow fall back on the CPU instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZntUgPagP8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  i = tf.Variable(3)\n",
        "  \n",
        "config = tf.ConfigProto()\n",
        "config.allow_soft_placement = True\n",
        "sess = tf.Session(config=config)\n",
        "sess.run(i.initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLGQJq6Mgwzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NChj2h2eg3UO",
        "colab_type": "text"
      },
      "source": [
        "### Parallel Execution\n",
        "\n",
        "When TensorFlow evaluates a graph, it first evaluates all of the nodes with no dependencies, i.e. the source nodes. Once it evaluates a node which another depends on, the latter node's dependency counter decreases. Once it reaches zero, that node is evaluated. Once all of the nodes TensorFlow needs to evaluate are done, it outputs the result.\n",
        "\n",
        "For nodes evaluated on the CPU, the evaluations are dispatched into a queue in a thread pool called the _inter-op thread pool_. If the CPU has multiple cores, then the operations are executed in parallel. If the operations themselves have multithreaded kernels, then these kernels split their task into sub-operations which are placed in a queue in another thread pool called the _intra-op thread pool_.\n",
        "\n",
        "On the GPU, operations in the queue are evaluated sequentially. Operations which have multithreaded kernels are executed in parallel implemented by CUDA, cuDNN, and other GPU libraries that TensorFlow depends on.\n",
        "\n",
        "### Control Dependencies\n",
        "\n",
        "Sometimes, we do not want to evaluate nodes right when their dependency counter reaches zero. These nodes may take up a lot of compute resources to evaluate, and we may not need their values later. Or alternatively, some nodes rely on a lot of data not localized in the machine, so it may more make sense to evaluate them sequentially instead of in parallel.\n",
        "\n",
        "Below is an example of adding _control dependencies_ in a TensorFlow graph, i.e. nodes which need to wait on the evaluation of other nodes even if they do not directly depend on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr8SaXpkRPvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "a = tf.constant(1.0)\n",
        "b = a + 2.0\n",
        "\n",
        "with tf.control_dependencies([a, b]):\n",
        "  x = tf.constant(3.0)\n",
        "  y = tf.constant(4.0)\n",
        "  \n",
        "z = x + y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj87HhPARfrU",
        "colab_type": "text"
      },
      "source": [
        "Here, the evaluation of `z` depends on the evaluation of `a` and `b` even though `z`'s value does not depend on `a` or `b`. Since `b` depends on `a`, you need only list `b` as a control dependency, but sometimes it is better to be explicit.\n",
        "\n",
        "## Distributing Devices Across Multiple Servers\n",
        "\n",
        "In order to run a graph across multiple devices, you need to define a _cluster_ i.e. a group of TensorFlow servers called _tasks_ spread across several machines. Each task belongs to a _job_ i.e. a group of tasks which perform a common role.\n",
        "\n",
        "The following code defines a _cluster specification_ which defines two jobs: `ps` and `worker`, the former is a _parameter server_ which records the model parameters whereas workers perform computations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i22nF68jDbBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_spec = tf.train.ClusterSpec({\n",
        "    'ps': [\n",
        "        '127.0.0.1:2221',\n",
        "        '127.0.0.1:2222',\n",
        "    ],\n",
        "    'worker': [\n",
        "        '127.0.0.1:2223',\n",
        "        '127.0.0.1:2224',\n",
        "        '127.0.0.1:2225',\n",
        "    ],\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luu_auK7DsAH",
        "colab_type": "text"
      },
      "source": [
        "The following code instantiates a TensorFlow `Server` object by passing it a cluster spec and then parameters to indicate its job and task number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OizKOpX0DrHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps0 = tf.train.Server(cluster_spec, job_name='ps', task_index=0)\n",
        "ps1 = tf.train.Server(cluster_spec, job_name='ps', task_index=1)\n",
        "worker0 = tf.train.Server(cluster_spec, job_name='worker', task_index=0)\n",
        "worker1 = tf.train.Server(cluster_spec, job_name='worker', task_index=1)\n",
        "worker2 = tf.train.Server(cluster_spec, job_name='worker', task_index=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "736lvOMgFDHk",
        "colab_type": "text"
      },
      "source": [
        "Typically you run one task per machine, but you can run multiple tasks per machine as long as you ensure that they don't all try to use all of the RAM on each GPU.\n",
        "\n",
        "If you want the process to do nothing other than run the TensorFlow server, you can block the main thread by using the `join()` method:\n",
        "\n",
        "```python\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkLK3DVt-T65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will block the main thread until the server finishes.\n",
        "\n",
        "server.join()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpfAQFxuGCs4",
        "colab_type": "text"
      },
      "source": [
        "### Opening a Session\n",
        "\n",
        "Once all of the tasks are up and running, you can open a session on any of the servers from a client on any machine using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2j7Lf9NGRDS",
        "colab_type": "code",
        "outputId": "7ee38934-9a57-4019-bf4c-640368ad2b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = tf.constant(1.0)\n",
        "b = a + 2\n",
        "c = b * 3\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2223') as sess:\n",
        "  print(c.eval())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw5v9CCIGmG8",
        "colab_type": "text"
      },
      "source": [
        "The code creates a simple graph then opens a session on machine B and evaluates `c`. The master places operations on the appropriate device, if we do not pin the operation to a particular device then the master will place it on the machine's default device.\n",
        "\n",
        "### The Master and Worker Services\n",
        "\n",
        "The client uses _gRPC_ to communicate with the server. A protocol which uses HTTP2 to open a lasting connection for bidirectional communication. It exchanges data using _protocol buffers_.\n",
        "\n",
        "Every TensorFlow server provides two services: a _master service_ and a _worker service_. The master allows clients to open sessions and run graphs whereas the worker service actually performs computations. This architecture allows a server to open multiple sessions from one or more clients.\n",
        "\n",
        "### Pinning Operations Across Tasks\n",
        "\n",
        "Below is an example of using a device block to pin an operation to a particular task and to a particular device on that task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g-DLMR0NUr5",
        "colab_type": "code",
        "outputId": "ee22d584-bab9-48e2-f817-a88ebddac90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# This block pins `a` to task 0 of the `ps` job's CPU.\n",
        "with tf.device('/job:ps/task:0/cpu:0'):\n",
        "  a = tf.constant(1.0)\n",
        "  \n",
        "# This block pins `b` to task 1 of the `worker` job's GPU.\n",
        "with tf.device('/job:worker/task:1/gpu:0'):\n",
        "  b = a + 2\n",
        "\n",
        "c = a + b\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2225') as sess:\n",
        "  print(c.eval())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3yEESRRRKE",
        "colab_type": "text"
      },
      "source": [
        "### Sharding Variables Across Multiple Parameter Servers\n",
        "\n",
        "It is common to have a _parameter server_ job to store parameters while training a complex model. Some models, like DNNs, can have thousands or even millions of parameters. To avoid network saturation, it is common to distribute storing parameters across multiple servers.\n",
        "\n",
        "Since manually pinning every variable to a different task can be tedious, TensorFlow provides a `replica_device_setter()` which distributes variables across servers. Below is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6e6VnSGRzwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(tf.train.replica_device_setter(ps_tasks=2)):\n",
        "  v1 = tf.Variable(1.0) # pinned to /job:ps/task:0\n",
        "  v2 = tf.Variable(2.0) # pinned to /job:ps/task:1\n",
        "  v3 = tf.Variable(3.0) # pinned to /job:ps/task:0\n",
        "  v4 = tf.Variable(4.0) # pinned to /job:ps/task:1\n",
        "  v5 = tf.Variable(5.0) # pinned to /job:ps/task:0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OxduM4eSIM2",
        "colab_type": "text"
      },
      "source": [
        "Alternatively you can pass the cluster spec and TensorFlow will automatically compute the number of tasks in the `ps` job.\n",
        "\n",
        "If you create operations that are not just variables, then by default they are pinned to `/job:worker` which will default to the first device of the first worker task. You can pin them to devices using device blocks. Below is an example of a graph pinned to multiple tasks and multiple devices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2SyOjSbXiVb",
        "colab_type": "code",
        "outputId": "4b014486-0a81-401b-f359-9e7206e2fb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(tf.train.replica_device_setter(ps_tasks=2, ps_device='/job:ps',\n",
        "                                              worker_device='/job:worker')):\n",
        "  v1 = tf.Variable(1.0) # pinned to /job:ps/task:0\n",
        "  v2 = tf.Variable(2.0) # pinned to /job:ps/task:1\n",
        "  v3 = tf.Variable(3.0) # pinned to /job:ps/task:0\n",
        "  \n",
        "  s = v1 + v2 # pinned to /job:worker/task:0/cpu:0\n",
        "  \n",
        "  with tf.device('/gpu:0'):\n",
        "    p1 = 2 * s # pinned to /job:worker/task:0/gpu:0\n",
        "    \n",
        "    with tf.device('/task:1'):\n",
        "      p2 = 3 * s # pinned to /job:worker/task:1/cpu:0\n",
        "      \n",
        "with tf.Session('grpc://127.0.0.1:2221') as sess:\n",
        "  v1.initializer.run()\n",
        "  v2.initializer.run()\n",
        "  print(s.eval())\n",
        "  print(p1.eval())\n",
        "  print(p2.eval())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "3.0\n",
            "6.0\n",
            "9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yeuDp4DbJHZ",
        "colab_type": "text"
      },
      "source": [
        "### Sharing State Across Sessions Using Resource Containers\n",
        "\n",
        "When using a plain _local session_, variables values are stored in the session object, so when the session ends the values are deleted. Moreover multiple local sessions cannot share any state, even if they run the same graph.\n",
        "\n",
        "When you are using _distributed sessions_, variable state is managed by _resource containers_ located on the cluster and persist across sessions. An example of this is given by the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utrMhotjkHdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.Variable(0.0, name='x')\n",
        "increment_x = tf.assign(x, x + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDO7AO4Ykf2X",
        "colab_type": "code",
        "outputId": "6d076ac8-3ef8-4bef-af7a-6a04f221f26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(x.initializer)\n",
        "  sess.run(increment_x)\n",
        "  print(x.eval())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C2hcJwmk5cE",
        "colab_type": "code",
        "outputId": "935f181c-8f6a-4587-b02b-fe9beb59c7df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(increment_x)\n",
        "  print(x.eval())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4la_pAale00",
        "colab_type": "text"
      },
      "source": [
        "While this feature can be convenient, you have to be careful to not use the same variable names by accident. One way to avoid this is by using a variable scope with a unique name for each computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS1AIxlRlu22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.variable_scope('problem_1'):\n",
        "  x1 = tf.Variable(0.0, name='x')\n",
        "  increment_x1 = tf.assign(x1, x1 + 1)\n",
        "  \n",
        "with tf.variable_scope('problem_2'):\n",
        "  x2 = tf.Variable(0.0, name='x')\n",
        "  increment_x2 = tf.assign(x2, x2 + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4KFk7oA_nVh",
        "colab_type": "code",
        "outputId": "a646b52c-c2fd-4c4e-b5f0-03bbfe6e630a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  x1.initializer.run()\n",
        "  print(increment_x1.eval())\n",
        "  \n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  x2.initializer.run()\n",
        "  print(increment_x2.eval())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqYDwyvQm9Ma",
        "colab_type": "text"
      },
      "source": [
        "You can even use resource containers to store variables across different graphs. In order to reset a resource container, run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSbuQGC0-kk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222', ['problem_1'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mrKVhJN_2Ko",
        "colab_type": "text"
      },
      "source": [
        "### Asynchronous Communication Using TensorFlow Queues\n",
        "\n",
        "Queues are a way to share data across multiple sessions. One common use case is for passing mini-batches of data between sessions for training. One graph may load the client data and push it to the queue where another pulls the data and trains a neural network.\n",
        "\n",
        "TensorFlow provides various kinds of queues, the most simple is the _first-in first-out_ (FIFO) queue. Below is an example of a FIFO queue with TensorFlow that can hold up to 10 tensors containing 2 floats:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeFYlcBnAtOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[[2]], name='q',\n",
        "                 shared_name='shared_q')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk-lzJrRBKOq",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow uses the `shared_name` parameter to refer to queues across sessions.\n",
        "\n",
        "#### Enqueuing data\n",
        "\n",
        "To push data into the queue, you need to use the `enqueue` method. The following code pushes data to the queue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyOJwczEBfwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_instance = tf.placeholder(tf.float32, shape=[2])\n",
        "enqueue = q.enqueue([training_instance])\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue, feed_dict={training_instance: [1.0, 2.0]})\n",
        "  sess.run(enqueue, feed_dict={training_instance: [3.0, 4.0]})\n",
        "  sess.run(enqueue, feed_dict={training_instance: [5.0, 6.0]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXQsiXhGCIuW",
        "colab_type": "text"
      },
      "source": [
        "You can enqueue multiple tensors at once using the `enqueue_many` method below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY0qulU5CNRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_instances = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "enqueue_many = q.enqueue_many([training_instances])\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue_many, feed_dict={\n",
        "      training_instances: [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]],\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw_9xtuNC_Li",
        "colab_type": "text"
      },
      "source": [
        "#### Dequeuing data\n",
        "\n",
        "You use the `dequeue` method to pull tensors from the queue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srZ59XYNDMgQ",
        "colab_type": "code",
        "outputId": "13e28e92-70ba-4b3f-f44d-fa475835663b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "dequeue = q.dequeue()\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  print(sess.run(dequeue))\n",
        "  print(sess.run(dequeue))\n",
        "  print(sess.run(dequeue))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 2.]\n",
            "[3. 4.]\n",
            "[5. 6.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCUuezuQE1LG",
        "colab_type": "text"
      },
      "source": [
        "In order to dequeue multiple items at once, you should use the `dequeue_many` method and specify how many items to dequeue each time. If you call it when there are not enough items in the queue, it will block execution until the queue has the specified amount."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhEb7sQ2FFMY",
        "colab_type": "code",
        "outputId": "3618344e-c763-4de1-a8f3-a4b44a600457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "batch_size = 2\n",
        "dequeue_mini_batch = q.dequeue_many(batch_size)\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  print(sess.run(dequeue_mini_batch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2.]\n",
            " [3. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpgVTnC-Ft0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-oOfS4XF6Bk",
        "colab_type": "text"
      },
      "source": [
        "#### Queues of tuples\n",
        "\n",
        "Queues can also hold tuples of tensors of different shapes instead of a single tensor. The following queue stores two tensors: a scalar `int32` tensor and a `float32` tensor with shape `[3,2]`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DziNS45ZGbM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = tf.FIFOQueue(capacity=10, dtypes=[tf.int32, tf.float32], shapes=[[], [3,2]],\n",
        "                 name='q', shared_name='shared_q')\n",
        "\n",
        "a = tf.placeholder(tf.int32, shape=())\n",
        "b = tf.placeholder(tf.float32, shape=(3, 2))\n",
        "enqueue = q.enqueue((a, b))\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue, feed_dict={a: 10, b: [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]})\n",
        "  sess.run(enqueue, feed_dict={a: 11, b: [[2.0, 4.0], [6.0, 8.0], [0.0, 2.0]]})\n",
        "  sess.run(enqueue, feed_dict={a: 12, b: [[3.0, 6.0], [9.0, 2.0], [5.0, 8.0]]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUSEExcJHqlq",
        "colab_type": "text"
      },
      "source": [
        "The `dequeue` method now creates a tuple of operations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6itG72fHwL4",
        "colab_type": "code",
        "outputId": "f543c262-e7a4-45ef-ba3b-d234114503e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "dequeue_a, dequeue_b = q.dequeue()\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  a, b = sess.run([dequeue_a, dequeue_b])\n",
        "  print(a)\n",
        "  print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "[[1. 2.]\n",
            " [3. 4.]\n",
            " [5. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdyGGFfnIvA0",
        "colab_type": "text"
      },
      "source": [
        "`dequeue_many` also returns atuple of operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdVKqjqMI92E",
        "colab_type": "code",
        "outputId": "85540d3d-f243-4cfe-e57d-16fac6961d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "batch_size = 2\n",
        "dequeue_as, dequeue_bs = q.dequeue_many(batch_size)\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  a, b = sess.run([dequeue_as, dequeue_bs])\n",
        "  print(a)\n",
        "  print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11 12]\n",
            "[[[2. 4.]\n",
            "  [6. 8.]\n",
            "  [0. 2.]]\n",
            "\n",
            " [[3. 6.]\n",
            "  [9. 2.]\n",
            "  [5. 8.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kys29N-2KrU-",
        "colab_type": "text"
      },
      "source": [
        "#### Closing a queue\n",
        "\n",
        "It is possible to close a queue so that the other sessions can no longer enqueue data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eKS-of1K0Tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close_q = q.close()\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(close_q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL4501StMha_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKWQpel3K86w",
        "colab_type": "text"
      },
      "source": [
        "Attempting to push to a closed queue will raise an exception, but any pending enqueues will be honored unless you call `q.close(cancel_pending_enqueues=True)`. Subsequent dequeue operations will still work as long as there are enough itmes in the queue as long as there are at least as many items left in the queue, otherwise the operation will fail.\n",
        "\n",
        "You can use the `dequeue_up_to` method instead which will empty the queue if there are less than `batch_size` items left.\n",
        "\n",
        "#### RandomShuffleQueue\n",
        "\n",
        "Another type of queue that TensorFlow supports is the `RandomShuffleQueue` which returns items in the queue in random order. Below is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFR57kkxMflJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = tf.RandomShuffleQueue(capacity=50, min_after_dequeue=10,\n",
        "                          dtypes=[tf.float32], shapes=[()], name='q',\n",
        "                          shared_name='shared_q')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIGkLbx9N5xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_instances = tf.placeholder(tf.float32, shape=(None))\n",
        "enqueue = q.enqueue_many([training_instances])\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue, feed_dict={\n",
        "      training_instances: [float(i) for i in range(25)]\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sca_lNhM-_7",
        "colab_type": "text"
      },
      "source": [
        "The `min_after_dequeue` specifies the minimum number of items that should be left in the queue after a dequeue operation to ensure the random behavior of queue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d18Nk8egPPC-",
        "colab_type": "code",
        "outputId": "9d805b24-a0ec-4ba5-9f3f-fa9609813335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "batch_size = 5\n",
        "dequeue = q.dequeue_many(batch_size)\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  print(sess.run(dequeue))\n",
        "  print(sess.run(dequeue))\n",
        "  print(sess.run(dequeue))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 9. 12.  7. 24.  3.]\n",
            "[18. 19. 14.  8. 11.]\n",
            "[22.  2. 17. 10.  5.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzpnQnz0PjAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqV9YperPpQ5",
        "colab_type": "text"
      },
      "source": [
        "#### PaddingFIFOQueue\n",
        "\n",
        "A `PaddingFIFOQueue` is a queue which holds tensors of any dimension as long as they are the same rank. When you dequeue them individually the tensors come back as they were enqueued. When you use `dequeue_many` or `dequeue_up_to`, each tensor is padded with zeros so that they are each the same size as the largest tensor in the mini-batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-PTmYV7SQG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = tf.PaddingFIFOQueue(capacity=50, dtypes=[tf.float32],\n",
        "                        shapes=[(None, None)], name='q',\n",
        "                        shared_name='shared_q')\n",
        "v = tf.placeholder(tf.float32, shape=(None, None))\n",
        "enqueue = q.enqueue([v])\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue, feed_dict={v: [[1., 2.], [3., 4.], [5., 6.]]})\n",
        "  sess.run(enqueue, feed_dict={v: [[1.]]})\n",
        "  sess.run(enqueue, feed_dict={v: [[7., 8., 9., 5.], [6., 7., 8., 9.]]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwQXgPBhTP-j",
        "colab_type": "code",
        "outputId": "46a44b73-cab2-4fd5-d746-a6e6dc6e045e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "batch_size = 3\n",
        "dequeue = q.dequeue_many(batch_size)\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  print(sess.run(dequeue))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1. 2. 0. 0.]\n",
            "  [3. 4. 0. 0.]\n",
            "  [5. 6. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[7. 8. 9. 5.]\n",
            "  [6. 7. 8. 9.]\n",
            "  [0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAbp4-PVTZR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czQJC7dPTcFW",
        "colab_type": "text"
      },
      "source": [
        "This type of queue is useful for variable length inputs such as sequences of words.\n",
        "\n",
        "### Loading Data Directly from the Graph\n",
        "\n",
        "So far, we have only fed training data to the TensorFlow clusters using placeholders, which involves three steps:\n",
        "\n",
        "1. Load the data from the filesystem to the client task.\n",
        "\n",
        "2. Send the data from the client to the master task.\n",
        "\n",
        "3. Send the data from the master task to other tasks which need the data for computation.\n",
        "\n",
        "This process can be very inefficient for large datasets or when the training graph is distributed across many tasks.\n",
        "\n",
        "#### Preload the data into a variable\n",
        "\n",
        "If the dataset fits into memory, one option is to load the training data into a variable and use that variable in your graph. This is called _preloading_ the training set. This way the data only needs to be upload the data to the cluster once, though it may need to be transferred across tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd59nVWUmsb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "data = \\\n",
        "  [[0., 1., 2., 3., 4.],\n",
        "   [5., 6., 7., 8., 9.],\n",
        "   [10., 11., 12., 13., 14.],\n",
        "   [15., 16., 17., 18., 19.]]\n",
        "\n",
        "training_set_init = tf.placeholder(tf.float32, shape=(4, 5))\n",
        "training_set = tf.Variable(training_set_init, trainable=False, collections=[],\n",
        "                           name='training_set')\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(training_set.initializer, feed_dict={training_set_init: data})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Awyb8mw8KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvRCsQ13qZ6d",
        "colab_type": "text"
      },
      "source": [
        "Setting `trainable=False` so optimizers don't change the value and `collections=[]` prevents a `Saver` from storing the value of the variable in memory.\n",
        "\n",
        "\n",
        "#### Reading the training data directly from the graph\n",
        "\n",
        "Below is an example of using TensorFlow to read data from a CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VITLIOx8q6ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Writing the CSV file.\n",
        "\n",
        "csv_data = \\\n",
        "  'x1, x2, target\\n' \\\n",
        "  '1., 2., 0\\n' \\\n",
        "  '4., 5., 1\\n' \\\n",
        "  '7., , 0'\n",
        "csv_filename = 'my_test.csv'\n",
        "\n",
        "with open(csv_filename, 'w') as f:\n",
        "  f.write(csv_data)\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Za0zkQZrln5",
        "colab_type": "code",
        "outputId": "3fe36c4c-ad1f-45ac-bd30-501584fbd6d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Creating a TextLineReader object, a stateful object which reads data\n",
        "# from a file.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "reader = tf.TextLineReader(skip_header_lines=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-72d2ad0d8bc6>:4: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh88PmmOsu5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a queue to keep track of which files we want to read from.\n",
        "# Including a placeholder for the filename, an enqueue operation, and\n",
        "# a close operation.\n",
        "\n",
        "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
        "filename = tf.placeholder(tf.string)\n",
        "enqueue_filename = filename_queue.enqueue([filename])\n",
        "close_filename_queue = filename_queue.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HtH95KZuIAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the read operation using the Reader object. The key is a unique for\n",
        "# each record (filename:line_number) and the value is a string containing the\n",
        "# content of the line.\n",
        "\n",
        "key, value = reader.read(filename_queue)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kz939lTvHLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parsing the string to create the training set features\n",
        "\n",
        "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
        "features = tf.stack([x1, x2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvkJjjzwvXbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally push the training instance to a RandomShuffleQueue that will be\n",
        "# part of the training graph.\n",
        "\n",
        "instance_queue = tf.RandomShuffleQueue(capacity=10, min_after_dequeue=2,\n",
        "                                       dtypes=[tf.float32, tf.int32],\n",
        "                                       shapes=[[2], []], name='instance_q',\n",
        "                                       shared_name='shared_instance_q')\n",
        "enqueue_instance = instance_queue.enqueue([features, target])\n",
        "close_instance_queue = instance_queue.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1_9_N9iwsrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example of a Session that adds training instances to the instance queue\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue_filename, feed_dict={filename: csv_filename})\n",
        "  sess.run(close_filename_queue)\n",
        "  try:\n",
        "    while True:\n",
        "      sess.run(enqueue_instance)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "  sess.run(close_instance_queue)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XWZPJry4cFR",
        "colab_type": "code",
        "outputId": "2f60e6cf-c588-497c-e664-006cc5a00cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# An example of reading training data from the instance queue.\n",
        "\n",
        "mini_batch_instances, mini_batch_targets = instance_queue.dequeue_up_to(2)\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  try:\n",
        "    while True:\n",
        "      print(sess.run([mini_batch_instances, mini_batch_targets]))\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[4., 5.],\n",
            "       [1., 2.]], dtype=float32), array([1, 0], dtype=int32)]\n",
            "[array([[7., 0.]], dtype=float32), array([0], dtype=int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFKqTS0441RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amj_k8f6yKMp",
        "colab_type": "text"
      },
      "source": [
        "In addition to CSV files, you can also have TensorFlow read from fixed-length binary records or TensorFlow's TFRecord format which is baed on protocol buffers.\n",
        "\n",
        "One limitation of this architecture uses only one thread to read records and push them to the instance queue. \n",
        "\n",
        "#### Multithreaded readers using a Coordinator and a QueueRunner\n",
        "\n",
        "Below is an example of using TensorFlow's `Coordinator` and `QueueRunner` classes which are used to read data from multiple threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpEK0MQs0aT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
        "filename = tf.placeholder(tf.string)\n",
        "enqueue_filename = filename_queue.enqueue([filename])\n",
        "close_filename_queue = filename_queue.close()\n",
        "\n",
        "reader = tf.TextLineReader(skip_header_lines=1)\n",
        "\n",
        "key, value = reader.read(filename_queue)\n",
        "\n",
        "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
        "features = tf.stack([x1, x2])\n",
        "\n",
        "instance_queue = tf.RandomShuffleQueue(capacity=10, min_after_dequeue=2,\n",
        "                                       dtypes=[tf.float32, tf.int32],\n",
        "                                       shapes=[[2], []], name='instance_q',\n",
        "                                       shared_name='shared_instance_q')\n",
        "enqueue_instance = instance_queue.enqueue([features, target])\n",
        "close_instance_queue = instance_queue.close()\n",
        "\n",
        "mini_batch_instances, mini_batch_targets = instance_queue.dequeue_up_to(2)\n",
        "\n",
        "# New code here!\n",
        "n_threads = 5\n",
        "queue_runner = tf.train.QueueRunner(instance_queue,\n",
        "                                    [enqueue_instance] * n_threads)\n",
        "coord = tf.train.Coordinator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLXczJVj5XZL",
        "colab_type": "code",
        "outputId": "cba24932-6ac3-40cb-daab-ff0ab6d50a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue_filename, {filename: csv_filename})\n",
        "  sess.run(close_filename_queue)\n",
        "  enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
        "  try:\n",
        "    while True:\n",
        "      print(sess.run([mini_batch_instances, mini_batch_targets]))\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[4., 5.],\n",
            "       [7., 0.]], dtype=float32), array([1, 0], dtype=int32)]\n",
            "[array([[1., 2.]], dtype=float32), array([0], dtype=int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcZiIbq65V-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUid5mt-5pHH",
        "colab_type": "text"
      },
      "source": [
        "Another way you can further parallelize this process is by sharding your training set into multiple CSV files and read from multiple file queues. The following code defines a function which creates a reader which pushes data from a file queue to an instance queue. In order to read from multiple files in parallel, simply provide multiple file queues to the `QueueRunner`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNlEafWh8s-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function to read from a file queue and push to an instance queue.\n",
        "\n",
        "def read_and_push_instance(file_queue, instance_queue, skip_header_lines=1):\n",
        "  reader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n",
        "  key, value = reader.read(file_queue)\n",
        "  x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
        "  features = tf.stack([x1, x2])\n",
        "  enqueue_instance = instance_queue.enqueue([features, target])\n",
        "  return enqueue_instance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8LcDZdN9LVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the TensorFlow graph to read from the file.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
        "filename = tf.placeholder(tf.string)\n",
        "enqueue_filename = filename_queue.enqueue([filename])\n",
        "close_filename_queue = filename_queue.close()\n",
        "\n",
        "instance_queue = tf.RandomShuffleQueue(capacity=10, min_after_dequeue=2,\n",
        "                                       dtypes=[tf.float32, tf.int32],\n",
        "                                       shapes=[[2], []], name='instance_q',\n",
        "                                       shared_name='shared_instance_q')\n",
        "\n",
        "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
        "\n",
        "read_and_enqueue_ops = [read_and_push_instance(filename_queue, instance_queue)\n",
        "                        for _ in range(5)]\n",
        "\n",
        "queue_runner = tf.train.QueueRunner(instance_queue, read_and_enqueue_ops)\n",
        "coord = tf.train.Coordinator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5iuoeZR-7Rx",
        "colab_type": "code",
        "outputId": "897a3b83-565e-4c44-c992-e83f55953c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue_filename, {filename: csv_filename})\n",
        "  sess.run(close_filename_queue)\n",
        "  enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
        "  try:\n",
        "    while True:\n",
        "      print(sess.run([minibatch_instances, minibatch_targets]))\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[1., 2.],\n",
            "       [7., 0.]], dtype=float32), array([0, 0], dtype=int32)]\n",
            "[array([[4., 5.]], dtype=float32), array([1], dtype=int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yXCmoeE_W7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQysTLDPrvRp",
        "colab_type": "text"
      },
      "source": [
        "#### Other convenience functions\n",
        "\n",
        "TensorFlow offers the following convenience functions to simplify common tasks for reading training data. Below are a list of a few:\n",
        "\n",
        "- `string_input_producer()` takes a 1D tensor that is a list of filenames and creates a thread that pushes one filename at a time to the filename queue and closes the queue afterwards. If you specify a number of epochs, it will go through the each name once per epoch. It also shuffles the filenames at each epoch. The function adds a `QueueRunner` to to the `Graph.Keys.QUEUE_RUNNERS` collection. To start all `QueueRunner` instances, call `tf.train.start_queue_runners()` which is required in order to start training.\n",
        "\n",
        "- `input_producer()`, `range_input_producer()`, and `slice_input_producer()` all create a queue and corresponding `QueueRunner` for running the enqueue operations.\n",
        "\n",
        "- `shuffle_batch()` takes a list of tensors (i.e. `[features, target]`) and produces a `RandomShuffleQueue`, a `QueueRunner` to enqueue the tensors, and a `dequeue_many` operartion to extract mini-batches. `batch()`, `batch_join()`, and `shuffle_batch_join()` provide similar functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arzLBzAEt6tf",
        "colab_type": "text"
      },
      "source": [
        "## Parallelizing Neural Networks on a TensorFlow Cluster\n",
        "\n",
        "### One Neural Network per Device\n",
        "\n",
        "The simplest form of parallelization is to run the same neural network on multiple devices. This allows you to train multiple versions of the same model in the same amount of time it takes to train a single model on a single device. This is ideal for both hyperparameter tuning.\n",
        "\n",
        "You can also use this parallelization scheme in order to handle high traffic to a web service which uses the trained model to make predictions.\n",
        "\n",
        "### In-Graph Versus Between-Graph Replication\n",
        "\n",
        "_In-graph replication_ is a parallelization technique for training a neural network ensemble model by defining the same graph to run on multiple devices, then aggregating each device's model's result to reach the final prediction.\n",
        "\n",
        "_Between-graph replication_ is when you define a different graph for each device, then coordinate the execution of the different graphs using queues. One client coordinates enqueuing all of the input data and another client aggregates each graph's computation into the final result.\n",
        "\n",
        "In-graph replication is generally easier to implement, but between-graph replication allows you to structure your model in a more modular way. Between-graph replication also allows more flexibility.\n",
        "\n",
        "One way you can prevent your model to fail because it was waiting on a dequeue operation is to set a timeout. Below are two ways to do so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gnQo8b7x3se",
        "colab_type": "code",
        "outputId": "2baf017a-8c75-417b-92bd-406e0a520f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "queue = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[()])\n",
        "v = tf.placeholder(tf.float32, shape=())\n",
        "enqueue = queue.enqueue([v])\n",
        "dequeue = queue.dequeue()\n",
        "output = dequeue + 1\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2223') as sess:\n",
        "  try:\n",
        "    init.run()\n",
        "    \n",
        "    enqueue.run(feed_dict={v: 0.0})\n",
        "    enqueue.run(feed_dict={v: 1.0})\n",
        "    \n",
        "    run_opts = tf.RunOptions()\n",
        "    run_opts.timeout_in_ms = 1000\n",
        "    \n",
        "    print(output.eval())\n",
        "    print(output.eval())\n",
        "    print(sess.run(output, options=run_opts))\n",
        "  except tf.errors.DeadlineExceededError:\n",
        "    print('Deadline exceeded.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "2.0\n",
            "Deadline exceeded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTMBDHYazHrH",
        "colab_type": "code",
        "outputId": "3cc132ef-4463-4671-d1ad-ed9f4eda84a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# This way lets you set a global timeout\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.operation_timeout_in_ms = 1000\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2223', config=config) as sess:\n",
        "  try:\n",
        "    init.run()\n",
        "    \n",
        "    enqueue.run(feed_dict={v: 0.0})\n",
        "    enqueue.run(feed_dict={v: 1.0})\n",
        "    \n",
        "    print(output.eval())\n",
        "    print(output.eval())\n",
        "    print(output.eval())\n",
        "  except tf.errors.DeadlineExceededError:\n",
        "    print('Deadline exceeded.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "2.0\n",
            "Deadline exceeded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKI79sOt0BgF",
        "colab_type": "text"
      },
      "source": [
        "### Model Parallelism\n",
        "\n",
        "_Model parallelism_ is the technique of training a single neural network on multiple devices by splitting different parts of the model (e.g. different layers) onto different devices. In practice for DNNs this technique does not work well, since the interconnected neurons would need to communicate frequently with other machines. Therefore the added latency from communication between devices outweights the benefit of training the model in parallel.\n",
        "\n",
        "Some neural network architectures like convolutional neural networks (Chapter 13) use partially connected neuron layers. Recurrent neural networks (Chapter 14) are made up of several layers of _memory cells_, i.e. layers whose outputs at time $t$ are fed back to the layer at time $t+1$. In this case, splitting the layers onto different devices lets you parallelize the recurring training steps. Since in RNNs each cell is fairly complex, parallelizing training gives a performance boost which outweights the communication latency.\n",
        "\n",
        "### Data Parallelism\n",
        "\n",
        "Another way to train neural networks in parallel is to run a training step for the same model across different devices in parallel, then aggregate the results to compute the error gradient for optimization. This is called _data parallelism_.\n",
        "\n",
        "#### Synchronous updates\n",
        "\n",
        "When using synchronous updates for data parallelism, the aggregator waits for each individual model to finish before computing the gradients. It then updates each model simultaneously updates each model with the new parameters.\n",
        "\n",
        "The downside is if one device is slower, it slows down all of training. Also sending updated parameters to all of the models at once can saturate your network.\n",
        "\n",
        "#### Asynchronous updates\n",
        "\n",
        "Asynchronous updates is when each replica trains their model independently from the others. Once they run through a training step, they send their updates to every model in the network. Since this happens at different times for each replica, the network does not get saturated.\n",
        "\n",
        "The model is an attractive choice due to its simplicity, but has a downside: if a model has been updated multiple times during a training step, its gradient may be out of date or _stale_. Stale gradients can slow down convergence. Here are some ways to deal with stale gradients:\n",
        "\n",
        "- Reduce the learning rate.\n",
        "\n",
        "- Drop stale gradients or scale them down.\n",
        "\n",
        "- Adjust the mini-batch size.\n",
        "\n",
        "- Start the first few epochs with one replica, then begin parallelization since stale gradients are most damaging at the beginning of training.\n",
        "\n",
        "A [paper](https://arxiv.org/pdf/1604.00981v2.pdf) published by the Google Brain team showed that of the various forms of parallelism that data parallelism with synchronous updates across just a few replicas had the best performance, though this is an ongoing area of research.\n",
        "\n",
        "#### Bandwidth saturation\n",
        "\n",
        "Due to the fact that data parallelism requires you to send new parameters to each model at the start of a training step and to send the results for gradient computation at the end, there reaches a point where adding more devices does not improve the performance due to the added latency of inter-device communication. This is more severe for dense models, but less so for sparse models. Google Brain [reported](https://www.youtube.com/watch?v=sUzQpd-Ku4o) 25-40x speedups when distirbuting a dense model across 50 GPUs and a 300x speedup for sparse models across 500 GPUs. After that point, performance starts to degrade.\n",
        "\n",
        "Some ways to reduce latency are:\n",
        "\n",
        "- Group your GPUs on a few servers rather than scattering them across many servers.\n",
        "\n",
        "- Shard the parameters across multiple servers.\n",
        "\n",
        "- Drop the model's parameters' precision from `float32` to `float16` to cut the bits sent in half without sacrificing much performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azZY4JvBHapn",
        "colab_type": "text"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "### 1. If you get a `CUDA_ERROR_OUT_OF_MEMORY` when starting your TensorFlow program, what is probably going on? What can you do about it?\n",
        "\n",
        "This error suggests that the TensorFlow program could be using all of the memory on the GPU device, and you may need to separate the program onto multiple devices.\n",
        "\n",
        "This error could also indicate that another TensorFlow program is using all of the available memory on that particular GPU. You can either run this program on a separate device, you can have the both programs only use a fraction of the available GPU memory:\n",
        "\n",
        "```python\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
        "...\n",
        "sess = tf.Session(config=config)\n",
        "...\n",
        "sess.close()\n",
        "```\n",
        "\n",
        "or you can have both programs only take memory as needed:\n",
        "\n",
        "```python\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "...\n",
        "sess = tf.Session(config=config)\n",
        "...\n",
        "sess.close()\n",
        "```\n",
        "\n",
        "If both programs require more memory than is available in the GPU, you should run the programs on separate devices.\n",
        "\n",
        "### 2. What is the difference between pinning and operation on a device and placing an operation on a device?\n",
        "\n",
        "Pinning an operation to a device is a manual action where you specify which device an operation should run at. TensorFlow places operations on devices based on where they are pinned. If the operation is not pinned to any device, TensorFlow places the operation on the machine's default device.\n",
        "\n",
        "### 3. If you are running on a GPU-enabled TensorFlow installation, and you just use the default placement, will all operations be placed on the GPU?\n",
        "\n",
        "Yes, TensorFlow will place the operation on the GPU if it is available. If there is no GPU available then it will use the CPU.\n",
        "\n",
        "### 4. If you pin a variable to `\"/gpu:0\"`, can it be used by operations placed on `\"/gpu:1\"`? Or by operations placed on `\"/cpu:0\"`? Or by operations pinned to devices located on other servers?\n",
        "\n",
        "I do not have a device available with two GPUs, so I cannot test this out with code. However, since you can run graphs with operations across devices and acrosss servers, it seems like you should be able to run operations across GPUs.\n",
        "\n",
        "Below is an example of using a graph with operations across devices:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAaMTCktNBYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96ee0ebb-463b-4b9b-e342-b4d3b8646381"
      },
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  a = tf.constant(2.0, name='a')\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  b = tf.constant(3.0, name='b')\n",
        "\n",
        "c = a + b\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  print(c.eval())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8NSOFYzPAiD",
        "colab_type": "text"
      },
      "source": [
        "Below is an example of a graph with operations defined on different devices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3C8fBouPSDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0fb1c85-f35f-4544-b6d5-9bcf0eb7dc60"
      },
      "source": [
        "with tf.device('/job:ps/task:0/cpu:0'):\n",
        "  a = tf.constant(2.0, name='a')\n",
        "  \n",
        "with tf.device('/job:ps/task:1/gpu:0'):\n",
        "  b = tf.constant(3.0, name='b')\n",
        "  \n",
        "c = a + b\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  print(c.eval())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9uFD82WZXe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_rZAxuNRONG",
        "colab_type": "text"
      },
      "source": [
        "### 5. Can two operations placed on the same device run in parallel?\n",
        "\n",
        "Yes, you can run two operations on the same device in parallel. You can run the operations on a CPU with multiple cores. CUDA or cuDNN also allows you to run operations in parallel on the GPU as well.\n",
        "\n",
        "### 6. What is a control dependency and when would you want to use it?\n",
        "\n",
        "A control dependency is when you create an artificial dependency between two nodes in the TensorFlow graph. The dependent node's evaluation may not depend on the control dependencies for evaluation, but the TensorFlow session will wait for the control dependencies to be evaluated before evaluating the dependent node.\n",
        "\n",
        "### 7. Suppose you train a DNN for days on a TensorFlow cluster, and immediately after your training program ends you realize you forgot to save the model using a `Saver`. Is your trained model lost?\n",
        "\n",
        "No, the trained model is not lost. Model parameters in distribured TensorFlow are saved in a resource container, so they can be accessed later in other sessions using the same cluster. If the server with the resource container crashes, the model will be lost if it was not saved to disk.\n",
        "\n",
        "### 8. Train several DNNs in parallel on a TensorFlow cluster using different hyperparameter values. Write a single client program that trains only one DNN, then run this program in multiple processes in parallel with different hyperparameter values. Use a validation set or cross-validation to select the top three models.\n",
        "\n",
        "I restarted the Colab kernel before this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORSexgIUZQuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading the MNIST dataset.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Bd2CVYfkXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the parameter space to choose from.\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "  return tf.maximum(alpha * z, z)\n",
        "\n",
        "n_neurons_values = [100, 120, 140, 160]\n",
        "activation_functions = [tf.nn.relu, tf.nn.elu, leaky_relu]\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "momentums = [0.9, 0.95, 0.99]\n",
        "batch_sizes = [50, 80, 100, 130, 150, 200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s6hFyoUjP7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a cluster spec for training the models\n",
        "\n",
        "cluster_spec = tf.train.ClusterSpec({\n",
        "    'ps': ['127.0.0.1:{}'.format(1000 + j) for j in range(10)],\n",
        "    'worker': ['127.0.0.1:{}'.format(1010 + j) for j in range(10)],\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zECJEdb9kRgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function which builds a graph and trains it. It then prints\n",
        "# the validation set accuracy and parameters selected to get those results.\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_outputs = 10\n",
        "\n",
        "def select_parameter(arr):\n",
        "  return arr[np.random.randint(0, len(arr))]\n",
        "\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "  rnd_idx = np.random.permutation(len(X))\n",
        "  n_batches = len(X) // batch_size\n",
        "  for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "    X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "    yield X_batch, y_batch\n",
        "\n",
        "def train_dnn(random_state, device):\n",
        "  np.random.seed(random_state)\n",
        "  n_neurons = select_parameter(n_neurons_values)\n",
        "  activation = select_parameter(activation_functions)\n",
        "  learning_rate = select_parameter(learning_rates)\n",
        "  momentum = select_parameter(momentums)\n",
        "  \n",
        "  with tf.device(device + '/gpu:0'):\n",
        "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "    y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "    \n",
        "    hidden1 = tf.layers.dense(X, n_neurons, name='hidden1',\n",
        "                              activation=activation)\n",
        "    hidden2 = tf.layers.dense(hidden1, n_neurons, name='hidden2',\n",
        "                              activation=activation)\n",
        "    hidden3 = tf.layers.dense(hidden2, n_neurons, name='hidden3',\n",
        "                              activation=activation)\n",
        "    hidden4 = tf.layers.dense(hidden3, n_neurons, name='hidden4',\n",
        "                              activation=activation)\n",
        "    hidden5 = tf.layers.dense(hidden4, n_neurons, name='hidden5',\n",
        "                              activation=activation)\n",
        "    \n",
        "    logits = tf.layers.dense(hidden5, n_outputs, name='logits')\n",
        "    y_proba = tf.nn.softmax(logits, name='y_proba')\n",
        "    \n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                              logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name='loss')\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
        "                                           momentum=momentum)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "    \n",
        "    correct = tf.nn.in_top_k(logits, y)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}