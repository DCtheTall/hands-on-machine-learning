{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistributingTensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJTQLhXE1TC2",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 12: Distributing TensorFlow Across Devices and Servers\n",
        "\n",
        "Since training a large DNN for a complex task on a single CPU can take days or even weeks, this chapter discusses distributing TensorFlow across multiple devices on the same machine then multiple devices across multiple machines. TensorFlow has built in support for distributed computing, making it an ideal machine learning framework for this task.\n",
        "\n",
        "## Multiple Devices on a Single Machine\n",
        "\n",
        "You can speed up training a neural network by adding multiple GPUs to your machine. In some cases, it is faster to train a neural network with 8 GPUs on a single machine than 16 GPUs on multiple machines, since network communications can slow down training.\n",
        "\n",
        "### Installation\n",
        "\n",
        "Below is code for installing Nvidia's _Compute Unified Device Architecture_ library (CUDA) in Google Colab. TensorFlow uses CUDA for using the GPU for training DNNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5rTfVZr5bRO",
        "colab_type": "code",
        "outputId": "9e36ee86-ea34-4a0f-c2e7-e6346bb6f456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-22 01:58:58--  https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 192.229.189.146\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|192.229.189.146|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb?RVHf4zI8qOLFTzZi5hgsQ6WEhCOKKz8X0qzCNAjGsaMWOqdaCrN-U5Y-ayZf6XIqS_JgSx-5TlQxVstV-A_cIfHGkm_NjkD1CD3LdQU5fK3rO-80vHEqP-NSS_Bem2PDvS25yT42v7k6v91g1hJu83L3L13WePWGC4SsRfiyqVkr2_6bqrFyvxRp3B7cCBL3uTMbm191IxoBp0yjhzY [following]\n",
            "--2019-05-22 01:58:58--  https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb?RVHf4zI8qOLFTzZi5hgsQ6WEhCOKKz8X0qzCNAjGsaMWOqdaCrN-U5Y-ayZf6XIqS_JgSx-5TlQxVstV-A_cIfHGkm_NjkD1CD3LdQU5fK3rO-80vHEqP-NSS_Bem2PDvS25yT42v7k6v91g1hJu83L3L13WePWGC4SsRfiyqVkr2_6bqrFyvxRp3B7cCBL3uTMbm191IxoBp0yjhzY\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.232.112, 2606:2800:247:2063:46e:21d:825:102e\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.232.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1267391958 (1.2G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.18G   132MB/s    in 7.5s    \n",
            "\n",
            "2019-05-22 01:59:06 (161 MB/s) - ‘cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb’ saved [1267391958/1267391958]\n",
            "\n",
            "(Reading database ... 143144 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-2-local (9.2.88-1) over (9.2.88-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-9-2-local (9.2.88-1) ...\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-9-2-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-9-2-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-9-2-local  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-9-2-local  Release [574 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:16 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Fetched 163 kB in 2s (80.5 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cuda is already the newest version (10.1.168-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxgbjTYNkySz",
        "colab_type": "code",
        "outputId": "37647cf0-a0fa-4c56-ca9d-45a895dba6df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Wed_Apr_24_19:10:27_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chA96tjx3_QO",
        "colab_type": "text"
      },
      "source": [
        "The following code installs the GPU-enabled version of TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Qm5kLikIXt",
        "colab_type": "code",
        "outputId": "1ec4d6a6-bcc5-4d34-f753-b15d248ffaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip3 install --upgrade tensorflow-gpu"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.3)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.9)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.7)\n",
            "Requirement already satisfied, skipping upgrade: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (3.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (0.15.4)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoN_7gmd2L7J",
        "colab_type": "text"
      },
      "source": [
        "### Managin the GPU RAM\n",
        "\n",
        "By default, TensorFlow grabs all the available RAM on GPUs the first time you run a graph. One option is to run each process on different GPU cards. Below is code for doing so:\n",
        "\n",
        "```bash\n",
        "CUDA_VISIBLE_DEVICES=0,1 python3 program1.py\n",
        "CUDA_VISIBLE_DEVICES=2,3 python3 program2.py\n",
        "```\n",
        "\n",
        "Another option is to tell TensorFlow to only use a fraction of the available memory. Code for doing so is below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZT-CIBNlHUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example code telling TensorFlow to grab only 40% of each GPU's memory\n",
        "# so that multiple TensorFlow programs can run.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
        "session = tf.Session(config=config)\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu-zyzx-lVOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatively you can have TensorFlow only grab memory when it needs to.\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.Session(config=config)\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_QDSkly4evG",
        "colab_type": "text"
      },
      "source": [
        "### Placing Operations on Devices\n",
        "\n",
        "The [TensorFlow whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf) presents a _dynamic placer_ algorithm that automatically distributes operations across all devices. This algorithm is internal to Google and is not released in the open source version of TensorFlow. This is due to the fact that in practice, a small set of placement rules specified by the user can perform just as well or better than dynamic placement.\n",
        "\n",
        "Until the dynamic placer is made public, the open source version of TensorFlow relies on the _simple placer_.\n",
        "\n",
        "#### Simple Placer\n",
        "\n",
        "Whenever you run a graph, if a node has not yet been placed, the simple placer will allocate the operation to a device using the following rules:\n",
        "\n",
        "- If a node has already been placed in a previous run of the graph, it is left on that device.\n",
        "\n",
        "- If the user _pinned_ a node to a device (described below) then the placer places it on that device.\n",
        "\n",
        "- Otherwise, it defaults to GPU #0 or the the CPU if there's no GPU.\n",
        "\n",
        "Below is an example of using TensorFlow to _pin_ a node to a device, in this case the code pins the variable `a` and the constant `b` on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsV06UHt84vU",
        "colab_type": "code",
        "outputId": "f5110797-8317-499d-90c8-7183cb4a4e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  a = tf.Variable(3.0, name='a')\n",
        "  b = tf.constant(4.0, name='b')\n",
        "c = a * b"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HokV65x-7fZ",
        "colab_type": "text"
      },
      "source": [
        "#### Logging Placements\n",
        "\n",
        "Below is code for logging which device each node is pinned to. The code in the book does not work due to [this TenorFlow issue](https://github.com/tensorflow/tensorflow/issues/3047). Below is an example workaround from "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OGu5gNQEx5E",
        "colab_type": "code",
        "outputId": "b8a69fc6-ee6f-4e57-f26e-e56d47891d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install wurlitzer"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.6/dist-packages (1.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ1yHQmt_BGV",
        "colab_type": "code",
        "outputId": "fa3228dc-257b-4218-e47c-26eb9eda9e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from wurlitzer import pipes\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "with pipes() as (out, err):\n",
        "  print(sess.run(a.initializer))\n",
        "\n",
        "print (out.read())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "a: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "a/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "a/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "a/initial_value: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "b: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwNqFOWxGNKF",
        "colab_type": "code",
        "outputId": "30e6cbe4-c6ff-40d0-ed99-895eaf438511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sess.run(c)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTltMR8HGOeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjoRTtxNGax2",
        "colab_type": "text"
      },
      "source": [
        "#### Dynamic Placement Function\n",
        "\n",
        "When you create a device block, you can also define a function which pins the nodes to devices. You can use this to implement more complex pinning algorithms such as pinning across GPUs in a round-robin fashion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUz-LK8IItNi",
        "colab_type": "code",
        "outputId": "b8bf2df5-9c81-4275-a53e-9898dc629de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def variables_on_cpu(op):\n",
        "  if op.type == 'Variable':\n",
        "    return '/cpu:0'\n",
        "  return '/gpu:0'\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(variables_on_cpu):\n",
        "  a = tf.Variable(3.0)\n",
        "  b = tf.constant(4.0)\n",
        "  c = a * b\n",
        "  \n",
        "sess = tf.Session()\n",
        "sess.run(a.initializer)\n",
        "sess.run(c)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_bSrICeHtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaCbqek6KApK",
        "colab_type": "text"
      },
      "source": [
        "#### Operations and Kernels\n",
        "\n",
        "For a TensorFlow variable to run on a device, it needs to have an implementation, or a _kernel_, for that device. Many operations have kernels for GPUs and CPUs. Integer variables, however, do not have a kernel for the GPU. The following code illustrates this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4o3GJias9e",
        "colab_type": "code",
        "outputId": "be2eb431-3227-453a-9008-8add6e31fe6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  i = tf.Variable(3)\n",
        "\n",
        "try:\n",
        "  sess = tf.Session()\n",
        "  sess.run(i.initializer)\n",
        "except Exception as ex:\n",
        "  print(type(ex).__name__)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "InvalidArgumentError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyN02Gi-gmtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgmzMXywgRVC",
        "colab_type": "text"
      },
      "source": [
        "#### Soft placement\n",
        "\n",
        "In order to prevent the exception being raised above, you can have TensorFlow fall back on the CPU instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZntUgPagP8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  i = tf.Variable(3)\n",
        "  \n",
        "config = tf.ConfigProto()\n",
        "config.allow_soft_placement = True\n",
        "sess = tf.Session(config=config)\n",
        "sess.run(i.initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLGQJq6Mgwzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NChj2h2eg3UO",
        "colab_type": "text"
      },
      "source": [
        "### Parallel Execution\n",
        "\n",
        "When TensorFlow evaluates a graph, it first evaluates all of the nodes with no dependencies, i.e. the source nodes. Once it evaluates a node which another depends on, the latter node's dependency counter decreases. Once it reaches zero, that node is evaluated. Once all of the nodes TensorFlow needs to evaluate are done, it outputs the result.\n",
        "\n",
        "For nodes evaluated on the CPU, the evaluations are dispatched into a queue in a thread pool called the _inter-op thread pool_. If the CPU has multiple cores, then the operations are executed in parallel. If the operations themselves have multithreaded kernels, then these kernels split their task into sub-operations which are placed in a queue in another thread pool called the _intra-op thread pool_.\n",
        "\n",
        "On the GPU, operations in the queue are evaluated sequentially. Operations which have multithreaded kernels are executed in parallel implemented by CUDA, cuDNN, and other GPU libraries that TensorFlow depends on.\n",
        "\n",
        "### Control Dependencies\n",
        "\n",
        "Sometimes, we do not want to evaluate nodes right when their dependency counter reaches zero. These nodes may take up a lot of compute resources to evaluate, and we may not need their values later. Or alternatively, some nodes rely on a lot of data not localized in the machine, so it may more make sense to evaluate them sequentially instead of in parallel.\n",
        "\n",
        "Below is an example of adding _control dependencies_ in a TensorFlow graph, i.e. nodes which need to wait on the evaluation of other nodes even if they do not directly depend on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr8SaXpkRPvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "a = tf.constant(1.0)\n",
        "b = a + 2.0\n",
        "\n",
        "with tf.control_dependencies([a, b]):\n",
        "  x = tf.constant(3.0)\n",
        "  y = tf.constant(4.0)\n",
        "  \n",
        "z = x + y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj87HhPARfrU",
        "colab_type": "text"
      },
      "source": [
        "Here, the evaluation of `z` depends on the evaluation of `a` and `b` even though `z`'s value does not depend on `a` or `b`. Since `b` depends on `a`, you need only list `b` as a control dependency, but sometimes it is better to be explicit.\n",
        "\n",
        "## Distributing Devices Across Multiple Servers\n",
        "\n",
        "In order to run a graph across multiple devices, you need to define a _cluster_ i.e. a group of TensorFlow servers called _tasks_ spread across several machines. Each task belongs to a _job_ i.e. a group of tasks which perform a common role.\n",
        "\n",
        "The following code defines a _cluster specification_ which defines two jobs: `ps` and `worker`, the former is a _parameter server_ which records the model parameters whereas workers perform computations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i22nF68jDbBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_spec = tf.train.ClusterSpec({\n",
        "    'ps': [\n",
        "        '127.0.0.1:2221',\n",
        "        '127.0.0.1:2222',\n",
        "    ],\n",
        "    'worker': [\n",
        "        '127.0.0.1:2223',\n",
        "        '127.0.0.1:2224',\n",
        "        '127.0.0.1:2225',\n",
        "    ],\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luu_auK7DsAH",
        "colab_type": "text"
      },
      "source": [
        "The following code instantiates a TensorFlow `Server` object by passing it a cluster spec and then parameters to indicate its job and task number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OizKOpX0DrHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps0 = tf.train.Server(cluster_spec, job_name='ps', task_index=0)\n",
        "ps1 = tf.train.Server(cluster_spec, job_name='ps', task_index=1)\n",
        "worker0 = tf.train.Server(cluster_spec, job_name='worker', task_index=0)\n",
        "worker1 = tf.train.Server(cluster_spec, job_name='worker', task_index=1)\n",
        "worker2 = tf.train.Server(cluster_spec, job_name='worker', task_index=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "736lvOMgFDHk",
        "colab_type": "text"
      },
      "source": [
        "Typically you run one task per machine, but you can run multiple tasks per machine as long as you ensure that they don't all try to use all of the RAM on each GPU.\n",
        "\n",
        "If you want the process to do nothing other than run the TensorFlow server, you can block the main thread by using the `join()` method:\n",
        "\n",
        "```python\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfBwQVrZRNyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This pauses the main thread until the server completes.\n",
        "server.join()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpfAQFxuGCs4",
        "colab_type": "text"
      },
      "source": [
        "### Opening a Session\n",
        "\n",
        "Once all of the tasks are up and running, you can open a session on any of the servers from a client on any machine using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2j7Lf9NGRDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.constant(1.0)\n",
        "b = a + 2\n",
        "c = b * 3\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2223') as sess:\n",
        "  print(c.eval())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw5v9CCIGmG8",
        "colab_type": "text"
      },
      "source": [
        "The code creates a simple graph then opens a session on machine B and evaluates `c`. The master places operations on the appropriate device, if we do not pin the operation to a particular device then the master will place it on the machine's default device.\n",
        "\n",
        "### The Master and Worker Services\n",
        "\n",
        "The client uses _gRPC_ to communicate with the server. A protocol which uses HTTP2 to open a lasting connection for bidirectional communication. It exchanges data using _protocol buffers_.\n",
        "\n",
        "Every TensorFlow server provides two services: a _master service_ and a _worker service_. The master allows clients to open sessions and run graphs whereas the worker service actually performs computations. This architecture allows a server to open multiple sessions from one or more clients.\n",
        "\n",
        "### Pinning Operations Across Tasks\n",
        "\n",
        "Below is an example of using a device block to pin an operation to a particular task and to a particular device on that task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g-DLMR0NUr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b4bd33d-0b47-4913-e087-8c57cee0a0d6"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# This block pins `a` to task 0 of the `ps` job's CPU.\n",
        "with tf.device('/job:ps/task:0/cpu:0'):\n",
        "  a = tf.constant(1.0)\n",
        "  \n",
        "# This block pins `b` to task 1 of the `worker` job's GPU.\n",
        "with tf.device('/job:worker/task:1/gpu:0'):\n",
        "  b = a + 2\n",
        "\n",
        "c = a + b\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2225') as sess:\n",
        "  print(c.eval())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3yEESRRRKE",
        "colab_type": "text"
      },
      "source": [
        "### Sharding Variables Across Multiple Parameter Servers\n",
        "\n",
        "It is common to have a _parameter server_ job to store parameters while training a complex model. Some models, like DNNs, can have thousands or even millions of parameters. To avoid network saturation, it is common to distribute storing parameters across multiple servers.\n",
        "\n",
        "Since manually pinning every variable to a different task can be tedious, TensorFlow provides a `replica_device_setter()` which distributes variables across servers. Below is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6e6VnSGRzwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(tf.train.replica_device_setter(ps_tasks=2)):\n",
        "  v1 = tf.Variable(1.0) # pinned to /job:ps/task:0\n",
        "  v2 = tf.Variable(2.0) # pinned to /job:ps/task:1\n",
        "  v3 = tf.Variable(3.0) # pinned to /job:ps/task:0\n",
        "  v4 = tf.Variable(4.0) # pinned to /job:ps/task:1\n",
        "  v5 = tf.Variable(5.0) # pinned to /job:ps/task:0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OxduM4eSIM2",
        "colab_type": "text"
      },
      "source": [
        "Alternatively you can pass the cluster spec and TensorFlow will automatically compute the number of tasks in the `ps` job.\n",
        "\n",
        "If you create operations that are not just variables, then by default they are pinned to `/job:worker` which will default to the first device of the first worker task. You can pin them to devices using device blocks. Below is an example of a graph pinned to multiple tasks and multiple devices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2SyOjSbXiVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "499f650d-5d99-4edc-8b6b-34197e360fdc"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(tf.train.replica_device_setter(ps_tasks=2, ps_device='/job:ps',\n",
        "                                              worker_device='/job:worker')):\n",
        "  v1 = tf.Variable(1.0) # pinned to /job:ps/task:0\n",
        "  v2 = tf.Variable(2.0) # pinned to /job:ps/task:1\n",
        "  v3 = tf.Variable(3.0) # pinned to /job:ps/task:0\n",
        "  \n",
        "  s = v1 + v2 # pinned to /job:worker/task:0/cpu:0\n",
        "  \n",
        "  with tf.device('/gpu:0'):\n",
        "    p1 = 2 * s # pinned to /job:worker/task:0/gpu:0\n",
        "    \n",
        "    with tf.device('/task:1'):\n",
        "      p2 = 3 * s # pinned to /job:worker/task:1/cpu:0\n",
        "      \n",
        "with tf.Session('grpc://127.0.0.1:2221') as sess:\n",
        "  v1.initializer.run()\n",
        "  v2.initializer.run()\n",
        "  print(s.eval())\n",
        "  print(p1.eval())\n",
        "  print(p2.eval())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n",
            "6.0\n",
            "9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yeuDp4DbJHZ",
        "colab_type": "text"
      },
      "source": [
        "### Sharing State Across Sessions Using Resource Containers\n",
        "\n",
        "When using a plain _local session_, variables values are stored in the session object, so when the session ends the values are deleted. Moreover multiple local sessions cannot share any state, even if they run the same graph.\n",
        "\n",
        "When you are using _distributed sessions_, variable state is managed by _resource containers_ located on the cluster and persist across sessions. An example of this is given by the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utrMhotjkHdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.Variable(0.0, name='x')\n",
        "increment_x = tf.assign(x, x + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDO7AO4Ykf2X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff8728d5-7d2b-4c74-ca03-354e2c7f4256"
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(x.initializer)\n",
        "  sess.run(increment_x)\n",
        "  print(x.eval())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C2hcJwmk5cE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba673305-6425-46ec-cf07-ae23d2997e8e"
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(increment_x)\n",
        "  print(x.eval())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4la_pAale00",
        "colab_type": "text"
      },
      "source": [
        "While this feature can be convenient, you have to be careful to not use the same variable names by accident. One way to avoid this is by using a variable scope with a unique name for each computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS1AIxlRlu22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.variable_scope('problem_1'):\n",
        "  x1 = tf.Variable(0.0, name='x')\n",
        "  increment_x1 = tf.assign(x1, x1 + 1)\n",
        "  \n",
        "with tf.variable_scope('problem_2'):\n",
        "  x2 = tf.Variable(0.0, name='x')\n",
        "  increment_x2 = tf.assign(x2, x2 + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqYDwyvQm9Ma",
        "colab_type": "text"
      },
      "source": [
        "You can even use resource containers to store variables across different graphs."
      ]
    }
  ]
}