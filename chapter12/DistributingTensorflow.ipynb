{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistributingTensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJTQLhXE1TC2",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 12: Distributing TensorFlow Across Devices and Servers\n",
        "\n",
        "Since training a large DNN for a complex task on a single CPU can take days or even weeks, this chapter discusses distributing TensorFlow across multiple devices on the same machine then multiple devices across multiple machines. TensorFlow has built in support for distributed computing, making it an ideal machine learning framework for this task.\n",
        "\n",
        "## Multiple Devices on a Single Machine\n",
        "\n",
        "You can speed up training a neural network by adding multiple GPUs to your machine. In some cases, it is faster to train a neural network with 8 GPUs on a single machine than 16 GPUs on multiple machines, since network communications can slow down training.\n",
        "\n",
        "### Installation\n",
        "\n",
        "Below is code for installing Nvidia's _Compute Unified Device Architecture_ library (CUDA) in Google Colab. TensorFlow uses CUDA for using the GPU for training DNNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5rTfVZr5bRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxgbjTYNkySz",
        "colab_type": "code",
        "outputId": "39e8b2c5-0c61-4e94-f69f-0479c2281be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Wed_Apr_24_19:10:27_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chA96tjx3_QO",
        "colab_type": "text"
      },
      "source": [
        "The following code installs the GPU-enabled version of TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Qm5kLikIXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install --upgrade tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoN_7gmd2L7J",
        "colab_type": "text"
      },
      "source": [
        "### Managin the GPU RAM\n",
        "\n",
        "By default, TensorFlow grabs all the available RAM on GPUs the first time you run a graph. One option is to run each process on different GPU cards. Below is code for doing so:\n",
        "\n",
        "```bash\n",
        "CUDA_VISIBLE_DEVICES=0,1 python3 program1.py\n",
        "CUDA_VISIBLE_DEVICES=2,3 python3 program2.py\n",
        "```\n",
        "\n",
        "Another option is to tell TensorFlow to only use a fraction of the available memory. Code for doing so is below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZT-CIBNlHUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example code telling TensorFlow to grab only 40% of each GPU's memory\n",
        "# so that multiple TensorFlow programs can run.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
        "session = tf.Session(config=config)\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu-zyzx-lVOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatively you can have TensorFlow only grab memory when it needs to.\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.Session(config=config)\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_QDSkly4evG",
        "colab_type": "text"
      },
      "source": [
        "### Placing Operations on Devices\n",
        "\n",
        "The [TensorFlow whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf) presents a _dynamic placer_ algorithm that automatically distributes operations across all devices. This algorithm is internal to Google and is not released in the open source version of TensorFlow. This is due to the fact that in practice, a small set of placement rules specified by the user can perform just as well or better than dynamic placement.\n",
        "\n",
        "Until the dynamic placer is made public, the open source version of TensorFlow relies on the _simple placer_.\n",
        "\n",
        "#### Simple Placer\n",
        "\n",
        "Whenever you run a graph, if a node has not yet been placed, the simple placer will allocate the operation to a device using the following rules:\n",
        "\n",
        "- If a node has already been placed in a previous run of the graph, it is left on that device.\n",
        "\n",
        "- If the user _pinned_ a node to a device (described below) then the placer places it on that device.\n",
        "\n",
        "- Otherwise, it defaults to GPU #0 or the the CPU if there's no GPU.\n",
        "\n",
        "Below is an example of using TensorFlow to _pin_ a node to a device, in this case the code pins the variable `a` and the constant `b` on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsV06UHt84vU",
        "colab_type": "code",
        "outputId": "017ede31-6e34-4a44-cd5d-4b4ce49c88d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  a = tf.Variable(3.0, name='a')\n",
        "  b = tf.constant(4.0, name='b')\n",
        "c = a * b"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HokV65x-7fZ",
        "colab_type": "text"
      },
      "source": [
        "#### Logging Placements\n",
        "\n",
        "Below is code for logging which device each node is pinned to. The code in the book does not work due to [this TenorFlow issue](https://github.com/tensorflow/tensorflow/issues/3047). Below is an example workaround from "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OGu5gNQEx5E",
        "colab_type": "code",
        "outputId": "55161c90-64cb-4c91-c598-7ea8dab9858a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install wurlitzer"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wurlitzer\n",
            "  Downloading https://files.pythonhosted.org/packages/60/17/de2820542c755f4630a58d295daad86bfa981fbf48b48e5f9e1f2ed806cc/wurlitzer-1.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: wurlitzer\n",
            "Successfully installed wurlitzer-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ1yHQmt_BGV",
        "colab_type": "code",
        "outputId": "a798d952-e7e0-40bb-b1d3-32e60004d923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from wurlitzer import pipes\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "with pipes() as (out, err):\n",
        "  print(sess.run(a.initializer))\n",
        "\n",
        "print (out.read())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "a: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "a/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "a/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "a/initial_value: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "b: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwNqFOWxGNKF",
        "colab_type": "code",
        "outputId": "46bbc380-5606-4cd9-b0e3-b50aa6a312ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sess.run(c)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTltMR8HGOeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjoRTtxNGax2",
        "colab_type": "text"
      },
      "source": [
        "#### Dynamic Placement Function\n",
        "\n",
        "When you create a device block, you can also define a function which pins the nodes to devices. You can use this to implement more complex pinning algorithms such as pinning across GPUs in a round-robin fashion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUz-LK8IItNi",
        "colab_type": "code",
        "outputId": "b8bf2df5-9c81-4275-a53e-9898dc629de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def variables_on_cpu(op):\n",
        "  if op.type == 'Variable':\n",
        "    return '/cpu:0'\n",
        "  return '/gpu:0'\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(variables_on_cpu):\n",
        "  a = tf.Variable(3.0)\n",
        "  b = tf.constant(4.0)\n",
        "  c = a * b\n",
        "  \n",
        "sess = tf.Session()\n",
        "sess.run(a.initializer)\n",
        "sess.run(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_bSrICeHtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaCbqek6KApK",
        "colab_type": "text"
      },
      "source": [
        "#### Operations and Kernels\n",
        "\n",
        "For a TensorFlow variable to run on a device, it needs to have an implementation, or a _kernel_, for that device. Many operations have kernels for GPUs and CPUs. Integer variables, however, do not have a kernel for the GPU. The following code illustrates this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4o3GJias9e",
        "colab_type": "code",
        "outputId": "be2eb431-3227-453a-9008-8add6e31fe6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  i = tf.Variable(3)\n",
        "\n",
        "try:\n",
        "  sess = tf.Session()\n",
        "  sess.run(i.initializer)\n",
        "except Exception as ex:\n",
        "  print(type(ex).__name__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "InvalidArgumentError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyN02Gi-gmtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgmzMXywgRVC",
        "colab_type": "text"
      },
      "source": [
        "#### Soft placement\n",
        "\n",
        "In order to prevent the exception being raised above, you can have TensorFlow fall back on the CPU instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZntUgPagP8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  i = tf.Variable(3)\n",
        "  \n",
        "config = tf.ConfigProto()\n",
        "config.allow_soft_placement = True\n",
        "sess = tf.Session(config=config)\n",
        "sess.run(i.initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLGQJq6Mgwzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NChj2h2eg3UO",
        "colab_type": "text"
      },
      "source": [
        "### Parallel Execution\n",
        "\n",
        "When TensorFlow evaluates a graph, it first evaluates all of the nodes with no dependencies, i.e. the source nodes. Once it evaluates a node which another depends on, the latter node's dependency counter decreases. Once it reaches zero, that node is evaluated. Once all of the nodes TensorFlow needs to evaluate are done, it outputs the result.\n",
        "\n",
        "For nodes evaluated on the CPU, the evaluations are dispatched into a queue in a thread pool called the _inter-op thread pool_. If the CPU has multiple cores, then the operations are executed in parallel. If the operations themselves have multithreaded kernels, then these kernels split their task into sub-operations which are placed in a queue in another thread pool called the _intra-op thread pool_.\n",
        "\n",
        "On the GPU, operations in the queue are evaluated sequentially. Operations which have multithreaded kernels are executed in parallel implemented by CUDA, cuDNN, and other GPU libraries that TensorFlow depends on.\n",
        "\n",
        "### Control Dependencies\n",
        "\n",
        "Sometimes, we do not want to evaluate nodes right when their dependency counter reaches zero. These nodes may take up a lot of compute resources to evaluate, and we may not need their values later. Or alternatively, some nodes rely on a lot of data not localized in the machine, so it may more make sense to evaluate them sequentially instead of in parallel.\n",
        "\n",
        "Below is an example of adding _control dependencies_ in a TensorFlow graph, i.e. nodes which need to wait on the evaluation of other nodes even if they do not directly depend on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr8SaXpkRPvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "a = tf.constant(1.0)\n",
        "b = a + 2.0\n",
        "\n",
        "with tf.control_dependencies([a, b]):\n",
        "  x = tf.constant(3.0)\n",
        "  y = tf.constant(4.0)\n",
        "  \n",
        "z = x + y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj87HhPARfrU",
        "colab_type": "text"
      },
      "source": [
        "Here, the evaluation of `z` depends on the evaluation of `a` and `b` even though `z`'s value does not depend on `a` or `b`. Since `b` depends on `a`, you need only list `b` as a control dependency, but sometimes it is better to be explicit.\n",
        "\n",
        "## Distributing Devices Across Multiple Servers\n",
        "\n",
        "In order to run a graph across multiple devices, you need to define a _cluster_ i.e. a group of TensorFlow servers called _tasks_ spread across several machines. Each task belongs to a _job_ i.e. a group of tasks which perform a common role.\n",
        "\n",
        "The following code defines a _cluster specification_ which defines two jobs: `ps` and `worker`, the former is a _parameter server_ which records the model parameters whereas workers perform computations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i22nF68jDbBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_spec = tf.train.ClusterSpec({\n",
        "    'ps': [\n",
        "        '127.0.0.1:2221',\n",
        "        '127.0.0.1:2222',\n",
        "    ],\n",
        "    'worker': [\n",
        "        '127.0.0.1:2223',\n",
        "        '127.0.0.1:2224',\n",
        "        '127.0.0.1:2225',\n",
        "    ],\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luu_auK7DsAH",
        "colab_type": "text"
      },
      "source": [
        "The following code instantiates a TensorFlow `Server` object by passing it a cluster spec and then parameters to indicate its job and task number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OizKOpX0DrHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps0 = tf.train.Server(cluster_spec, job_name='ps', task_index=0)\n",
        "ps1 = tf.train.Server(cluster_spec, job_name='ps', task_index=1)\n",
        "worker0 = tf.train.Server(cluster_spec, job_name='worker', task_index=0)\n",
        "worker1 = tf.train.Server(cluster_spec, job_name='worker', task_index=1)\n",
        "worker2 = tf.train.Server(cluster_spec, job_name='worker', task_index=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "736lvOMgFDHk",
        "colab_type": "text"
      },
      "source": [
        "Typically you run one task per machine, but you can run multiple tasks per machine as long as you ensure that they don't all try to use all of the RAM on each GPU.\n",
        "\n",
        "If you want the process to do nothing other than run the TensorFlow server, you can block the main thread by using the `join()` method:\n",
        "\n",
        "```python\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkLK3DVt-T65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will block the main thread until the server finishes.\n",
        "\n",
        "server.join()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpfAQFxuGCs4",
        "colab_type": "text"
      },
      "source": [
        "### Opening a Session\n",
        "\n",
        "Once all of the tasks are up and running, you can open a session on any of the servers from a client on any machine using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2j7Lf9NGRDS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1895c35-d49b-49f2-b0db-10822b3175ca"
      },
      "source": [
        "a = tf.constant(1.0)\n",
        "b = a + 2\n",
        "c = b * 3\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2223') as sess:\n",
        "  print(c.eval())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw5v9CCIGmG8",
        "colab_type": "text"
      },
      "source": [
        "The code creates a simple graph then opens a session on machine B and evaluates `c`. The master places operations on the appropriate device, if we do not pin the operation to a particular device then the master will place it on the machine's default device.\n",
        "\n",
        "### The Master and Worker Services\n",
        "\n",
        "The client uses _gRPC_ to communicate with the server. A protocol which uses HTTP2 to open a lasting connection for bidirectional communication. It exchanges data using _protocol buffers_.\n",
        "\n",
        "Every TensorFlow server provides two services: a _master service_ and a _worker service_. The master allows clients to open sessions and run graphs whereas the worker service actually performs computations. This architecture allows a server to open multiple sessions from one or more clients.\n",
        "\n",
        "### Pinning Operations Across Tasks\n",
        "\n",
        "Below is an example of using a device block to pin an operation to a particular task and to a particular device on that task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g-DLMR0NUr5",
        "colab_type": "code",
        "outputId": "ee22d584-bab9-48e2-f817-a88ebddac90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# This block pins `a` to task 0 of the `ps` job's CPU.\n",
        "with tf.device('/job:ps/task:0/cpu:0'):\n",
        "  a = tf.constant(1.0)\n",
        "  \n",
        "# This block pins `b` to task 1 of the `worker` job's GPU.\n",
        "with tf.device('/job:worker/task:1/gpu:0'):\n",
        "  b = a + 2\n",
        "\n",
        "c = a + b\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2225') as sess:\n",
        "  print(c.eval())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3yEESRRRKE",
        "colab_type": "text"
      },
      "source": [
        "### Sharding Variables Across Multiple Parameter Servers\n",
        "\n",
        "It is common to have a _parameter server_ job to store parameters while training a complex model. Some models, like DNNs, can have thousands or even millions of parameters. To avoid network saturation, it is common to distribute storing parameters across multiple servers.\n",
        "\n",
        "Since manually pinning every variable to a different task can be tedious, TensorFlow provides a `replica_device_setter()` which distributes variables across servers. Below is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6e6VnSGRzwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(tf.train.replica_device_setter(ps_tasks=2)):\n",
        "  v1 = tf.Variable(1.0) # pinned to /job:ps/task:0\n",
        "  v2 = tf.Variable(2.0) # pinned to /job:ps/task:1\n",
        "  v3 = tf.Variable(3.0) # pinned to /job:ps/task:0\n",
        "  v4 = tf.Variable(4.0) # pinned to /job:ps/task:1\n",
        "  v5 = tf.Variable(5.0) # pinned to /job:ps/task:0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OxduM4eSIM2",
        "colab_type": "text"
      },
      "source": [
        "Alternatively you can pass the cluster spec and TensorFlow will automatically compute the number of tasks in the `ps` job.\n",
        "\n",
        "If you create operations that are not just variables, then by default they are pinned to `/job:worker` which will default to the first device of the first worker task. You can pin them to devices using device blocks. Below is an example of a graph pinned to multiple tasks and multiple devices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2SyOjSbXiVb",
        "colab_type": "code",
        "outputId": "402843c0-ea10-4400-a37a-8d7f2118391a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.device(tf.train.replica_device_setter(ps_tasks=2, ps_device='/job:ps',\n",
        "                                              worker_device='/job:worker')):\n",
        "  v1 = tf.Variable(1.0) # pinned to /job:ps/task:0\n",
        "  v2 = tf.Variable(2.0) # pinned to /job:ps/task:1\n",
        "  v3 = tf.Variable(3.0) # pinned to /job:ps/task:0\n",
        "  \n",
        "  s = v1 + v2 # pinned to /job:worker/task:0/cpu:0\n",
        "  \n",
        "  with tf.device('/gpu:0'):\n",
        "    p1 = 2 * s # pinned to /job:worker/task:0/gpu:0\n",
        "    \n",
        "    with tf.device('/task:1'):\n",
        "      p2 = 3 * s # pinned to /job:worker/task:1/cpu:0\n",
        "      \n",
        "with tf.Session('grpc://127.0.0.1:2221') as sess:\n",
        "  v1.initializer.run()\n",
        "  v2.initializer.run()\n",
        "  print(s.eval())\n",
        "  print(p1.eval())\n",
        "  print(p2.eval())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n",
            "6.0\n",
            "9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yeuDp4DbJHZ",
        "colab_type": "text"
      },
      "source": [
        "### Sharing State Across Sessions Using Resource Containers\n",
        "\n",
        "When using a plain _local session_, variables values are stored in the session object, so when the session ends the values are deleted. Moreover multiple local sessions cannot share any state, even if they run the same graph.\n",
        "\n",
        "When you are using _distributed sessions_, variable state is managed by _resource containers_ located on the cluster and persist across sessions. An example of this is given by the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utrMhotjkHdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.Variable(0.0, name='x')\n",
        "increment_x = tf.assign(x, x + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDO7AO4Ykf2X",
        "colab_type": "code",
        "outputId": "6d076ac8-3ef8-4bef-af7a-6a04f221f26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(x.initializer)\n",
        "  sess.run(increment_x)\n",
        "  print(x.eval())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C2hcJwmk5cE",
        "colab_type": "code",
        "outputId": "935f181c-8f6a-4587-b02b-fe9beb59c7df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(increment_x)\n",
        "  print(x.eval())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4la_pAale00",
        "colab_type": "text"
      },
      "source": [
        "While this feature can be convenient, you have to be careful to not use the same variable names by accident. One way to avoid this is by using a variable scope with a unique name for each computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS1AIxlRlu22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.variable_scope('problem_1'):\n",
        "  x1 = tf.Variable(0.0, name='x')\n",
        "  increment_x1 = tf.assign(x1, x1 + 1)\n",
        "  \n",
        "with tf.variable_scope('problem_2'):\n",
        "  x2 = tf.Variable(0.0, name='x')\n",
        "  increment_x2 = tf.assign(x2, x2 + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4KFk7oA_nVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a646b52c-c2fd-4c4e-b5f0-03bbfe6e630a"
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  x1.initializer.run()\n",
        "  print(increment_x1.eval())\n",
        "  \n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  x2.initializer.run()\n",
        "  print(increment_x2.eval())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqYDwyvQm9Ma",
        "colab_type": "text"
      },
      "source": [
        "You can even use resource containers to store variables across different graphs. In order to reset a resource container, run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSbuQGC0-kk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222', ['problem_1'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mrKVhJN_2Ko",
        "colab_type": "text"
      },
      "source": [
        "### Asynchronouse Communication Using TensorFlow Queues\n",
        "\n",
        "Queues are a way to share data across multiple sessions. One common use case is for passing mini-batches of data between sessions for training. One graph may load the client data and push it to the queue where another pulls the data and trains a neural network.\n",
        "\n",
        "TensorFlow provides various kinds of queues, the most simple is the _first-in first-out_ (FIFO) queue. Below is an example of a FIFO queue with TensorFlow that can hold up to 10 tensors containing 2 floats:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeFYlcBnAtOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[[2]], name='q',\n",
        "                 shared_name='shared_q')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk-lzJrRBKOq",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow uses the `shared_name` parameter to refer to queues across sessions.\n",
        "\n",
        "#### Enqueuing data\n",
        "\n",
        "To push data into the queue, you need to use the `enqueue` method. The following code pushes data to the queue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyOJwczEBfwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_instance = tf.placeholder(tf.float32, shape=[2])\n",
        "enqueue = q.enqueue([training_instance])\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue, feed_dict={training_instance: [1.0, 2.0]})\n",
        "  sess.run(enqueue, feed_dict={training_instance: [3.0, 4.0]})\n",
        "  sess.run(enqueue, feed_dict={training_instance: [5.0, 6.0]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXQsiXhGCIuW",
        "colab_type": "text"
      },
      "source": [
        "You can enqueue multiple tensors at once using the `enqueue_many` method below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY0qulU5CNRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_instances = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "enqueue_many = q.enqueue_many([training_instances])\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue_many, feed_dict={\n",
        "      training_instances: [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]],\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw_9xtuNC_Li",
        "colab_type": "text"
      },
      "source": [
        "#### Dequeuing data\n",
        "\n",
        "You use the `dequeue` method to pull tensors from the queue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srZ59XYNDMgQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "13e28e92-70ba-4b3f-f44d-fa475835663b"
      },
      "source": [
        "dequeue = q.dequeue()\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  print(sess.run(dequeue))\n",
        "  print(sess.run(dequeue))\n",
        "  print(sess.run(dequeue))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 2.]\n",
            "[3. 4.]\n",
            "[5. 6.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCUuezuQE1LG",
        "colab_type": "text"
      },
      "source": [
        "In order to dequeue multiple items at once, you should use the `dequeue_many` method and specify how many items to dequeue each time. If you call it when there are not enough items in the queue, it will block execution until the queue has the specified amount."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhEb7sQ2FFMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3618344e-c763-4de1-a8f3-a4b44a600457"
      },
      "source": [
        "batch_size = 2\n",
        "dequeue_mini_batch = q.dequeue_many(batch_size)\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  print(sess.run(dequeue_mini_batch))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2.]\n",
            " [3. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpgVTnC-Ft0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-oOfS4XF6Bk",
        "colab_type": "text"
      },
      "source": [
        "#### Queues of tuples\n",
        "\n",
        "Queues can also hold tuples of tensors of different shapes instead of a single tensor. The following queue stores two tensors: a scalar `int32` tensor and a `float32` tensor with shape `[3,2]`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DziNS45ZGbM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = tf.FIFOQueue(capacity=10, dtypes=[tf.int32, tf.float32], shapes=[[], [3,2]],\n",
        "                 name='q', shared_name='shared_q')\n",
        "\n",
        "a = tf.placeholder(tf.int32, shape=())\n",
        "b = tf.placeholder(tf.float32, shape=(3, 2))\n",
        "enqueue = q.enqueue((a, b))\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue, feed_dict={a: 10, b: [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]})\n",
        "  sess.run(enqueue, feed_dict={a: 11, b: [[2.0, 4.0], [6.0, 8.0], [0.0, 2.0]]})\n",
        "  sess.run(enqueue, feed_dict={a: 12, b: [[3.0, 6.0], [9.0, 2.0], [5.0, 8.0]]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUSEExcJHqlq",
        "colab_type": "text"
      },
      "source": [
        "The `dequeue` method now creates a tuple of operations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6itG72fHwL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f543c262-e7a4-45ef-ba3b-d234114503e8"
      },
      "source": [
        "dequeue_a, dequeue_b = q.dequeue()\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  a, b = sess.run([dequeue_a, dequeue_b])\n",
        "  print(a)\n",
        "  print(b)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "[[1. 2.]\n",
            " [3. 4.]\n",
            " [5. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdyGGFfnIvA0",
        "colab_type": "text"
      },
      "source": [
        "`dequeue_many` also returns atuple of operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdVKqjqMI92E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "85540d3d-f243-4cfe-e57d-16fac6961d96"
      },
      "source": [
        "batch_size = 2\n",
        "dequeue_as, dequeue_bs = q.dequeue_many(batch_size)\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  a, b = sess.run([dequeue_as, dequeue_bs])\n",
        "  print(a)\n",
        "  print(b)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11 12]\n",
            "[[[2. 4.]\n",
            "  [6. 8.]\n",
            "  [0. 2.]]\n",
            "\n",
            " [[3. 6.]\n",
            "  [9. 2.]\n",
            "  [5. 8.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kys29N-2KrU-",
        "colab_type": "text"
      },
      "source": [
        "#### Closing a queue\n",
        "\n",
        "It is possible to close a queue so that the other sessions can no longer enqueue data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eKS-of1K0Tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close_q = q.close()\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(close_q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL4501StMha_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKWQpel3K86w",
        "colab_type": "text"
      },
      "source": [
        "Attempting to push to a closed queue will raise an exception, but any pending enqueues will be honored unless you call `q.close(cancel_pending_enqueues=True)`. Subsequent dequeue operations will still work as long as there are enough itmes in the queue as long as there are at least as many items left in the queue, otherwise the operation will fail.\n",
        "\n",
        "You can use the `dequeue_up_to` method instead which will empty the queue if there are less than `batch_size` items left.\n",
        "\n",
        "#### RandomShuffleQueue\n",
        "\n",
        "Another type of queue that TensorFlow supports is the `RandomShuffleQueue` which returns items in the queue in random order. Below is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFR57kkxMflJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = tf.RandomShuffleQueue(capacity=50, min_after_dequeue=10,\n",
        "                          dtypes=[tf.float32], shapes=[()], name='q',\n",
        "                          shared_name='shared_q')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIGkLbx9N5xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_instances = tf.placeholder(tf.float32, shape=(None))\n",
        "enqueue = q.enqueue_many([training_instances])\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue, feed_dict={\n",
        "      training_instances: [float(i) for i in range(25)]\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sca_lNhM-_7",
        "colab_type": "text"
      },
      "source": [
        "The `min_after_dequeue` specifies the minimum number of items that should be left in the queue after a dequeue operation to ensure the random behavior of queue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d18Nk8egPPC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9d805b24-a0ec-4ba5-9f3f-fa9609813335"
      },
      "source": [
        "batch_size = 5\n",
        "dequeue = q.dequeue_many(batch_size)\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  print(sess.run(dequeue))\n",
        "  print(sess.run(dequeue))\n",
        "  print(sess.run(dequeue))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 9. 12.  7. 24.  3.]\n",
            "[18. 19. 14.  8. 11.]\n",
            "[22.  2. 17. 10.  5.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzpnQnz0PjAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqV9YperPpQ5",
        "colab_type": "text"
      },
      "source": [
        "#### PaddingFIFOQueue\n",
        "\n",
        "A `PaddingFIFOQueue` is a queue which holds tensors of any dimension as long as they are the same rank. When you dequeue them individually the tensors come back as they were enqueued. When you use `dequeue_many` or `dequeue_up_to`, each tensor is padded with zeros so that they are each the same size as the largest tensor in the mini-batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-PTmYV7SQG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = tf.PaddingFIFOQueue(capacity=50, dtypes=[tf.float32],\n",
        "                        shapes=[(None, None)], name='q',\n",
        "                        shared_name='shared_q')\n",
        "v = tf.placeholder(tf.float32, shape=(None, None))\n",
        "enqueue = q.enqueue([v])\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue, feed_dict={v: [[1., 2.], [3., 4.], [5., 6.]]})\n",
        "  sess.run(enqueue, feed_dict={v: [[1.]]})\n",
        "  sess.run(enqueue, feed_dict={v: [[7., 8., 9., 5.], [6., 7., 8., 9.]]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwQXgPBhTP-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "46a44b73-cab2-4fd5-d746-a6e6dc6e045e"
      },
      "source": [
        "batch_size = 3\n",
        "dequeue = q.dequeue_many(batch_size)\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  print(sess.run(dequeue))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1. 2. 0. 0.]\n",
            "  [3. 4. 0. 0.]\n",
            "  [5. 6. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[7. 8. 9. 5.]\n",
            "  [6. 7. 8. 9.]\n",
            "  [0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAbp4-PVTZR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czQJC7dPTcFW",
        "colab_type": "text"
      },
      "source": [
        "This type of queue is useful for variable length inputs such as sequences of words.\n",
        "\n",
        "### Loading Data Directly from the Graph\n",
        "\n",
        "So far, we have only fed training data to the TensorFlow clusters using placeholders, which involves three steps:\n",
        "\n",
        "1. Load the data from the filesystem to the client task.\n",
        "\n",
        "2. Send the data from the client to the master task.\n",
        "\n",
        "3. Send the data from the master task to other tasks which need the data for computation.\n",
        "\n",
        "This process can be very inefficient for large datasets or when the training graph is distributed across many tasks.\n",
        "\n",
        "#### Preload the data into a variable\n",
        "\n",
        "If the dataset fits into memory, one option is to load the training data into a variable and use that variable in your graph. This is called _preloading_ the training set. This way the data only needs to be upload the data to the cluster once, though it may need to be transferred across tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd59nVWUmsb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "data = \\\n",
        "  [[0., 1., 2., 3., 4.],\n",
        "   [5., 6., 7., 8., 9.],\n",
        "   [10., 11., 12., 13., 14.],\n",
        "   [15., 16., 17., 18., 19.]]\n",
        "\n",
        "training_set_init = tf.placeholder(tf.float32, shape=(4, 5))\n",
        "training_set = tf.Variable(training_set_init, trainable=False, collections=[],\n",
        "                           name='training_set')\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(training_set.initializer, feed_dict={training_set_init: data})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Awyb8mw8KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvRCsQ13qZ6d",
        "colab_type": "text"
      },
      "source": [
        "Setting `trainable=False` so optimizers don't change the value and `collections=[]` prevents a `Saver` from storing the value of the variable in memory.\n",
        "\n",
        "\n",
        "#### Reading the training data directly from the graph\n",
        "\n",
        "Below is an example of using TensorFlow to read data from a CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VITLIOx8q6ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Writing the CSV file.\n",
        "\n",
        "csv_data = \\\n",
        "  'x1, x2, target\\n' \\\n",
        "  '1., 2., 0\\n' \\\n",
        "  '4., 5., 1\\n' \\\n",
        "  '7., , 0'\n",
        "csv_filename = 'my_test.csv'\n",
        "\n",
        "with open(csv_filename, 'w') as f:\n",
        "  f.write(csv_data)\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Za0zkQZrln5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3fe36c4c-ad1f-45ac-bd30-501584fbd6d8"
      },
      "source": [
        "# Creating a TextLineReader object, a stateful object which reads data\n",
        "# from a file.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "reader = tf.TextLineReader(skip_header_lines=1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-72d2ad0d8bc6>:4: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh88PmmOsu5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a queue to keep track of which files we want to read from.\n",
        "# Including a placeholder for the filename, an enqueue operation, and\n",
        "# a close operation.\n",
        "\n",
        "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
        "filename = tf.placeholder(tf.string)\n",
        "enqueue_filename = filename_queue.enqueue([filename])\n",
        "close_filename_queue = filename_queue.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HtH95KZuIAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the read operation using the Reader object. The key is a unique for\n",
        "# each record (filename:line_number) and the value is a string containing the\n",
        "# content of the line.\n",
        "\n",
        "key, value = reader.read(filename_queue)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kz939lTvHLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parsing the string to create the training set features\n",
        "\n",
        "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
        "features = tf.stack([x1, x2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvkJjjzwvXbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally push the training instance to a RandomShuffleQueue that will be\n",
        "# part of the training graph.\n",
        "\n",
        "instance_queue = tf.RandomShuffleQueue(capacity=10, min_after_dequeue=2,\n",
        "                                       dtypes=[tf.float32, tf.int32],\n",
        "                                       shapes=[[2], []], name='instance_q',\n",
        "                                       shared_name='shared_instance_q')\n",
        "enqueue_instance = instance_queue.enqueue([features, target])\n",
        "close_instance_queue = instance_queue.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1_9_N9iwsrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example of a Session that adds training instances to the instance queue\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue_filename, feed_dict={filename: csv_filename})\n",
        "  sess.run(close_filename_queue)\n",
        "  try:\n",
        "    while True:\n",
        "      sess.run(enqueue_instance)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "  sess.run(close_instance_queue)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XWZPJry4cFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2f60e6cf-c588-497c-e664-006cc5a00cf1"
      },
      "source": [
        "# An example of reading training data from the instance queue.\n",
        "\n",
        "mini_batch_instances, mini_batch_targets = instance_queue.dequeue_up_to(2)\n",
        "\n",
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  try:\n",
        "    while True:\n",
        "      print(sess.run([mini_batch_instances, mini_batch_targets]))\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[4., 5.],\n",
            "       [1., 2.]], dtype=float32), array([1, 0], dtype=int32)]\n",
            "[array([[7., 0.]], dtype=float32), array([0], dtype=int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFKqTS0441RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amj_k8f6yKMp",
        "colab_type": "text"
      },
      "source": [
        "In addition to CSV files, you can also have TensorFlow read from fixed-length binary records or TensorFlow's TFRecord format which is baed on protocol buffers.\n",
        "\n",
        "One limitation of this architecture uses only one thread to read records and push them to the instance queue. \n",
        "\n",
        "#### Multithreaded readers using a Coordinator and a QueueRunner\n",
        "\n",
        "Below is an example of using TensorFlow's `Coordinator` and `QueueRunner` classes which are used to read data from multiple threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpEK0MQs0aT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
        "filename = tf.placeholder(tf.string)\n",
        "enqueue_filename = filename_queue.enqueue([filename])\n",
        "close_filename_queue = filename_queue.close()\n",
        "\n",
        "reader = tf.TextLineReader(skip_header_lines=1)\n",
        "\n",
        "key, value = reader.read(filename_queue)\n",
        "\n",
        "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
        "features = tf.stack([x1, x2])\n",
        "\n",
        "instance_queue = tf.RandomShuffleQueue(capacity=10, min_after_dequeue=2,\n",
        "                                       dtypes=[tf.float32, tf.int32],\n",
        "                                       shapes=[[2], []], name='instance_q',\n",
        "                                       shared_name='shared_instance_q')\n",
        "enqueue_instance = instance_queue.enqueue([features, target])\n",
        "close_instance_queue = instance_queue.close()\n",
        "\n",
        "mini_batch_instances, mini_batch_targets = instance_queue.dequeue_up_to(2)\n",
        "\n",
        "# New code here!\n",
        "n_threads = 5\n",
        "queue_runner = tf.train.QueueRunner(instance_queue,\n",
        "                                    [enqueue_instance] * n_threads)\n",
        "coord = tf.train.Coordinator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLXczJVj5XZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cba24932-6ac3-40cb-daab-ff0ab6d50a1f"
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue_filename, {filename: csv_filename})\n",
        "  sess.run(close_filename_queue)\n",
        "  enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
        "  try:\n",
        "    while True:\n",
        "      print(sess.run([mini_batch_instances, mini_batch_targets]))\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[4., 5.],\n",
            "       [7., 0.]], dtype=float32), array([1, 0], dtype=int32)]\n",
            "[array([[1., 2.]], dtype=float32), array([0], dtype=int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcZiIbq65V-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUid5mt-5pHH",
        "colab_type": "text"
      },
      "source": [
        "Another way you can further parallelize this process is by sharding your training set into multiple CSV files and read from multiple file queues. The following code defines a function which creates a reader which pushes data from a file queue to an instance queue. In order to read from multiple files in parallel, simply provide multiple file queues to the `QueueRunner`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNlEafWh8s-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function to read from a file queue and push to an instance queue.\n",
        "\n",
        "def read_and_push_instance(file_queue, instance_queue, skip_header_lines=1):\n",
        "  reader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n",
        "  key, value = reader.read(file_queue)\n",
        "  x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
        "  features = tf.stack([x1, x2])\n",
        "  enqueue_instance = instance_queue.enqueue([features, target])\n",
        "  return enqueue_instance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8LcDZdN9LVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the TensorFlow graph to read from the file.\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
        "filename = tf.placeholder(tf.string)\n",
        "enqueue_filename = filename_queue.enqueue([filename])\n",
        "close_filename_queue = filename_queue.close()\n",
        "\n",
        "instance_queue = tf.RandomShuffleQueue(capacity=10, min_after_dequeue=2,\n",
        "                                       dtypes=[tf.float32, tf.int32],\n",
        "                                       shapes=[[2], []], name='instance_q',\n",
        "                                       shared_name='shared_instance_q')\n",
        "\n",
        "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
        "\n",
        "read_and_enqueue_ops = [read_and_push_instance(filename_queue, instance_queue)\n",
        "                        for _ in range(5)]\n",
        "\n",
        "queue_runner = tf.train.QueueRunner(instance_queue, read_and_enqueue_ops)\n",
        "coord = tf.train.Coordinator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5iuoeZR-7Rx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "897a3b83-565e-4c44-c992-e83f55953c03"
      },
      "source": [
        "with tf.Session('grpc://127.0.0.1:2222') as sess:\n",
        "  sess.run(enqueue_filename, {filename: csv_filename})\n",
        "  sess.run(close_filename_queue)\n",
        "  enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
        "  try:\n",
        "    while True:\n",
        "      print(sess.run([minibatch_instances, minibatch_targets]))\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[1., 2.],\n",
            "       [7., 0.]], dtype=float32), array([0, 0], dtype=int32)]\n",
            "[array([[4., 5.]], dtype=float32), array([1], dtype=int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yXCmoeE_W7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Session.reset('grpc://127.0.0.1:2222')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}