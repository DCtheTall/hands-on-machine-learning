{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParallelNeuralNetworks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3xzG7A5Xs1l",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 12: Distributing TensorFlow Across Devices and Servers\n",
        "\n",
        "This notebook is my solution to exercise 10 of chapter 12. It contains three distributed models. Each model requires you restart the kernel and run the code in the **Installation** section.\n",
        "\n",
        "## Exercise 10\n",
        "\n",
        "Train a DNN using between-graph replication and data parallelism with asynchronous updates, timimg how long it taeks to reach a satisfying performance. Next, try again using synchronous updates. Do synchronous updates produce a better model? Does it train faster? Split the DNN vertically and place each vertical slice on a different device, and train the model again. Is training any faster? Is performance any different?\n",
        "\n",
        "## Solution\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIhV8p2hXpz4",
        "colab_type": "code",
        "outputId": "b0ac524c-e298-4742-dab6-364bd418d557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka-bo8IbabiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install --upgrade tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3UVs6jja-sX",
        "colab_type": "text"
      },
      "source": [
        "### Asynchronous Updates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwwVpsEcTRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading MNIST dataset.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v94pVZfneU7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the cluster spec for the parallel model.\n",
        "\n",
        "n_dnns = 3\n",
        "\n",
        "cluster_spec = tf.train.ClusterSpec({\n",
        "    'ps': ['127.0.0.1:1000'],\n",
        "    'worker': ['127.0.0.1:100{}'.format(i + 1) for i in range(1, n_dnns)]\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEU9WH5-rHdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Abstracting the operations with the individual workers which train their own\n",
        "# copy of the DNN into a class. This model uses the hyperparameters that led\n",
        "# to the best performance on the validation set in exercise 8.\n",
        "\n",
        "n_outputs = 10\n",
        "\n",
        "class DNNTask:\n",
        "  def __init__(self, X, y, task, parameters, activation=tf.nn.elu, n_outputs=10,\n",
        "               learning_rate=0.01, momentum=0.95, n_epochs=200, batch_size=50):\n",
        "    self.task = task\n",
        "    self.parameters = parameters\n",
        "    self.batch_size = batch_size\n",
        "    self.n_batches = len(X_train) // batch_size\n",
        "    self.n_epochs = n_epochs\n",
        "\n",
        "    self._X = X\n",
        "    self._y = y\n",
        "    self._gpu_name = '/job:worker/task:{}/gpu:0'.format(task)\n",
        "    self._cpu_name = '/job:worker/task:{}/cpu:0'.format(task)\n",
        "    self._model_path = 'model{}.ckpt'.format(task)\n",
        "\n",
        "    with tf.device(self._gpu_name):\n",
        "      with tf.variable_scope('worker{}'.format(task)):\n",
        "        self._hidden_layers = []\n",
        "        for i, params in enumerate(parameters):\n",
        "          W, b = params\n",
        "          self._hidden_layers.append(\n",
        "              activation(\n",
        "                  tf.matmul(\n",
        "                      (X if i == 0 else self._hidden_layers[-1]), W) + b))\n",
        "        self._logits = tf.layers.dense(self._hidden_layers[-1], n_outputs)\n",
        "        self._xentropy = \\\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                           logits=self._logits)\n",
        "        self._loss = tf.reduce_mean(self._xentropy)\n",
        "        self._optimizer = \\\n",
        "            tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
        "                                       momentum=momentum)\n",
        "        self._training_op = self._optimizer.minimize(self._loss)\n",
        "\n",
        "    with tf.device(self._cpu_name):\n",
        "      with tf.variable_scope('worker{}'.format(task)):\n",
        "        self._saver = tf.train.Saver()\n",
        "        self._init = tf.global_variables_initializer()\n",
        "        self._correct = tf.nn.in_top_k(self._logits, y, 1)\n",
        "        self._accuracy = tf.reduce_mean(tf.cast(self._correct, tf.float32))\n",
        "        self._shuffle_queue = \\\n",
        "            tf.RandomShuffleQueue(capacity=len(X_train), min_after_dequeue=0,\n",
        "                                  dtypes=[tf.float32, tf.int32],\n",
        "                                  shapes=[(n_inputs), ()], name='input_queue',\n",
        "                                  shared_name='input_queue')\n",
        "        self._enqueue_op = self._shuffle_queue.enqueue_many([X, y],\n",
        "                                                            name='enqueue')\n",
        "        self._dequeue_op = self._shuffle_queue.dequeue_up_to(batch_size,\n",
        "                                                             name='dequeue')\n",
        "\n",
        "  def train_model(self, sess):\n",
        "    with sess.as_default():\n",
        "      sess.run(self._init)\n",
        "\n",
        "      best_loss = 0\n",
        "      rounds_since_best_loss = 0\n",
        "\n",
        "      for epoch in range(self.n_epochs):\n",
        "        sess.run(self._enqueue_op, feed_dict={self._X: X_train,\n",
        "                                               self._y: y_train})\n",
        "        for _ in range(self.n_batches):\n",
        "          X_batch, y_batch = sess.run(self._dequeue_op)\n",
        "          sess.run(self._training_op, feed_dict={self._X: X_batch,\n",
        "                                                 self._y: y_batch})\n",
        "        if epoch == 0:\n",
        "          best_loss = self._loss.eval(feed_dict={self._X: X_train,\n",
        "                                                 self._y: y_train})\n",
        "          self._saver.save(sess, self._model_path)\n",
        "        elif epoch % 5 == 0:\n",
        "          loss_val = self._loss.eval(feed_dict={self._X: X_train,\n",
        "                                                self._y: y_train})\n",
        "          if loss_val < best_loss:\n",
        "            best_loss = loss_val\n",
        "            rounds_since_best_loss = 0\n",
        "            self._saver.save(sess, self._model_path)\n",
        "          else:\n",
        "            rounds_since_best_loss += 1\n",
        "            if rounds_since_best_loss == 6:\n",
        "              break\n",
        "      else:\n",
        "        self._saver.save(sess, self._model_path)\n",
        "\n",
        "      self._saver.restore(sess, self._model_path)\n",
        "      acc_val = self._accuracy.eval(feed_dict={self._X: X_test,\n",
        "                                               self._y: y_test})\n",
        "      print('Task {} Complete!\\nTest set accuracy: {}'.format(\n",
        "          self.task, acc_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfT_0bVsmerm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2c6ff3ac-fb69-439b-92b3-de7640821973"
      },
      "source": [
        "# Defining the graph for the model.\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden_layers = 5\n",
        "n_neurons = 160\n",
        "stddev = 2.0 / np.sqrt(n_inputs + n_neurons)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "ensemble = []\n",
        "\n",
        "with tf.device('/job:ps/task:0/cpu:0'):\n",
        "  with tf.variable_scope('ps0'):\n",
        "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "    y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "    parameters = []\n",
        "    for i in range(n_hidden_layers):\n",
        "      W = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              (n_inputs if i == 0 else n_neurons, n_neurons),\n",
        "              mean=0.0, stddev=stddev))\n",
        "      b = tf.Variable(tf.zeros([n_neurons]))\n",
        "      parameters.append((W, b))\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "for task in range(n_dnns):\n",
        "  ensemble.append(DNNTask(X, y, task, parameters))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-06ad379b1ac0>:28: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYgWjZMBtfBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starting the servers.\n",
        "\n",
        "ps = tf.train.Server(cluster_spec, job_name='ps', task_index=0)\n",
        "workers = []\n",
        "\n",
        "for task in range(n_dnns):\n",
        "  workers.append(\n",
        "      tf.train.Server(cluster_spec, job_name='worker', task_index=task))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUUgK9nPu9fP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the Clock class for timing training.\n",
        "\n",
        "import time\n",
        "\n",
        "class Clock:\n",
        "  def __init__(self):\n",
        "    self.start_time = None\n",
        "  def start(self):\n",
        "    self.start_time = time.time()\n",
        "    return self\n",
        "  def stop(self):\n",
        "    dt = time.time() - self.start_time\n",
        "    self.start_time = None\n",
        "    h, m, s = int(dt // 3600), int(dt % 3600) // 60, dt % 60\n",
        "    return '{}h {}m {:.3f}s'.format(h, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUxoSk3nvSo4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "a0fcaa3b-d5c7-4b80-f57f-3faec2d21cea"
      },
      "source": [
        "# Running the training algorithm\n",
        "\n",
        "from threading import Thread\n",
        "\n",
        "clock = Clock().start()\n",
        "\n",
        "with tf.Session(ps.target) as sess:\n",
        "  sess.run(init)\n",
        "  threads = []\n",
        "  for task in range(n_dnns):\n",
        "    thread = Thread(target=lambda s: ensemble[task].train_model(s),\n",
        "                    args=(sess,))\n",
        "    thread.start()\n",
        "    threads.append(thread)\n",
        "  for thread in threads:\n",
        "    thread.join()\n",
        "print('Time taken to train model:', clock.stop())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model0.ckpt\n",
            "Task 0 Complete!\n",
            "Test set accuracy: 0.9850000143051147\n",
            "INFO:tensorflow:Restoring parameters from model1.ckpt\n",
            "Task 1 Complete!\n",
            "Test set accuracy: 0.9850999712944031\n",
            "INFO:tensorflow:Restoring parameters from model2.ckpt\n",
            "Task 2 Complete!\n",
            "Test set accuracy: 0.9847999811172485\n",
            "Time taken to train model: 1h 26m 26.734s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBKVf-wAqF8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkEpJZaHgbE5",
        "colab_type": "text"
      },
      "source": [
        "The model performed slightly better than a single neural network with those parameters, but training took significantly longer.\n",
        "\n",
        "### Synchronous Updates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF2WnRj2hF-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading MNIST dataset.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdQpZl82hNiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the cluster spec for the parallel model.\n",
        "\n",
        "n_dnns = 3\n",
        "\n",
        "cluster_spec = tf.train.ClusterSpec({\n",
        "    'ps': ['127.0.0.1:1000'],\n",
        "    'worker': ['127.0.0.1:100{}'.format(i + 1) for i in range(n_dnns)]\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDTudLkIh9Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Abstracting the training route of each DNN worker task into a class.\n",
        "# This class is not responsible for training the model. It receives\n",
        "# parameter updates from the parameter server job and then computes the\n",
        "# error gradient which it sends back to the parameter server to update.\n",
        "\n",
        "class DNNTask:\n",
        "  def __init__(self, X, y, task, parameters, activation=tf.nn.elu, n_outputs=10,\n",
        "               learning_rate=0.01, momentum=0.95):\n",
        "    self._X = tf.placeholder\n",
        "\n",
        "  def run_training_epoch(self, sess):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yEkRFR0jt2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the graph for training the model.\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden_layers = 5\n",
        "n_neurons = 160\n",
        "stddev = 2.0 / np.sqrt(n_inputs + n_neurons)\n",
        "\n",
        "with tf.device('/job:ps/task:0/cpu:0'):\n",
        "  with tf.variable_scope('ps0'):\n",
        "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "    y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "    parameters = []\n",
        "    for i in range(n_hidden_layers):\n",
        "      W = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              (n_inputs if i == 0 else n_neurons, n_neurons),\n",
        "              mean=0.0, stddev=stddev))\n",
        "      b = tf.Variable(tf.zeros([n_neurons]))\n",
        "      parameters.append((W, b))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}