{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParallelNeuralNetworks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3xzG7A5Xs1l",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 12: Distributing TensorFlow Across Devices and Servers\n",
        "\n",
        "This notebook is my solution to exercise 10 of chapter 12. It contains three distributed models. Each model requires you restart the kernel and run the code in the **Installation** section.\n",
        "\n",
        "## Exercise 10\n",
        "\n",
        "Train a DNN using between-graph replication and data parallelism with asynchronous updates, timing how long it takes to reach a satisfying performance. Next, try again using synchronous updates. Do synchronous updates produce a better model? Does it train faster? Split the DNN vertically and place each vertical slice on a different device, and train the model again. Is training any faster? Is performance any different?\n",
        "\n",
        "## Solution\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIhV8p2hXpz4",
        "colab_type": "code",
        "outputId": "728659b8-6328-44c5-a816-676e4015b685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka-bo8IbabiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install --upgrade tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3UVs6jja-sX",
        "colab_type": "text"
      },
      "source": [
        "### Asynchronous Updates\n",
        "\n",
        "Since I am not able to run TensorFlow from multiple clients in Colab, I am going to be using in-graph replication instead for asynchronous updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwwVpsEcTRN",
        "colab_type": "code",
        "outputId": "9fc947a5-0025-40ce-a7ce-0b5917c64401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Downloading MNIST dataset.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v94pVZfneU7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the cluster spec for the parallel model.\n",
        "\n",
        "n_dnns = 3\n",
        "\n",
        "cluster_spec = tf.train.ClusterSpec({\n",
        "    'ps': ['127.0.0.1:1000'],\n",
        "    'worker': ['127.0.0.1:100{}'.format(i + 1) for i in range(n_dnns)]\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEU9WH5-rHdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Abstracting the operations with the individual workers which train their own\n",
        "# copy of the DNN into a class. This model uses the hyperparameters that led\n",
        "# to the best performance on the validation set in exercise 8.\n",
        "\n",
        "class DNNTask:\n",
        "  def __init__(self, X, y, task, parameters, activation=tf.nn.elu, n_inputs=(28 ** 2),\n",
        "               n_outputs=10, learning_rate=0.01, momentum=0.95, n_epochs=100,\n",
        "               batch_size=50):\n",
        "    self.task = task\n",
        "    self.parameters = parameters\n",
        "    self.batch_size = batch_size\n",
        "    self.n_batches = len(X_train) // batch_size\n",
        "    self.n_epochs = n_epochs\n",
        "\n",
        "    self._gpu_name = '/job:worker/task:{}/gpu:0'.format(task)\n",
        "    self._cpu_name = '/job:worker/task:{}/cpu:0'.format(task)\n",
        "    self._X = X\n",
        "    self._y = y\n",
        "\n",
        "    with tf.device(self._gpu_name):\n",
        "      with tf.variable_scope('worker{}'.format(task)):\n",
        "        self._hidden_layers = []\n",
        "        self._grad_vars = []\n",
        "        for i, params in enumerate(parameters[:-1]):\n",
        "          W, b = params\n",
        "          self._grad_vars += [W, b]\n",
        "          self._hidden_layers.append(\n",
        "              activation(\n",
        "                  tf.matmul(\n",
        "                      (self._X if i == 0 else self._hidden_layers[-1]), W) + b))\n",
        "        W, b = parameters[-1]\n",
        "        self._logits = tf.matmul(self._hidden_layers[-1], W) + b\n",
        "        self._xentropy = \\\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self._y,\n",
        "                                                           logits=self._logits)\n",
        "        self._loss = tf.reduce_mean(self._xentropy)\n",
        "        self._optimizer = \\\n",
        "            tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
        "                                       momentum=momentum)\n",
        "        self._training_op = self._optimizer.minimize(self._loss,\n",
        "                                                     var_list=parameters)\n",
        "\n",
        "    with tf.device(self._cpu_name):\n",
        "      with tf.variable_scope('worker{}'.format(task)):\n",
        "        self._init = tf.global_variables_initializer()\n",
        "        self._correct = tf.nn.in_top_k(self._logits, self._y, 1)\n",
        "        self._accuracy = tf.reduce_mean(tf.cast(self._correct, tf.float32))\n",
        "        self._shuffle_queue = \\\n",
        "            tf.RandomShuffleQueue(capacity=len(X_train), min_after_dequeue=0,\n",
        "                                  dtypes=[tf.float32, tf.int32],\n",
        "                                  shapes=[(n_inputs), ()], name='input_queue',\n",
        "                                  shared_name='input_queue')\n",
        "        self._enqueue_op = self._shuffle_queue.enqueue_many([self._X, self._y],\n",
        "                                                            name='enqueue')\n",
        "        self._dequeue_op = self._shuffle_queue.dequeue_up_to(batch_size,\n",
        "                                                             name='dequeue')\n",
        "\n",
        "  def train_model(self, sess):\n",
        "    with sess.as_default():\n",
        "      sess.run(self._init)\n",
        "\n",
        "      for epoch in range(self.n_epochs):\n",
        "        sess.run(self._enqueue_op, feed_dict={self._X: X_train,\n",
        "                                               self._y: y_train})\n",
        "        for _ in range(self.n_batches):\n",
        "          X_batch, y_batch = sess.run(self._dequeue_op)\n",
        "          sess.run(self._training_op, feed_dict={self._X: X_batch,\n",
        "                                                 self._y: y_batch})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfT_0bVsmerm",
        "colab_type": "code",
        "outputId": "99c0e1f5-7328-4451-c8d4-d994b89e039b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Defining the graph for the model.\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden_layers = 5\n",
        "n_neurons = 160\n",
        "stddev = 2.0 / np.sqrt(n_inputs + n_neurons)\n",
        "n_outputs = 10\n",
        "\n",
        "tf.reset_default_graph()\n",
        "ensemble = []\n",
        "\n",
        "with tf.device('/job:ps/task:0/cpu:0'):\n",
        "  with tf.variable_scope('ps0'):\n",
        "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "    y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "    parameters = []\n",
        "    for i in range(n_hidden_layers):\n",
        "      W = tf.Variable(\n",
        "          tf.truncated_normal(\n",
        "              (n_inputs if i == 0 else n_neurons, n_neurons),\n",
        "              mean=0.0, stddev=stddev))\n",
        "      b = tf.Variable(tf.zeros([n_neurons]))\n",
        "      parameters.append((W, b))\n",
        "    W = tf.Variable(\n",
        "        tf.truncated_normal((n_neurons, n_outputs),\n",
        "            mean=0.0, stddev=stddev))\n",
        "    b = tf.Variable(tf.zeros([n_outputs]))\n",
        "    parameters.append((W, b))\n",
        "\n",
        "    hidden_layers = []\n",
        "    for i, params in enumerate(parameters[:-1]):\n",
        "      W, b = params\n",
        "      hidden_layers.append(\n",
        "          tf.nn.elu(tf.matmul((X if i == 0 else hidden_layers[-1]), W) + b))\n",
        "    W, b = parameters[-1]\n",
        "    logits = tf.matmul(hidden_layers[-1], W) + b\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "for task in range(n_dnns):\n",
        "  ensemble.append(DNNTask(X, y, task, parameters))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYgWjZMBtfBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starting the servers.\n",
        "\n",
        "ps = tf.train.Server(cluster_spec, job_name='ps', task_index=0)\n",
        "workers = []\n",
        "\n",
        "for task in range(n_dnns):\n",
        "  workers.append(\n",
        "      tf.train.Server(cluster_spec, job_name='worker', task_index=task))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUUgK9nPu9fP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the Clock class for timing training.\n",
        "\n",
        "import time\n",
        "\n",
        "class Clock:\n",
        "  def __init__(self):\n",
        "    self.start_time = None\n",
        "  def start(self):\n",
        "    self.start_time = time.time()\n",
        "    return self\n",
        "  def stop(self):\n",
        "    dt = time.time() - self.start_time\n",
        "    self.start_time = None\n",
        "    h, m, s = int(dt // 3600), int(dt % 3600) // 60, dt % 60\n",
        "    return '{}h {}m {:.3f}s'.format(h, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUxoSk3nvSo4",
        "colab_type": "code",
        "outputId": "bf3b6349-479b-4259-b210-1a2d7e5216b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Running the training algorithm\n",
        "\n",
        "from threading import Thread\n",
        "\n",
        "clock = Clock().start()\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.Session(ps.target, config=config) as sess:\n",
        "  sess.run(init)\n",
        "  threads = []\n",
        "  for task in range(n_dnns):\n",
        "    thread = Thread(target=lambda s: ensemble[task].train_model(s),\n",
        "                    args=(sess,))\n",
        "    thread.start()\n",
        "    threads.append(thread)\n",
        "  for thread in threads:\n",
        "    thread.join()\n",
        "  acc_val = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "print('Time taken to train model:', clock.stop())\n",
        "print('Test set accuracy', acc_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to train model: 0h 43m 50.663s\n",
            "Test set accuracy 0.9832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkEpJZaHgbE5",
        "colab_type": "text"
      },
      "source": [
        "The model performed about as well as a single neural network with those parameters, but training took significantly longer.\n",
        "\n",
        "### Synchronous Updates\n",
        "\n",
        "TODO using TensorFlow's experimental `Strategry` API after going through their guide."
      ]
    }
  ]
}