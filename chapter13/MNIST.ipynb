{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlgMFaMI8M12",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 13: Convolutional Neural Networks\n",
        "\n",
        "## Exercise 7\n",
        "\n",
        "Build your own CNN and try to achieve the highest possible accuracy on MNIST.\n",
        "\n",
        "## Solution\n",
        "\n",
        "For this exercise, I am going to implement an augmented version of LeNet-5 where the convolutional layers with a 5$\\times$5 kernel have been replaced by two layers with 3$\\times$3 kernels. This insight came from the model in [this article on Kaggle](https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist). The model achieves over 99% accuracy on the test set, beating the book's solution just barely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkg36BK18LoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLX3TzRJRL5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "08704ff2-aeb6-4ae5-d895-0caa74c8cbf0"
      },
      "source": [
        "# Downloading MNIST dataset.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28, 28, 1) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28, 28, 1) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbqQ8u42RNLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the TensorFlow graph.\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "batch_size = 50\n",
        "n_batches = len(X_train) // batch_size\n",
        "\n",
        "with graph.as_default():\n",
        "  with tf.device('/cpu:0'):\n",
        "    training = tf.placeholder_with_default(False, shape=())\n",
        "  with tf.device('/gpu:0'):\n",
        "    X = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name='X')\n",
        "    X_padded = tf.pad(X, paddings=[[0, 0], [2, 2], [2, 2], [0, 0]],\n",
        "                      name='X_padded')\n",
        "    y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
        "\n",
        "    C1 = tf.nn.relu(tf.layers.conv2d(X_padded, filters=6, kernel_size=3,\n",
        "                                     strides=[1,1], name='C1'))\n",
        "    C2 = tf.nn.relu(tf.layers.conv2d(C1, filters=6, kernel_size=3,\n",
        "                                     strides=[1,1], name='C2'))\n",
        "    S3 = tf.nn.relu(tf.nn.avg_pool(C2, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
        "                                   name='S3', padding='VALID'))\n",
        "    C4 = tf.nn.relu(tf.layers.conv2d(S3, filters=16, kernel_size=3,\n",
        "                                     strides=[1,1], name='C4'))\n",
        "    C5 = tf.nn.relu(tf.layers.conv2d(C4, filters=16, kernel_size=3,\n",
        "                                     strides=[1,1], name='C6'))\n",
        "    S6 = tf.nn.relu(tf.nn.avg_pool(C5, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
        "                                   name='S4', padding='VALID'))\n",
        "    C7 = tf.nn.relu(tf.layers.conv2d(S6, filters=120, kernel_size=3,\n",
        "                                     strides=[1,1], name='C7'))\n",
        "    C8 = tf.nn.relu(tf.layers.conv2d(C7, filters=120, kernel_size=3,\n",
        "                                     strides=[1,1], name='C8'))\n",
        "    F9 = tf.layers.dense(C8, 84, activation=tf.math.tanh, name='F6')\n",
        "    flatten = tf.squeeze(tf.squeeze(F9, axis=1), axis=1)\n",
        "    D = tf.nn.dropout(flatten, rate=0.4)\n",
        "    logits = tf.layers.dense(D, 10, name='logits')\n",
        "\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                              logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name='loss')\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "  with tf.device('/cpu:0'):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "    rnd_queue = tf.RandomShuffleQueue(capacity=len(X_train),\n",
        "                                      min_after_dequeue=0,\n",
        "                                      dtypes=[tf.float32, tf.int32],\n",
        "                                      shapes=[(28, 28, 1), ()])\n",
        "    enqueue_op = rnd_queue.enqueue_many([X, y])\n",
        "    dequeue_op = rnd_queue.dequeue_up_to(batch_size)\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOjnKrx1huCQ",
        "colab_type": "code",
        "outputId": "91048426-4589-4455-bde3-d0b6e4b3346f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        }
      },
      "source": [
        "# Training the model using early stopping.\n",
        "\n",
        "n_epochs = 100\n",
        "model_path = 'my_model.ckpt'\n",
        "max_rounds_without_improvement = 20\n",
        "\n",
        "with graph.as_default():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    rounds_since_best_loss = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "      sess.run(enqueue_op, feed_dict={X: X_train, y: y_train})\n",
        "      for _ in range(n_batches):\n",
        "        X_batch, y_batch = sess.run(dequeue_op)\n",
        "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch,\n",
        "                                         training: True})\n",
        "      loss_val = loss.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "      if loss_val < best_loss:\n",
        "        best_loss = loss_val\n",
        "        rounds_since_best_loss = 0\n",
        "        saver.save(sess, model_path)\n",
        "      else:\n",
        "        rounds_since_best_loss += 1\n",
        "      acc_val = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "      print('Epoch:', epoch+1, 'Loss:', loss_val, 'Accuracy:', acc_val)\n",
        "      if rounds_since_best_loss >= max_rounds_without_improvement:\n",
        "        print('Early stopping at epoch:', epoch+1)\n",
        "        break\n",
        "    else:\n",
        "      saver.save(sess, model_path)\n",
        "\n",
        "    saver.restore(sess, model_path)\n",
        "    train_acc_val = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
        "    test_acc_val = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "\n",
        "    print('Model training complete.')\n",
        "    print('Training epochs until convergence:', epoch+1)\n",
        "    print('Training set accuracy:', train_acc_val)\n",
        "    print('Test set accuracy:', test_acc_val)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Loss: 0.07682998 Accuracy: 1.0\n",
            "Epoch: 2 Loss: 0.052485272 Accuracy: 0.98\n",
            "Epoch: 3 Loss: 0.058004133 Accuracy: 0.96\n",
            "Epoch: 4 Loss: 0.05148828 Accuracy: 1.0\n",
            "Epoch: 5 Loss: 0.048665736 Accuracy: 1.0\n",
            "Epoch: 6 Loss: 0.038181033 Accuracy: 1.0\n",
            "Epoch: 7 Loss: 0.04755825 Accuracy: 1.0\n",
            "Epoch: 8 Loss: 0.03901503 Accuracy: 1.0\n",
            "Epoch: 9 Loss: 0.042196285 Accuracy: 0.98\n",
            "Epoch: 10 Loss: 0.044133294 Accuracy: 1.0\n",
            "Epoch: 11 Loss: 0.04028176 Accuracy: 1.0\n",
            "Epoch: 12 Loss: 0.04125908 Accuracy: 1.0\n",
            "Epoch: 13 Loss: 0.04644689 Accuracy: 1.0\n",
            "Epoch: 14 Loss: 0.03866143 Accuracy: 1.0\n",
            "Epoch: 15 Loss: 0.03517187 Accuracy: 1.0\n",
            "Epoch: 16 Loss: 0.052956782 Accuracy: 1.0\n",
            "Epoch: 17 Loss: 0.039823458 Accuracy: 1.0\n",
            "Epoch: 18 Loss: 0.038623672 Accuracy: 1.0\n",
            "Epoch: 19 Loss: 0.03389454 Accuracy: 1.0\n",
            "Epoch: 20 Loss: 0.055777185 Accuracy: 1.0\n",
            "Epoch: 21 Loss: 0.035255823 Accuracy: 1.0\n",
            "Epoch: 22 Loss: 0.038339764 Accuracy: 1.0\n",
            "Epoch: 23 Loss: 0.037628587 Accuracy: 1.0\n",
            "Epoch: 24 Loss: 0.03967332 Accuracy: 1.0\n",
            "Epoch: 25 Loss: 0.044898078 Accuracy: 1.0\n",
            "Epoch: 26 Loss: 0.04625517 Accuracy: 1.0\n",
            "Epoch: 27 Loss: 0.047377307 Accuracy: 1.0\n",
            "Epoch: 28 Loss: 0.042113226 Accuracy: 1.0\n",
            "Epoch: 29 Loss: 0.04502317 Accuracy: 1.0\n",
            "Epoch: 30 Loss: 0.056843184 Accuracy: 1.0\n",
            "Epoch: 31 Loss: 0.04129801 Accuracy: 1.0\n",
            "Epoch: 32 Loss: 0.03751177 Accuracy: 1.0\n",
            "Epoch: 33 Loss: 0.04558097 Accuracy: 1.0\n",
            "Epoch: 34 Loss: 0.049863067 Accuracy: 1.0\n",
            "Epoch: 35 Loss: 0.037521522 Accuracy: 1.0\n",
            "Epoch: 36 Loss: 0.040142067 Accuracy: 1.0\n",
            "Epoch: 37 Loss: 0.047085267 Accuracy: 1.0\n",
            "Epoch: 38 Loss: 0.04623806 Accuracy: 1.0\n",
            "Epoch: 39 Loss: 0.046935886 Accuracy: 1.0\n",
            "Early stopping at epoch: 39\n",
            "INFO:tensorflow:Restoring parameters from my_model.ckpt\n",
            "Model training complete.\n",
            "Training epochs until convergence: 39\n",
            "Training set accuracy: 0.99718183\n",
            "Test set accuracy: 0.9904\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}