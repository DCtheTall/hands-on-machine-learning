{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SupportVectorMachines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "XNntXFPp09Dk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter 5: Support Vector Machines\n",
        "\n",
        "<i>Support Vector Machines</i> are a machine learning algorithm capable of both\n",
        "linear and nonlinear classification. They are best for small- to medium-sized\n",
        "datasets.\n",
        "\n",
        "## Linear SVM Classification\n",
        "\n",
        "One useful property of SVMs is that unlike other linear classifiers, SVMs'\n",
        "decision boundary will be as far from the training instances as possible. This\n",
        "is called <i>large margin classification.</i> SVMs try to find the largest possible margin (or \"street\") that can be used as a decision boundary between classes.\n",
        "\n",
        "Adding training instances far off the street will not affect the outcome\n",
        "of training the model. Instances on the edge of the street are referred to as <i>support vectors.</i>\n",
        "\n",
        "SVMs are also sensitive to the scale of the data being used to train the model.\n",
        "\n",
        "### Soft Margin Classification\n",
        "\n",
        "<i>Hard margin classification</i> is when a model requires each instance of each class be on the same side of the street. Training models this way is only possible when the data is linearly separable and is sensitive to outliers.\n",
        "\n",
        "Another way to train the model is to aim for the widest street possible while limiting the number of <i>margin violations</i> (instances that end up on the street or on the wrong side). This method is called <i>soft margin classification</i>. In Scikit-Learn, you can tune how lenient the model is with margin violations using the $C$ hyperparameter. Large values of $C$ allow for fewer margin violations and a thinner street, whereas smaller values of $C$ allow for more margin violations and a larger street."
      ]
    },
    {
      "metadata": {
        "id": "WvgqyADo06e9",
        "colab_type": "code",
        "outputId": "9fee6b5a-b79e-4377-e352-a961b88e31bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# The following code uses Scikit-Learn to classify the iris dataset using\n",
        "# an SVM\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris['data'][:, (2, 3)]  # petal length, width\n",
        "y = (iris['target'] == 2).astype(np.float64)\n",
        "\n",
        "svm_clf = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('linear_svc', LinearSVC(C=1, loss='hinge'))\n",
        "])\n",
        "\n",
        "svm_clf.fit(X, y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "E7UOR1Sm-ULf",
        "colab_type": "code",
        "outputId": "0360c5ed-baca-42aa-9a32-0c4decf7d064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "svm_clf.predict([[5.5, 1.7]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "CsIebKSp-mgG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also uses the `SVC` class using `SVC(kernel='linear', C='1')`, but it is much slower than `LinearSVC`. You can also use the `SGDClassifier` class using `SGDClassifier(loss='hinge', alpha=1/(m*C))` which would apply Stochastic Gradient Descent to train a linear SVM classifier.\n",
        "\n",
        "## Nonlinear SVM Classification\n",
        "\n",
        "Often datasets are not linearly separable. One thing you can do is add polynomial features to the dataset to make it linearly separable. An example of this is shown below."
      ]
    },
    {
      "metadata": {
        "id": "h4rbJ8Py_9tc",
        "colab_type": "code",
        "outputId": "c9bb0d1c-7906-4aa8-8228-dbb100edd689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X, y = make_moons()\n",
        "\n",
        "poly_svm_clf = Pipeline([\n",
        "  ('poly_features', PolynomialFeatures(degree=3)),\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('svm_clf', LinearSVC(C=10, loss='hinge')),\n",
        "])\n",
        "\n",
        "poly_svm_clf.fit(X, y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "7-KtLLe0BfZA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Polynomial Kernel\n",
        "\n",
        "It can be difficult to choose the right degree of polynomial to use for transforming the data before fitting. SVMs are able train themselves to use\n",
        "polynomials without you having to add them using the <i>kernel trick</i>. An example of doing this with Scikit-Learn is below using polynomial features up to degree 3."
      ]
    },
    {
      "metadata": {
        "id": "IVPsQ21FCEwY",
        "colab_type": "code",
        "outputId": "4cf5f129-5096-455c-fef2-7640c68bca21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('svm_clf', SVC(kernel='poly', degree=3, coef0=0.1, C=5))\n",
        "])\n",
        "\n",
        "poly_kernel_svm_clf.fit(X, y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=0.1,\n",
              "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
              "  shrinking=True, tol=0.001, verbose=False))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "WPpCstxlDgYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the model is underfitting, you can increase the degree (or lower the degree if it is overfitting). You can also tune the `coef0` hyperparameter which controls how sensitive the model is to high degree polynomials versus low degree polynomials.\n",
        "\n",
        "### Adding Similarity Features\n",
        "\n",
        "Another way to solve nonlinear problems is to add features computed using a <i>similarity function</i> that measures how much each instance resembles a <i>landmark</i>. One example of a similarity function is called the Gaussian <i>Radial Basis Function</i> (RBF),\n",
        "\n",
        "$$ \\phi_\\gamma(\\mathbf{x}, \\mathbf{\\ell}) =\n",
        "\\exp\\left(-\\gamma\\,||\\mathbf{x} - \\mathbf{\\ell} ||^2\\right) $$\n",
        "\n",
        "which has a range of 0 (very far from the landmark) to 1 (at the landmark). Often one uses each instance in the training set as a landmark, then drops the original features. However, for large datasets this can greatly increase the time it takes to train a model.\n",
        "\n",
        "### Guassian RBF Kernel\n",
        "\n",
        "Scikit-Learn's `SVC` class can use a kernel trick to train a model as if it had computed all of the similarity features without the combinatorial overhead. An example implementation is below."
      ]
    },
    {
      "metadata": {
        "id": "t3pdW-5Z85PM",
        "colab_type": "code",
        "outputId": "bb149f54-f9c3-4324-dc0f-81ee30d951c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "rbf_kernel_clf = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('', SVC(kernel='rbf', C=.001, gamma=5)),\n",
        "])\n",
        "\n",
        "rbf_kernel_clf.fit(X, y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=5, kernel='rbf',\n",
              "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=False))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "c_HQTMU99Oay",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the hyerperameter parameter `gamma` influences the shape of the bell curve. See the mathematical definition for the Gaussian RBF above.\n",
        "\n",
        "### Computational Complexity\n",
        "\n",
        "The `LinearSVC` class uses a library called [`liblinear`](https://www.csie.ntu.edu.tw/~cjlin/liblinear/), which uses an [optimized algorithm](https://www.csie.ntu.edu.tw/~cjlin/papers/cddual.pdf) for training linear kernels. It does not support the kernel trick. The computational complexity is $O(n \\times m)$ where $n$ is the dimension of the vectors in the training set and $m$ is the size of the training set.\n",
        "\n",
        "The `SVC` class uses the [`libsvm`](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) which implements an [algorithm](https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F69644%2Ftr-98-14.pdf) which supports the kernel trick. The training time complexity of training an algorithm is generally in the range of $O(m^2 \\times n)$ to $O(m^3 \\times n)$, making it very slow when the training set has a large size. This makes nonlinear SVM classifiers best for small to medium size, complex training sets.\n",
        "\n",
        "## SVM Regression\n",
        "\n",
        "SVMs also support regression by reversing the objective. It tries to create streets which intersects the most training instances. Scikit-Learn's `LinearSVR` class implements this regression, and uses the hyperparameter $\\epsilon$ to determine the width of the margin, the larger $\\epsilon$, the larger the margin used when determining the optimal \"street\" to train the model. Below is an example of using Scikit-Learn's `LinearSVR` class."
      ]
    },
    {
      "metadata": {
        "id": "RFO8e5Ko_Et8",
        "colab_type": "code",
        "outputId": "9eed58f6-68c0-4aad-8036-9b336dc20a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "svm_reg = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('svr', LinearSVR(epsilon=1.5)),\n",
        "])\n",
        "svm_reg.fit(X, y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svr', LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
              "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
              "     random_state=None, tol=0.0001, verbose=0))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "QQTzkyu6T7a-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For nonlinear regression, the `SVR` class can fit the data with a polynomial function. It can be tuned with the $\\epsilon$ hyperparameter, also a regularization parameter $C$, and you can set the maximum degree the algorithm will use. Below is an example of Scikit-Learn's SVR Class."
      ]
    },
    {
      "metadata": {
        "id": "LPsnmEH9VI8x",
        "colab_type": "code",
        "outputId": "69501c4f-f94a-468e-9443-c47520f89019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_poly_reg = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('svr', SVR(kernel='poly', degree=2, C=100, epsilon=0.1)),\n",
        "])\n",
        "svm_poly_reg.fit(X, y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svr', SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1,\n",
              "  gamma='auto_deprecated', kernel='poly', max_iter=-1, shrinking=True,\n",
              "  tol=0.001, verbose=False))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "BI3s0xf3Zv6G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Under the Hood\n",
        "\n",
        "Below is notes on the mathematics behind the implementation of SVMs. Unlike chapter 4, which used the vector $\\theta$ as the values being computed by the training algorithm, in this section we will use $b$ to denote the bias term and $\\mathbf{w}$ as the weights vector.\n",
        "\n",
        "### Decision Function and Predictions\n",
        "\n",
        "The linear SVM classifier predicts the class of an instance by first computing the decision function\n",
        "\n",
        "$$ \\mathbf{w}^T \\cdot \\mathbf{x} + b\n",
        "= \\left( \\sum\\limits_{i = 1}^n w_nx_n \\right) + b. $$\n",
        "\n",
        "The classifier then makes the decision based on the following criteria\n",
        "\n",
        "$$ \\hat{y} = \\left\\{ \\begin{matrix}\n",
        "0 && \\text{if} \\;\\; \\mathbf{w}^T \\cdot \\mathbf{x} + b < 0 \\\\\n",
        "1 && \\text{if} \\;\\; \\mathbf{w}^T \\cdot \\mathbf{x} + b \\geq 0\n",
        "\\end{matrix} \\right. $$\n",
        "\n",
        "The decision boundary is the set of all points in the training set space where the decision function is equal to 0. The edges of the margin are where the decision function is equal to $\\pm1$.\n",
        "\n",
        "### Training Objective\n",
        "\n",
        "The slope of the decision function is equal to the norm of the weight vector, $||\\mathbf{w}||$. This means that the width of the margin is inversely proportional to the norm of $\\mathbf{w}$. If we want to avoid any margin violations, the decision function must be greater than 1 for positive instances and less than -1 for negative instances. This constraint can be expressed as\n",
        "\n",
        "$$ t^{(i)}\\left( \\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b \\right) \\geq 1 $$\n",
        "\n",
        "where $t^{(i)}$ is -1 for all negative instances and 1 for all positive instances. Thus the hard margin linear SVM classifier objective is the <i>constrainted optimization</i> below\n",
        "\n",
        "$$ \\underset{\\mathbf{w},\\;b}{\\text{minimize}} \\;\\; \\frac{1}{2}\n",
        "\\mathbf{w}^T \\cdot \\mathbf{w} $$\n",
        "\n",
        "subject to the constraint that\n",
        "\n",
        "$$ t^{(i)} \\left( \\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b \\right) \\geq 1. $$\n",
        "\n",
        "To get the soft margin objective, wee need to introduce a <i>slack variable</i>, $\\zeta^{(i)} \\geq 0$ for each training instance. Each $\\zeta^{(i)}$ measures how much the $i^\\text{th}$ instance is allowed to violate the margin. Therefore for soft margin classification, we have two problems: minimize the slack variables as well as $\\frac{1}{2}\\mathbf{w}^T\\cdot\\mathbf{x}$. The $C$ hyperparameter allows us to define the trade-off between the two objectives. The constrained optimization problem for soft margin classification is given by\n",
        "\n",
        "$$ \\underset{\\mathbf{w},\\;b,\\;\\zeta}{\\text{minimize}} \\;\\;\n",
        "\\frac{1}{2}\\mathbf{w}^T\\cdot\\mathbf{w} \\, + \\,\n",
        "C\\sum\\limits_{i=1}^m \\zeta^{(i)} $$\n",
        "\n",
        "subject to the constraint that\n",
        "\n",
        "$$ t^{(i)} \\left( \\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b \\right)\n",
        "\\geq 1 - \\zeta^{(i)} \\;\\; \\text{and} \\;\\; \\zeta^{(i)} \\geq 0 \\;\\; \n",
        "\\text{for} \\;\\; i = 1, 2, ..., m. $$\n",
        "\n",
        "### Quadratic Programming\n",
        "\n",
        "The hard and soft margin classification problems belong to a class of problems known as [<i>Quadratic Programming</i>](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf) (QP) problems. The general problem formulation for QP problems is given by\n",
        "\n",
        "$$ \\underset{\\mathbf{p}}{\\text{minimize}} \\;\\;\n",
        "\\frac{1}{2} \\mathbf{p}^T \\cdot \\mathbf{H} \\cdot \\mathbf{p} +\n",
        "\\mathbf{f}^T \\cdot \\mathbf{p} $$\n",
        "\n",
        "subject to\n",
        "\n",
        "$$ \\mathbf{A} \\cdot \\mathbf{p} \\leq \\mathbf{b} $$\n",
        "\n",
        "where\n",
        "\n",
        "- $\\mathbf{p}$ is an $n_p$-dimensional vector ($n_p$ is the number of parameters)\n",
        "- $\\mathbf{H}$ is an $n_p \\times n_p$ matrix\n",
        "- $\\mathbf{f}$ is an $n_p$-dimensional vector\n",
        "- $\\mathbf{A}$ is an $n_c \\times n_p$ matrix ($n_c$ is the number of constraints)\n",
        "- $\\mathbf{b}$ is an $n_c$-dimensional vector\n",
        "\n",
        "QP problems can be solved using off-the-shelf solvers which are available in different code libraries.\n",
        "\n",
        "### The Dual Problem\n",
        "\n",
        "Given a constrained optimization problem, i.e. the <i>primal problem</i>, there is a different but related problem known as its <i>dual problem</i>. In the special case that the objective function is convex, and the inequality constraint is both convex and continuously differentiable (such as SVMs), the solution to the dual problem is also a solution to the primal problem. The dual form of the SVM problem is given by\n",
        "\n",
        "$$ \\underset{\\alpha}{\\text{minimize}} \\;\\;\n",
        "\\frac{1}{2} \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^m\n",
        "\\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} \\mathbf{x}^{(i)\\,T}\n",
        "\\cdot \\mathbf{x}^{(j)} \\; - \\; \\sum\\limits_{i=1}^m \\alpha^{(i)} $$\n",
        "\n",
        "subject to the constraint that\n",
        "\n",
        "$$ \\alpha^{(i)} \\geq 0 \\;\\; \\text{for} \\;\\; i=1,2,...,m. $$\n",
        "\n",
        "Once you find the $\\hat{\\alpha}$ which minimizes the objective function, you can compute $\\hat{\\mathbf{w}}$ and $\\hat{b}$ that minimize the primal problem using\n",
        "\n",
        "$$ \\hat{\\mathbf{w}} = \\sum\\limits_{i=1}^m \\hat{\\alpha}^{(i)} t^{(i)}\n",
        "\\mathbf{x}^{(i)} \\\\\n",
        "\\hat{b} = \\frac{1}{n_s} \\underset{\\hat{\\alpha}^{(i)} > 0}{\\sum\\limits_{i=1}^m}\n",
        "\\left( t^{(i)} - \\hat{\\mathbf{w}}^T \\cdot \\mathbf{x}^{(i)} \\right). $$\n",
        "\n",
        "The dual problem is generally faster to solve than the primal one, and it also it is possible to use the kernel trick on the dual problem.\n",
        "\n",
        "### Kernelized SVM\n",
        "\n",
        "Suppose you want to apply a $2^\\text{nd}$-degree polynomial transformation to a two-dimensional dataset using a function, $\\phi$, the transformation would be given by\n",
        "\n",
        "$$ \\phi(\\mathbf{x}) = \\phi\\left(\\left[ \\begin{matrix}\n",
        "x_1 \\\\ x_2\n",
        "\\end{matrix} \\right]\\right) =\n",
        "\\left[ \\begin{matrix}\n",
        "x_1^{\\;2} \\\\ \\sqrt{2} \\, x_1x_2 \\\\ x_2^{\\;2}\n",
        "\\end{matrix} \\right]. $$\n",
        "\n",
        "If you were to take the dot product after transforming two vectors, $\\mathbf{a}$ and $\\mathbf{b}$, you will find that\n",
        "\n",
        "$$ \\phi(\\mathbf{a})^T \\cdot \\phi(\\mathbf{b}) =\n",
        "\\left[ \\begin{matrix} a_1^{\\;2} \\\\ \\sqrt{2}\\,a_1a_2 \\\\ a_2^{\\;2} \\end{matrix}\n",
        "\\right]^{\\;T}\n",
        "\\cdot \\left[ \\begin{matrix}\n",
        "b_1^{\\;2} \\\\ \\sqrt{2}\\,b_1b_2 \\\\ b_2^{\\;2}\n",
        "\\end{matrix} \\right] = a_1^{\\;2}b_1^{\\;2} + 2 \\, a_1b_1a_2b_2 +\n",
        "a_2^{\\;2}b_2^{\\;2} = \\left( \\mathbf{a}^T \\cdot \\mathbf{b} \\right)^2. $$\n",
        "\n",
        "This means that we can apply the $2^\\text{nd}$ degree polynomial transformation of the dot product in the dual SVM problem by simply squaring the quantity $\\mathbf{x}^T \\cdot \\mathbf{x}$. This is how the polynomial SVM kernel is able to apply the polynomial transformation during training with much more computational efficiency.\n",
        "\n",
        "The function $K(\\mathbf{a}, \\mathbf{b}) = \\left( \\mathbf{a}^T \\cdot \\mathbf{b} \\right)^2$ is called the $2^\\text{nd}$-degree <i>polynomial kernel</i> where a <i>kernel</i> is a function capable of computing the quantity\n",
        "\n",
        "$$ \\phi(\\mathbf{a})^T \\cdot \\phi(\\mathbf{b}) $$\n",
        "\n",
        "without needing to compute either $\\phi(\\mathbf{a})$ or $\\phi(\\mathbf{b})$. Some of the most commonly used kernels are\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "\\text{Linear:} && K(\\mathbf{a},\\mathbf{b}) = \\mathbf{a}^T \\cdot \\mathbf{b} \\\\\n",
        "\\text{Polynomial:} && K(\\mathbf{a},\\mathbf{b}) =\n",
        "\\left( \\gamma\\,\\mathbf{a}^T \\cdot \\mathbf{b} + r \\right)^d \\\\\n",
        "\\text{Gaussian RBF:} && K(\\mathbf{a},\\mathbf{b}) =\n",
        "\\exp\\left( -\\gamma || \\mathbf{a} - \\mathbf{b} ||^2 \\right) \\\\\n",
        "\\text{Sigmoid:} && K(\\mathbf{a},\\mathbf{b}) =\n",
        "\\tanh\\left( \\gamma\\,\\mathbf{a}^T \\cdot \\mathbf{b} + r \\right)\n",
        "\\end{matrix} $$\n",
        "\n",
        "According to <i>Mercer's theorem</i>, if a function $K(\\mathbf{a},\\mathbf{b})$ meets a set of conditions called <i>Mercer's conditions</i>, then there exists a function $\\phi$ that maps $\\mathbf{a}$ and $\\mathbf{b}$ into another space (possibly with a higher dimension) such that $K(\\mathbf{a},\\mathbf{b}) = \\phi(\\mathbf{a})^T \\cdot \\phi(\\mathbf{b})$, so you can use $K$ as a kernel even if you do not know what $\\phi$ is. Some kernels (e.g. the Sigmoid kernel) do not follow all of Mercer's condition but still work in practice.\n",
        "\n",
        "The caveat is that the formula which determines the weights vector and bias term from the solution to the dual problem require that you compute $\\phi\\left(\\mathbf{x}^{(i)}\\right)$ and that $\\hat{\\mathbf{w}}$ has the same dimension as $\\phi\\left(\\mathbf{x}^{(i)}\\right)$ which can be very large or even infinite. However, it is possible to express the decision function using the kernel:\n",
        "\n",
        "$$ h_{\\hat{\\mathbf{w}},\\,\\hat{b}} \\left( \\phi\\left( \\mathbf{x}^{(n)} \\right) \\right) = \\hat{\\mathbf{w}}^T \\cdot \\phi\\left( \\mathbf{x}^{(n)} \\right) + \\hat{b} = \\left( \\sum\\limits_{i=1}^m \\hat{\\alpha}^{(i)} t^{(i)} \\phi\\left( \\mathbf{x}^{(i)} \\right) \\right)^T \\cdot \\phi\\left( \\mathbf{x}^{(n)} \\right) + \\hat{b} \\\\\n",
        "= \\sum\\limits_{i=1}^m \\hat{\\alpha}^{(i)} t^{(i)} \\left( \\phi\\left( \\mathbf{x}^{(i)} \\right)^T \\cdot \\phi\\left( \\mathbf{x}^{(n)} \\right) \\right) + \\hat{b} \\\\\n",
        "= \\underset{\\hat{\\alpha}^{(i)} > 0}{\\sum\\limits_{i=1}^m} \\hat{\\alpha}^{(i)} t^{(i)} K\\left( \\mathbf{x}^{(i)}, \\mathbf{x}^{(n)} \\right) + \\hat{b}. $$\n",
        "\n",
        "The terms where $\\hat{\\alpha}^{(i)}$ are greater than 0 are the <i>support vectors</i>, influencing their position can influence the decision boundary. This formula shows that in order to get a prediction, we need only compute the kernel for the training instances which are support vectors. The bias term $\\hat{b}$ is given by the formula below\n",
        "\n",
        "$$ \\hat{b} = \\frac{1}{n_s} \\sum\\limits_{i=1}^m \\left( 1 - t^{(i)}\\hat{\\mathbf{w}}^T \\cdot \\phi\\left( \\mathbf{x}^{(i)} \\right) \\right) =\n",
        "\\frac{1}{n_s} \\sum\\limits_{i=1}^m \\left(\n",
        "1 - t^{(i)} \\left( \\sum\\limits_{j=1}^m \\hat{\\alpha}^{(j)} t^{(j)} \\phi\\left( \\mathbf{x}^{(j)} \\right) \\right)^T \\cdot \\phi\\left( \\mathbf{x}^{(i)} \\right)\n",
        "\\right) \\\\\n",
        "= \\frac{1}{n_s} \\underset{\\hat{\\alpha}^{(i)} > 0}{\\sum\\limits_{i=1}^m}\n",
        "\\left( 1 - t^{(i)} \\underset{\\hat{\\alpha}^{(j)} > 0}{\\sum\\limits_{j=1}^m}\n",
        "\\hat{\\alpha}^{(j)}t^{(j)}K\\left( \\mathbf{x}^{(i)}, \\mathbf{x}^{(j)} \\right)\n",
        "\\right) $$\n",
        "\n",
        "again, the model needs only to compute the kernel for the support vectors, and not every training instance, saving a lot of computational effort.\n",
        "\n",
        "### Online SVMs\n",
        "\n",
        "One way to have an online linear SVM is to have it use gradient descent with the following cost function\n",
        "\n",
        "$$ J(\\mathbf{w}, b) = \\frac{1}{2} \\, \\mathbf{w}^T \\cdot \\mathbf{w} \\; + \\;\n",
        "C \\sum\\limits_{i=1}^n max\\left(0, 1 - t^{(i)} \\left( \\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b \\right) \\right)\n",
        "$$\n",
        "\n",
        "the first term in the sum keeps the norm of $\\mathbf{w}$ minimized, the second term computes the total number of margin violations. The downside is that this method of training converges much more slowly than use a QP solver to find the solution to the dual problem.\n",
        "\n",
        "There exists online kernelized SVMs, for example using [\"Incremental and Decremental SVM Learning\"](http://isn.ucsd.edu/papers/nips00_inc.pdf) or [\"Fast Kernel Classifiers with Online and Active Learning,\"](http://www.jmlr.org/papers/volume6/bordes05a/bordes05a.pdf) but it is probably better to use neural networks for online nonlinear problems.\n",
        "\n",
        "### Hinge Loss\n",
        "\n",
        "The function $max(0, 1 - t)$ is called the <i>hinge loss</i> function. The derivative is equal to -1 when $t$ is in $(-\\infty,1)$, the function is not differentiable when $t=1$ and is 0 elsewhere. Even though it is not differentiable at $t=1$, you can use the [<i>subderivative</i>](https://en.wikipedia.org/wiki/Subderivative) to use this function in Gradient Descent (like using the subgradient for Lasso regression).\n",
        "\n",
        "## Exercises\n",
        "\n",
        "### 1. What is the fundamental idea behind Support Vector Machines?\n",
        "\n",
        "SVM classifiers try to find a decision boundary with a margin where there are as few training instances in the margin around the decision boundary. SVM regressors, on the other hand, try to find margins around the decision function which has as many training instances contained in them.\n",
        "\n",
        "### 2. What is a support vector?\n",
        "\n",
        "A support vector is a vector that is close to the decision boundary. Changing the features of these vectors slightly is likely to change the resulting decision boundary.\n",
        "\n",
        "### 3. Why is it important to scale the inputs when using SVMs?\n",
        "\n",
        "Scaling the feature set prevents one particular feature's value overpowering and information that may also be in features with a smaller value.\n",
        "\n",
        "### 4. Could an SVM output a confidence score when it classifies an instance? What about a probability?\n",
        "\n",
        "You could use the distance from the function's decision boundary as a measure of confidence in the model's decision. It will be less confident when a new instance is close to the decision boundary, so the function should be monotonically increasing as a function of the sample's distance from the decision boundary.\n",
        "\n",
        "You could also use a logistic curve which is a function of the new instance's distance from the deicision boundary which asymptotically approaches 0 on one side and 1 on the other. This function would give you the probability that the model belongs to a particular class.\n",
        "\n",
        "### 5. Should you use the primal or dual form of the SVM problem to train a model on a training set with millions of instances with hundreds of features?\n",
        "\n",
        "You should use the primal problem and train the model with a QP solver. You will need to transform nonlinear data since the primal problem is not capable of the kernel trick. The primal problem is faster when the number of training instances is largely greater than the number of features.\n",
        "\n",
        "### 6. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease $\\gamma$?\n",
        "\n",
        "You should decrease the $\\gamma$ hyperparameter. Doing so will increase the influence the instances in the training set has over the decision boundary.\n",
        "\n",
        "### 7. How should you set the QP parameters, $( \\mathbf{H}, \\mathbf{f}, \\mathbf{A},$ and $\\mathbf{b})$ to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?\n",
        "\n",
        "It was stated above that the general formula for a QP problem is given by\n",
        "\n",
        "$$ \\underset{\\mathbf{p}}{\\text{minimize}} \\;\\;\n",
        "\\frac{1}{2} \\mathbf{p}^T \\cdot \\mathbf{H} \\cdot \\mathbf{p} +\n",
        "\\mathbf{f}^T \\cdot \\mathbf{p} $$\n",
        "\n",
        "and is subjected to the constraint that\n",
        "\n",
        "$$ \\mathbf{A} \\cdot \\mathbf{p} \\leq \\mathbf{b} $$\n",
        "\n",
        "Also, it was stated that dual form of the linear soft margin SVM problem is given by\n",
        "\n",
        "$$ \\underset{\\alpha}{\\text{minimize}} \\;\\;\n",
        "\\frac{1}{2} \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^m\n",
        "\\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} \\mathbf{x}^{(i)\\,T}\n",
        "\\cdot \\mathbf{x}^{(j)} \\; - \\; \\sum\\limits_{i=1}^m \\alpha^{(i)} $$\n",
        "\n",
        "and is subject to the constraint that\n",
        "\n",
        "$$ \\alpha^{(i)} \\geq 0 \\;\\; \\text{for} \\;\\; i=1,2,...,m. $$\n",
        "\n",
        "It follows that the paramters that should be used for an off-the-shelf QP solver, substitutiong $\\alpha$ for the term $\\mathbf{p}$, the should be:\n",
        "\n",
        "$$ \\mathbf{H} = ||h_{ij}|| \\;\\;\n",
        "\\text{for} \\;\\; i,j = 1, 2, ..., m $$\n",
        "\n",
        "where each term in the matrix $h_{ij}$ is given by\n",
        "\n",
        "$$ h_{ij} = t^{(i)}t^{(j)} (\\mathbf{x}^{(i)\\,T} \\cdot \\mathbf{x}^{(j)}). $$\n",
        "\n",
        "The term $\\mathbf{f}$ should just be an $m$-dimensional vector where each component is -1. Finally, the term $\\mathbf{A}$ should simply be the $m\\times m$ identity matrix and the term $\\mathbf{b}$ should just be the $m$-dimensional zero vector.\n",
        "\n",
        "### 8. Train a `LinearSVC` classifier on a linearly separable dataset. Then train an `SGDClassifier` on the same dataset. See if you can get them to produce roughly the same model.\n",
        "\n",
        "For this exercise I will use the Iris dataset tot try to find the species of the flower using petal width and petal length. First, we download the data."
      ]
    },
    {
      "metadata": {
        "id": "L5CgPiPmRPL2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2,3)]\n",
        "y = iris.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkQfU48-VO0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we see that using the `LinearSVC` with the default settings and we see it is able to predict the species of the flower fairly well."
      ]
    },
    {
      "metadata": {
        "id": "VKf72QaLwro0",
        "colab_type": "code",
        "outputId": "338d1e77-a05b-4dac-a276-28b52dd41fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "svm_clf = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('svm', LinearSVC())\n",
        "])\n",
        "cross_val_score(svm_clf, X_train, y_train, cv=5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.95833333, 0.86956522, 0.95652174, 1.        , 0.95238095])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "PKc1t0GpyikR",
        "colab_type": "code",
        "outputId": "f45c4d4c-3dcc-4ff2-d45d-f5844f106114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "svm_clf.fit(X_train, y_train)\n",
        "svm_clf.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "DQdeTLpr0RgH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we train an `SGDClassifier` on the same dataset."
      ]
    },
    {
      "metadata": {
        "id": "4FHfJYpTyr-W",
        "colab_type": "code",
        "outputId": "0d71d4a5-0f32-41ea-979e-a57650fea62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('sgd', SGDClassifier(alpha=0.01)),\n",
        "])\n",
        "cross_val_score(sgd_clf, X_train, y_train, cv=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.79166667, 1.        , 0.95652174, 0.9047619 , 0.9047619 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "p7P_w9b30XBZ",
        "colab_type": "code",
        "outputId": "2d418e4b-6cff-44de-e8ea-a75ac0620a7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "sgd_clf.fit(X_train, y_train)\n",
        "sgd_clf.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "fPcbPyyf04tL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we compare how each model would make predictions on the test set and we see that the models coincide exactly."
      ]
    },
    {
      "metadata": {
        "id": "9brs2tJb0_46",
        "colab_type": "code",
        "outputId": "64de2c57-f6e9-4133-f65b-39618f45c856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "sgd_clf.score(X_test, svm_clf.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "C0Og-NxLjn7X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 9"
      ]
    },
    {
      "metadata": {
        "id": "n6v5bpgdjehk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Some code to send me an email when the model finishes training.\n",
        "\n",
        "import smtplib\n",
        "\n",
        "def notify(msg):\n",
        "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "    server.starttls()\n",
        "    server.login(\"\", \"\")\n",
        "    server.sendmail(\n",
        "        \"\",\n",
        "        \"\", \n",
        "        'SUBJECT: Colab\\n\\n' + str(msg))\n",
        "    server.quit()\n",
        "    return msg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Drp7lZwWr6sG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fetching the MNIST dataset."
      ]
    },
    {
      "metadata": {
        "id": "LPJ_LwZjwbDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fetching the dataset.\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "mnist.target = mnist.target.astype(np.int8)\n",
        "sorted_indices = np.argsort(mnist.target)\n",
        "X = mnist.data[sorted_indices]\n",
        "y = mnist.target[sorted_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E_PIDLTjsRo-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "outputId": "ece4aa88-48a3-44d8-8ab1-489917922fef"
      },
      "cell_type": "code",
      "source": [
        "# Training a model using grid search\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "rand_idx = np.random.permutation(len(X))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[rand_idx[0:20]], y[rand_idx[0:20]])\n",
        "param_grid = {\n",
        "  'svm__kernel': ('linear', 'poly', 'rbf'),\n",
        "  # 'svm__C': (10, 1, .1),\n",
        "}\n",
        "pipeline = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('svm', SVC()),\n",
        "])\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=2).fit(X_train, y_train)\n",
        "clf = grid_search.best_estimator_\n",
        "notify('Best score: {:.4f}\\n\\nBest Params: {}\\n\\nBest Estimator: {}' + \\\n",
        "  '\\n\\nTest Score: {:.4f}'.format(\n",
        "    grid_search.best_score_, grid_search.best_params_,\n",
        "    clf, clf.score(X_test, y_test)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SMTPAuthenticationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSMTPAuthenticationError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-069b2ea13fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m notify('Best score: {:.4f}\\n\\nBest Params: {}\\n\\nBest Estimator: {}' +   '\\n\\nTest Score: {:.4f}'.format(\n\u001b[1;32m     18\u001b[0m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     clf, clf.score(X_test, y_test)))\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-b78af0b8f790>\u001b[0m in \u001b[0;36mnotify\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmtplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSMTP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'smtp.gmail.com'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m587\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarttls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dylan.james.cutler@gmail.com\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"C73.B47!C5A?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     server.sendmail(\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"dylan.james.cutler@gmail.com\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/smtplib.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(self, user, password, initial_response_ok)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;31m# We could not login successfully.  Return result of last attempt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mlast_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarttls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcertfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/smtplib.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(self, user, password, initial_response_ok)\u001b[0m\n\u001b[1;32m    719\u001b[0m                 (code, resp) = self.auth(\n\u001b[1;32m    720\u001b[0m                     \u001b[0mauthmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                     initial_response_ok=initial_response_ok)\n\u001b[0m\u001b[1;32m    722\u001b[0m                 \u001b[0;31m# 235 == 'Authentication successful'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# 503 == 'Error: already authenticated'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/smtplib.py\u001b[0m in \u001b[0;36mauth\u001b[0;34m(self, mechanism, authobject, initial_response_ok)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m235\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m503\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mSMTPAuthenticationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauth_cram_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchallenge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSMTPAuthenticationError\u001b[0m: (534, b'5.7.14 <https://accounts.google.com/signin/continue?sarp=1&scc=1&plt=AKgnsbsx\\n5.7.14 F29IDBMUvCrIppRrV0C8htrZC6lqBrCwJWHBxNZvXbrOvCjb6R0Gd1leHLuToZVZC2AT5v\\n5.7.14 jvNuwU4BCSrLgXxK25Su3JANFg9bvYHM7y9kl4XD78q83kCQ1BUZtF1ss7zsWw5lPmXOGl\\n5.7.14 FGb7rlxox2RrOL-3erkOJLIoGrZwG9a1DdldHBXa8f-_Br1vTpHohUJN> Please log\\n5.7.14 in via your web browser and then try again.\\n5.7.14  Learn more at\\n5.7.14  https://support.google.com/mail/answer/78754 l1sm34181562pgp.9 - gsmtp')"
          ]
        }
      ]
    }
  ]
}