{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SupportVectorMachines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "XNntXFPp09Dk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter 5: Support Vector Machines\n",
        "\n",
        "<i>Support Vector Machines</i> are a machine learning algorithm capable of both\n",
        "linear and nonlinear classification. They are best for small- to medium-sized\n",
        "datasets.\n",
        "\n",
        "## Linear SVM Classification\n",
        "\n",
        "One useful property of SVMs is that unlike other linear classifiers, SVMs'\n",
        "decision boundary will be as far from the training instances as possible. This\n",
        "is called <i>large margin classification.</i> SVMs try to find the largest possible margin (or \"street\") that can be used as a decision boundary between classes.\n",
        "\n",
        "Adding training instances far off the street will not affect the outcome\n",
        "of training the model. Instances on the edge of the street are referred to as <i>support vectors.</i>\n",
        "\n",
        "SVMs are also sensitive to the scale of the data being used to train the model.\n",
        "\n",
        "### Soft Margin Classification\n",
        "\n",
        "<i>Hard margin classification</i> is when a model requires each instance of each class be on the same side of the street. Training models this way is only possible when the data is linearly separable and is sensitive to outliers.\n",
        "\n",
        "Another way to train the model is to aim for the widest street possible while limiting the number of <i>margin violations</i> (instances that end up on the street or on the wrong side). This method is called <i>soft margin classification</i>. In Scikit-Learn, you can tune how lenient the model is with margin violations using the $C$ hyperparameter. Large values of $C$ allow for fewer margin violations and a thinner street, whereas smaller values of $C$ allow for more margin violations and a larger street."
      ]
    },
    {
      "metadata": {
        "id": "WvgqyADo06e9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "b6b83cd7-c8d5-46de-9b58-9bc631ac6bda"
      },
      "cell_type": "code",
      "source": [
        "# The following code uses Scikit-Learn to classify the iris dataset using\n",
        "# an SVM\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris['data'][:, (2, 3)]  # petal length, width\n",
        "y = (iris['target'] == 2).astype(np.float64)\n",
        "\n",
        "svm_clf = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('linear_svc', LinearSVC(C=1, loss='hinge'))\n",
        "])\n",
        "\n",
        "svm_clf.fit(X, y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "E7UOR1Sm-ULf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b1794ea-c76d-40ee-b510-2904e8aa15f1"
      },
      "cell_type": "code",
      "source": [
        "svm_clf.predict([[5.5, 1.7]])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "CsIebKSp-mgG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also uses the `SVC` class using `SVC(kernel='linear', C='1')`, but it is much slower than `LinearSVC`. You can also use the `SGDClassifier` class using `SGDClassifier(loss='hinge', alpha=1/(m*C))` which would apply Stochastic Gradient Descent to train a linear SVM classifier.\n",
        "\n",
        "## Nonlinear SVM Classification\n",
        "\n",
        "Often datasets are not linearly separable. One thing you can do is add polynomial features to the dataset to make it linearly separable. An example of this is shown below."
      ]
    },
    {
      "metadata": {
        "id": "h4rbJ8Py_9tc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "8dd197e2-c715-4e4f-f9f0-e96aa691f9ee"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X, y = make_moons()\n",
        "\n",
        "poly_svm_clf = Pipeline([\n",
        "  ('poly_features', PolynomialFeatures(degree=3)),\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('svm_clf', LinearSVC(C=10, loss='hinge')),\n",
        "])\n",
        "\n",
        "poly_svm_clf.fit(X, y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "7-KtLLe0BfZA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Polynomial Kernel\n",
        "\n",
        "It can be difficult to choose the right degree of polynomial to use for transforming the data before fitting. SVMs are able train themselves to use\n",
        "polynomials without you having to add them using the <i>kernel trick</i>. An example of doing this with Scikit-Learn is below using polynomial features up to degree 3."
      ]
    },
    {
      "metadata": {
        "id": "IVPsQ21FCEwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b8d7dc67-61bd-45f9-b8c6-4272cca40cf3"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('svm_clf', SVC(kernel='poly', degree=3, coef0=0.1, C=5))\n",
        "])\n",
        "\n",
        "poly_kernel_svm_clf.fit(X, y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=0.1,\n",
              "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
              "  shrinking=True, tol=0.001, verbose=False))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "WPpCstxlDgYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the model is underfitting, you can increase the degree (or lower the degree if it is overfitting). You can also tune the `coef0` hyperparameter which controls how sensitive the model is to high degree polynomials versus low degree polynomials.\n",
        "\n",
        "### Adding Similarity Features\n",
        "\n",
        "Another way to solve nonlinear problems is to add features computed using a <i>similarity function</i> that measures how much each instance resembles a <i>landmark</i>. One example of a similarity function is called the Gaussian <i>Radial Basis Function</i>,\n",
        "\n",
        "$$ \\phi_\\gamma(\\mathbf{x}, \\mathbf{\\ell}) =\n",
        "\\exp\\left(-\\gamma\\,||\\mathbf{x} - \\mathbf{\\ell} ||^2\\right) $$\n",
        "\n",
        "which has a range of 0 (very far from the landmark) to 1 (at the landmark). Often one uses each instance in the training set as a landmark, then drops the original features. However, for large datasets this can greatly increase the time it takes to train a model."
      ]
    }
  ]
}