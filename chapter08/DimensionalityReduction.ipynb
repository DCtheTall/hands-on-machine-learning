{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DimensionalityReduction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mg_y0SZpgupW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter 8: Dimensionality Reduction\n",
        "\n",
        "Training sets that are composed of instances with a large number of features (i.e. vectors of high dimension) take a long time to train. This is often referred to as the <i>curse of dimensionality</i>. In addition to speeding up training, reducing the dimension of the data also helps with visualizations, since it is not possible to create visual representations of high dimensional data.\n",
        "\n",
        "## The Curse of Dimensionality\n",
        "\n",
        "High dimensional datasets are both more likely to have instances which have certain features with extreme values. Also, points in higher dimensions are more sparse than points in lower dimensions. Both of these factors make training models with high dimensional datasets more difficult.\n",
        "\n",
        "## Main Approaches for Dimensionality Reduction\n",
        "\n",
        "## Projection\n",
        "\n",
        "Often times, all of the instances in a training set have roughly uniform values in some dimensions and vary more in others. These instances can be treated as if they resided in a lower-dimensional subspace of the original, high-dimensional space. Projecting the training instances onto a lower dimensional subspace can be an effective way of reducing the dimension of the training set.\n",
        "\n",
        "## Manifold Learning\n",
        "\n",
        "A $d$-dimensional manifold is a part of an $n$-dimensional space (where $d < n$) that locally resembles a $d$-dimensional hyperplane. Dimensionality reduction algorithms which work by modeling the manifold on which the training set lies are called <i>Manifold Learning</i> algorithms. They rely on the <i>manifold assumption</i> (or <i>manifold hypothesis</i>) which states that most real-world high-dimensional datasets lie close to a lower-dimensional manifold.\n",
        "\n",
        "In addition to the manifold assumption, there is an implicit assumption that the decision boundary will be simpler in the lower-dimensional manifold. Whether or not this assumption holds varies by dataset and choice of manifold.\n",
        "\n",
        "## PCA\n",
        "\n",
        "<i>Principal Component Analysis</i> is the most popular dimensionality reduction algorithm. It tries to find the manifold that the data lies in, then projects the data onto the manifold.\n",
        "\n",
        "### Preserving Variance\n",
        "\n",
        "One of the goals of PCA is to select the axis where the dataset has the most variance so that you lose as little information as possible. One way to do this is to minimize the mean squared distance from the original dataset to the projected dataset.\n",
        "\n",
        "### Principal Components\n",
        "\n",
        "Once PCA finds an axis which preserves the variance, it then continues to find a 2<sup>nd</sup> axis orthogonal to the first which also preserves the variance of the original dataset. PCA continues to do so until it finds a set of basis vectors which span the entire space of the original dataset.\n",
        "\n",
        "One can use a matrix factorization technique called <i>Singular Value Decomposition</i> (SVD) to find the principal components of a training set, $\\mathbf{X}$ by decomposing $\\mathbf{X}$ into the product of three matrices: $\\mathbf{U}$, $\\Sigma$, and $\\mathbf{V}^{\\,T}$ where $\\mathbf{V}$ is the matrix whose columns are the principal components, i.e.\n",
        "\n",
        "$$ V = \\left( \\mathbf{c}_1, \\mathbf{c}_2, ..., \\mathbf{c}_n \\right) $$\n",
        "\n",
        "where $\\mathbf{c}_i$ are the principal components of the dataset. The following code implements SVD using `numpy`."
      ]
    },
    {
      "metadata": {
        "id": "8OyebCRf3dRH",
        "colab_type": "code",
        "outputId": "d37bccbd-fd73-43a0-db41-4465d5084918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_swiss_roll(n_samples=1000)\n",
        "\n",
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, Vt = np.linalg.svd(X_centered)\n",
        "c1 = Vt.T[:, 0]\n",
        "c2 = Vt.T[:, 1]\n",
        "X"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  6.38144336,  17.01725492,  -9.66212276],\n",
              "       [ 11.63126723,   3.73275675,   5.88358088],\n",
              "       [ -3.46406728,  11.54922767,   7.52641605],\n",
              "       ...,\n",
              "       [  5.4931084 ,  16.74563414,   4.24279253],\n",
              "       [ -9.07578652,  16.76742501,  -3.73816409],\n",
              "       [  2.33157508,   6.96198493, -10.95992464]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "B6ntwTbagSFI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Projecting Down to d Dimensions\n",
        "\n",
        "Once you find all of the principal components of the dataset's space, you can project them down to a $d$-dimensional hyperplane by projecting each instance onto the first $d$ principal components.\n",
        "\n",
        "You can do so by multiplying the matrix of instances, $\\mathbf{X}$, by the matrix $\\mathbf{W}_d$ which is a matrix whose columns are made up of the first $d$ principal components, i.e.\n",
        "\n",
        "$$ \\mathbf{X}_\\text{d-proj} = \\mathbf{W}_d \\cdot \\mathbf{X}. $$"
      ]
    },
    {
      "metadata": {
        "id": "zykc7kD_hHjq",
        "colab_type": "code",
        "outputId": "48ec6d56-9aec-4eff-cfa4-2734c0b54574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "W2 = Vt.T[:, :2]\n",
        "X_2D = X_centered.dot(W2)\n",
        "X_2D"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  7.41422946,  -8.08518674],\n",
              "       [-10.00714385,  -6.13858947],\n",
              "       [ -3.44878455,   8.40813911],\n",
              "       ...,\n",
              "       [ -4.3288731 ,  -0.58920941],\n",
              "       [  9.59091616,   8.23450285],\n",
              "       [  9.46103517,  -5.95716486]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "_8OQxjufhcEK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using Scikit-Learn\n",
        "\n",
        "Scikit-Learn's `PCA` class uses SVD decomposition. An example of doing the projection above with Scikit-Learn is below."
      ]
    },
    {
      "metadata": {
        "id": "k5AuOQgfhpzx",
        "colab_type": "code",
        "outputId": "441cb3f5-beca-4ee1-c1c3-05f7105d6811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_2D = pca.fit_transform(X)\n",
        "X_2D"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-7.41422946, -8.08518674],\n",
              "       [10.00714385, -6.13858947],\n",
              "       [ 3.44878455,  8.40813911],\n",
              "       ...,\n",
              "       [ 4.3288731 , -0.58920941],\n",
              "       [-9.59091616,  8.23450285],\n",
              "       [-9.46103517, -5.95716486]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "ydSdN4yjiJ2i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Explained Variance Ratio\n",
        "\n",
        "The <i>explained variance ratio</i> is a list containing what percentage of the data's variance is along each particular principal component. Scikit-Learn's `PCA` class lets you access this data."
      ]
    },
    {
      "metadata": {
        "id": "YX753NLIiX8V",
        "colab_type": "code",
        "outputId": "5765a1c9-dc85-4665-9652-5c1d5593f861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.39200023, 0.32446571])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "ZAUBTLGRi1Ds",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Choosing the Right Number of Dimensions\n",
        "\n",
        "When using PCA, you want to project to a subspace which preserves about 95% of the variance of the original dataset. For visualization, you typically want to project the data onto a 2- or 3-dimensional dataset.\n",
        "\n",
        "The following code finds the principal components without reducing dimensionality, then computes how many dimensions you need to preserve 95% of the original variance."
      ]
    },
    {
      "metadata": {
        "id": "K2SohNi8kofW",
        "colab_type": "code",
        "outputId": "2828f850-7fbf-4d19-dbf2-0432b016f92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "np.argmax(cumsum >= 0.95) + 1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "6NoHPvOLmcdg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setting n_components to a float between 0 and 1, p, will have PCA automatically compute the dimension which preserves (100 * p) percent of the variance."
      ]
    },
    {
      "metadata": {
        "id": "SjrZAZ0Dljwj",
        "colab_type": "code",
        "outputId": "63a4b80f-d2bd-43ef-f495-7d84a7e475bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "len(X_reduced[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "2IAIOyRmmlVa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another option is to plot the explained variance as a function of the dimension. Once the curve reaches a certain dimension, the curve levels off.\n",
        "\n",
        "### PCA for Compression\n",
        "\n",
        "Projecting the data onto a lower dimension can speed up training a model without losing much information. For example, you can reduce the MNIST dataset from over 700 features to 150 features and preserve 95% of its variance.\n",
        "\n",
        "It is also possible to decompress the data back to its original dimension, but since a projection operation is not invertible, you do not get the lost variance back. The <i>reconstruction error</i> is the mean squared distance between the reconstructed dataset and the original one."
      ]
    },
    {
      "metadata": {
        "id": "44qOqS_vqZZ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "mnist.target = mnist.target.astype(np.int8)\n",
        "sorted_indices = np.argsort(mnist.target)\n",
        "X = mnist.data[sorted_indices]\n",
        "y = mnist.target[sorted_indices]\n",
        "rand_idx = np.random.permutation(len(X))\n",
        "X, y = X[rand_idx], y[rand_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4aGGhwaqyL_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=154)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "X_recovered = pca.inverse_transform(X_reduced)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x7CXLmzjrK6Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This equation for the inverse transformation is given by\n",
        "\n",
        "$$ \\mathbf{X}_\\text{recovered} = \\mathbf{X}_\\text{d-proj} \\cdot \\mathbf{W}_d^{\\;\\;T}. $$\n",
        "\n",
        "### Incremental PCA\n",
        "\n",
        "One of the weaknesses of the PCA method described above is that you need the entire training set in memory in order to use SVD. <i>Incremental PCA</i> (IPCA) is an alternative method which allows you to apply PCA online by using SVD on mini-batches of the dataset. Below is an example of using IPCA on the MNIST dataset."
      ]
    },
    {
      "metadata": {
        "id": "QQxsy1W4sUPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_batches = 100\n",
        "ipca = IncrementalPCA(n_components=154)\n",
        "for X_batch in np.array_split(X, n_batches):\n",
        "  ipca.partial_fit(X_batch)\n",
        "X_reduced = ipca.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oVDY_S08tlxY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "NumPy's [`memmap`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.memmap.html) class also lets you treat a binary file stored in disk as if it were stored as an array in memory. This allows you to use IPCA's `fit()` method once you specify the `batch_size` hyperparameter.\n",
        "\n",
        "### Randomized PCA\n",
        "\n",
        "Another method is <i>Randomized PCA</i> which approximates the first $d$ principal components with a computational complexity of $O\\left(m \\times d^{\\,2}\\right) + O\\left(d^{\\,3}\\right)$ whereas normal PCA runs in $O\\left(m\\times n^{\\,2}\\right) + O\\left( n^{\\,3} \\right)$ time. Randomized PCA is much faster when $d$ is much less than $n$.\n",
        "\n",
        "## Kernel PCA\n",
        "\n",
        "Recall the kernel trick from [chapter 5](https://github.com/DCtheTall/hands-on-machine-learning/blob/master/chapter05/SupportVectorMachines.ipynb) which was used to find nonlinear decision boundaries for training models. It turns out it is possible to use the kernel trick on PCA as well, this is known as [Kernel PCA](http://pca.narod.ru/scholkopf_kernel.pdf) (kPCA). It is good for preserving clusters of datasets and sometimes unrolling datasets that live on a twisted manifold.\n",
        "\n",
        "The following code is an example of using kPCA with an RBF kernel."
      ]
    },
    {
      "metadata": {
        "id": "YHvvnquWyQJw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "rbf_pca = KernelPCA(n_components=154, kernel='rbf', gamma=0.4)\n",
        "X_reduced = rbf_pca.fit_transform(X[:1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jqpU487RzzfS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Selecting a Kernel and Tuning Hyperparameters\n",
        "\n",
        "kPCA is an unsupervised learning aglorithm and it is difficult to define a performance metric to use for training. Typically unsupervised algorithms are a preprocessing step for supervised algorithms. One way you can tune kPCA to find the kernel and hyperparameters that lead to the best results for your supervised model. The code below is an example of using `GridSearchCV` to tune kPCA."
      ]
    },
    {
      "metadata": {
        "id": "KumMGcjM189j",
        "colab_type": "code",
        "outputId": "3d91b6b5-ddca-4197-e1be-7c9408e12ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = Pipeline([\n",
        "  ('kpca', KernelPCA(n_components=2)),\n",
        "  ('log_reg', LogisticRegression(solver='lbfgs', multi_class='auto')),\n",
        "])\n",
        "\n",
        "param_grid = [{\n",
        "  'kpca__gamma': np.linspace(0.03, 0.05, 5),\n",
        "  'kpca__kernel': ('rbf', 'poly'),\n",
        "}]\n",
        "\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
        "grid_search.fit(X[:1000], y[:1000])\n",
        "grid_search.best_params_"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kpca__gamma': 0.03, 'kpca__kernel': 'poly'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "B1KhGIvC4NEs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another way to tune kPCA is to find the kernel which does minimizes the reconstruction error. However, this is more mathematically complicated than reconstructing the data after linear PCA.\n",
        "\n",
        "The kernel trick maps the original data into an infinite-dimensional feature space, then uses linear PCA to project the infinite dimensional feature space to its principal components. Since it is not possible to compute the reconstruction error in the infinitely-dimensional feature space, we instead compute the reconstruction error by mapping the reduced space back to the original space. The code below is an example of doing this with Scikit-Learn."
      ]
    },
    {
      "metadata": {
        "id": "qcZ6KAkXG3NK",
        "colab_type": "code",
        "outputId": "68fd2f46-1467-46b8-ff86-0e569bda3843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "rbf_pca = KernelPCA(n_components=154, kernel='poly', gamma=0.03,\n",
        "                    fit_inverse_transform=True)\n",
        "X_reduced = rbf_pca.fit_transform(X[:1000])\n",
        "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
        "mean_squared_error(X_preimage, X[:1000])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.8386584102069303e-19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "XWOpoCeQH94Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LLE\n",
        "\n",
        "<i>Locally Linear Embedding</i> (LLE) is a form of nonlinear dimensionality reduction which uses Manifold Learning. In short, it works by finding the linear relationships between an instance and its neighbors then reduces the dimension of the dataset by finding components which preserve the relationship between them. The following code uses LLE to reduce the dimension of a Swiss roll dataset."
      ]
    },
    {
      "metadata": {
        "id": "KX5d99QNF_cK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "X, y = make_swiss_roll(n_samples=1000)\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_reduced = lle.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dAlt55-BGg0L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each training instace, $\\mathbf{x}^{(i)}$, LLE first finds its $k$ closest neighbors, then tries to reconstruct $\\mathbf{x}^{(i)}$ as a linear function of its neighbors by finding weights, $w_{i,j}$, that minimize the value\n",
        "\n",
        "$$ \\left(\\,\\mathbf{x}^{(i)} - \\sum\\limits_{j\\,=\\,1}^m w_{i,j} \\mathbf{x}^{(j)}\\right)^2 $$\n",
        "\n",
        "where $\\mathbf{w}_{i,j}$ is zero if $\\mathbf{x}^{(j)}$ is not one of the $k$ nearest neighbors and all nonzero weights sum to one. Let $\\mathbf{W}$ be the matrix of weights $w_{i,j}$, then the first step LLE becomes the optimization problem of finding\n",
        "\n",
        "$$ \\hat{\\mathbf{W}} = \\underset{\\mathbf{W}}{\\text{argmin}} \\sum\\limits_{i\\,=\\,1}^m \\left( \\mathbf{x}^{(i)} - \\sum\\limits_{j\\,=\\,1}^m w_{i,j} \\mathbf{x}^{(j)} \\right)^2 $$\n",
        "\n",
        "subject to the constraints that\n",
        "\n",
        "$$ w_{i,j} = 0 \\;\\;\\text{if}\\; \\mathbf{x}^{(j)} \\; \\text{is not one of the}\\, k \\, \\text{closest neighbors of}\\; \\mathbf{x}^{(i)} $$\n",
        "\n",
        "and also that\n",
        "\n",
        "$$ \\sum\\limits_{j\\,=\\,1}^m w_{i,j} = 1 \\;\\; \\text{for} \\;\\; i = 1, 2, ..., m. $$\n",
        "\n",
        "After finding the optimal weight matrix, $\\hat{\\mathbf{W}}$, for each training instance $\\mathbf{x}^{(i)}$, LLE finds the corresponding vector $\\mathbf{z}^{(i)}$ in the reduced, $d$-dimensional space that minimizes the distance between each $\\mathbf{z}^{(i)}$ and the sum\n",
        "\n",
        "$$ \\sum\\limits_{j\\,=\\,1}^m \\hat{w}_{i,j} \\, \\mathbf{z}^{(j)}. $$\n",
        "\n",
        "If we let $\\mathbf{Z}$ be the matrix of all of the training instances in the new, reduced space, then this becomes the following optimization problem of finding\n",
        "\n",
        "$$ \\hat{\\mathbf{Z}} = \\underset{\\mathbf{Z}}{\\text{argmin}} \\sum\\limits_{i\\,=\\,1}^m \\left( \\mathbf{z}^{(i)} - \\sum\\limits_{j\\,=\\,1}^m \\hat{w}_{i,j} \\, \\mathbf{z}^{(j)} \\right)^2. $$\n",
        "\n",
        "Scikit-Learn's LLE algorithm finds the $k$ nearest neighbors in $O(m \\log(m)n\\log(k))$ time, it finds the optimal weight matrix in $O(mnk^3)$ time, and it finds the low-dimensional representations in $O(dm^2)$ time. The $m^2$ term means this algorithm does not scale well for large datasets.\n",
        "\n",
        "## Other Dimensionality Reduction Techniques\n",
        "\n",
        "- <i>Multidimensional Scaling</i> (MDS) reduces the dimensionality while trying to preserve distances between instances.\n",
        "\n",
        "- <i>Isomap</i> creates a graph by connecting each instance to its nearest neighbors, then tries to preserve its <i>geodesic distance</i>, i.e. the number of nodes in the shortest distance between two nodes in the graph, between instances.\n",
        "\n",
        "- <i>t-Distributed Stochastic Neighbor Embedding</i> (t-SNE) tries to maintain the close distance between similar instances and tries to keep dissimilar instaces apart. This technique is mostly used tfor visualization of clusters in the training data.\n",
        "\n",
        "- <i>Linear Discriminant Analysis</i> (LDA) is a classification algorithm which finds which finds which axes influence its prediction and then projects the training data onto the resulting hyperplane. This algorithm tries to keep separate classes as far away as possible, and is a good way to reduce dimensionality before training another classification algorithm.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "### 1. What are the main motivations for reducing a dataset's dimensionality? What are the main drawbacks?\n",
        "\n",
        "Reducing a dataset's dimensionality can speed up training a predictive model. It also alleviates the curse of dimensionality, making the data less likely to have extreme values and making the data less sparse.\n",
        "\n",
        "The drawback is that by reducing the dimensionality of the dataset, you inevitably lose variance. This is why dimensionality reduction algorithms try to minimize the variance lost by transforming the dataset to a lower-dimesional space.\n",
        "\n",
        "### 2. What is the curse of dimensionality?\n",
        "\n",
        "The curse of dimensionality is the fact that high-dimensional datasets are both more likely to . have extreme values in a particular dimension and are also more likely to be very sparse. This makes training models with these datasets more difficult.\n",
        "\n",
        "### 3. Once a dataset's dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?\n",
        "\n",
        "Once a dataset's dimensionality has been reduced, it is not possible to completely invert the transformation. This is due to the fact that by reducing the dimension of the dataset, you inevitably lose some information about the original dataset.\n",
        "\n",
        "### 4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
        "\n",
        "Yes, you can use Kernel PCA to reduce the dimensionality of highly nonlinear datasets while still maintaining the nonlinear relationships between different instances.\n",
        "\n",
        "### 5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
        "\n",
        "The dimension of the resulting dataset will depend on the original dataset. It would often have  a lower dimensionality than the original dataset, but it is possible it would have the same dimensionality as well.\n",
        "\n",
        "### 6. In what case would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?\n",
        "\n",
        "You would use vanilla PCA when trying to reduce the dimensionality of a linear dataset that can fit entirely in the memory of a single process.\n",
        "\n",
        "You would use Incremental PCA if your dataset is too large to fit in memory of a single process and needs to be reduced incrementally.\n",
        "\n",
        "Randomized PCA is best for when the dimension of the target space is much less than the dimension of the original space, since Randomized PCA will be much faster.\n",
        "\n",
        "Kernel PCA is best for reducing the dimension of highly nonlinear datasets.\n",
        "\n",
        "### 7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?\n",
        "\n",
        "Since dimensionality reduction is generally a preprocessing step before training a supervised learning algorithm with the reduced dataset, you can measure the performance of dimensionality reduction by looking at the performance of the supervised model.\n",
        "\n",
        "Another way to measure the performance of dimensionality reduction is to transform the original dataset back to the original space and measure the mean squared distance between the instances after the inverse transform and the original instances.\n",
        "\n",
        "### 8. Does it make any sense to chain two different dimensionality reduction algorithms?\n",
        "\n",
        "No, it does not. One dimensionality reduction algorithm should be enough to reduce the dimension of the dataset without losing too much information."
      ]
    }
  ]
}