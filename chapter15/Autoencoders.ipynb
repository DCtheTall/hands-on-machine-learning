{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoders.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD2Yqnh1gXS8",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 15: Autoencoders\n",
        "\n",
        "Autoencoders are an unsupervised neural network architecture who are tasked with reproducing their input. They do so by learning how to encode their inputs using hidden layers that are _smaller_ than the input layer. This forces the model to learn an efficient representation of the data, called _codings_.\n",
        "\n",
        "Below is some setup code which the author uses for the code throughout the chapter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLsyTnsMgU2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# Plot styling.\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Code for saving figures.\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"autoencoders\"\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, 'images', '{}.png'.format(fig_id))\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl0abSFRt-yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_image(image, shape=[28, 28]):\n",
        "    plt.imshow(image.reshape(shape), cmap='Greys', interpolation='nearest')\n",
        "    plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olJpqlG4KYMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtkKk8HxGzL5",
        "colab_type": "text"
      },
      "source": [
        "## Performing PCA with an Undercomplete Linear Autoencoder\n",
        "\n",
        "An autoencoder with only linear activation functions that uses MSE as the loss function can be shown to be equivalent to PCA (Chapter 8)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQXLRrN3Gymx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a 3D dataset.\n",
        "\n",
        "import numpy.random as rnd\n",
        "\n",
        "rnd.seed(42)\n",
        "m = 200\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = rnd.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "data = np.empty((m, 3))\n",
        "data[:, 0] = np.cos(angles) + (np.sin(angles) / 2) + (noise * rnd.randn(m) / 2)\n",
        "data[:, 1] = (np.sin(angles) * 0.7) + (noise * rnd.randn(m) / 2)\n",
        "data[:, 2] = (data[:, 0] * w1) + (data[:, 1] * w2) + (noise * rnd.randn(m))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4N4ceT_H1-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale the data with StandardScaler.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(data[:100])\n",
        "X_test = scaler.transform(data[100:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKrj7NTVIl1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model graph.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 3\n",
        "n_hidden = 2\n",
        "n_outputs = n_inputs\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "hidden = tf.layers.dense(X, n_hidden)\n",
        "outputs = tf.layers.dense(hidden, n_outputs)\n",
        "\n",
        "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
        "\n",
        "opt = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = opt.minimize(reconstruction_loss)\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWZb2YAsJoK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the autoencoder.\n",
        "\n",
        "n_iterations = 100\n",
        "codings = hidden\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for i in range(n_iterations):\n",
        "    training_op.run(feed_dict={X: X_train})\n",
        "  codings_val = codings.eval(feed_dict={X: X_test})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUAIdt0oKDO0",
        "colab_type": "code",
        "outputId": "73066368-53a6-4ad1-8e8d-858714500108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "# Plotting the figure.\n",
        "\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "plt.plot(codings_val[:,0], codings_val[:, 1], 'b.')\n",
        "plt.xlabel('$z_1$', fontsize=18)\n",
        "plt.ylabel('$z_2$', fontsize=18, rotation=0)\n",
        "save_fig('linear_autoencoder_pca_plot')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving figure linear_autoencoder_pca_plot\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAADQCAYAAADcQn7hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc0YcMeLd8PE",
        "colab_type": "text"
      },
      "source": [
        "## Stacked Autoencoders\n",
        "\n",
        "Autoencoders with multiple hidden layers are called _stacked autoencoders_ (or _deep autoencoders_). You must be wary that if the autoencoder is too deep, it may just learn how to reproduce the training set, leading to overfitting. Below is a TensorFlow implementation of a stacked autoencoder with 3 hidden layers used for generating handwritten digits using the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGCKRSUzfNG9",
        "colab_type": "code",
        "outputId": "254165be-874d-409a-fd56-95e88cf472f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Downloading the data.\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuAyScFRf3Cd",
        "colab_type": "text"
      },
      "source": [
        "### Train all layers at once\n",
        "\n",
        "The example below trains all of the hidden layers of the stacked autoencoder at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0E4zltAf2iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model graph.\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 512\n",
        "n_hidden2 = 256\n",
        "n_hidden3 = n_hidden1\n",
        "n_outputs = n_inputs\n",
        "n_outputs = n_inputs\n",
        "\n",
        "learning_rate = 0.001\n",
        "l2_reg = 0.0001\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "\n",
        "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
        "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
        "\n",
        "dense = partial(tf.layers.dense, activation=tf.nn.relu,\n",
        "                kernel_initializer=he_init, kernel_regularizer=regularizer)\n",
        "\n",
        "hidden1 = dense(X, n_hidden1)\n",
        "hidden2 = dense(hidden1, n_hidden2)\n",
        "hidden3 = dense(hidden2, n_hidden3)\n",
        "outputs = dense(hidden3, n_outputs, activation=None)\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(outputs - X))\n",
        "opt = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = opt.minimize(loss)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWhm_cP7opNl",
        "colab_type": "code",
        "outputId": "00abad60-72a4-48d1-86d4-b26ffc5c72f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Train the model.\n",
        "\n",
        "n_epochs = 5\n",
        "batch_size = 150\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    n_batches = mnist.train.num_examples // batch_size\n",
        "    for i in range(n_batches):\n",
        "      print('\\r{}%'.format((100 * i) // n_batches), end=\"\")\n",
        "      sys.stdout.flush()\n",
        "      X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "      sess.run(training_op, feed_dict={X: X_batch})\n",
        "    loss_train = loss.eval(feed_dict={X: X_batch})\n",
        "    print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
        "    saver.save(sess, './my_model_all_layers.ckpt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Train MSE: 0.015859812\n",
            "1 Train MSE: 0.014265095\n",
            "2 Train MSE: 0.013388703\n",
            "3 Train MSE: 0.012680453\n",
            "4 Train MSE: 0.012523079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0fb-Hn6tGGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a plotting function.\n",
        "\n",
        "def show_reconstructed_digits(X, outputs, model_path=None, n_test_digits=2):\n",
        "  with tf.Session() as sess:\n",
        "    if model_path:\n",
        "      saver.restore(sess, model_path)\n",
        "    X_test = mnist.test.images[:n_test_digits]\n",
        "    outputs_val = outputs.eval(feed_dict={X: X_test})\n",
        "  for digit_idx in range(n_test_digits):\n",
        "    plt.subplot(n_test_digits, 2, (digit_idx * 2) + 1)\n",
        "    plot_image(X_test[digit_idx])\n",
        "    plt.subplot(n_test_digits, 2, (digit_idx * 2) + 2)\n",
        "    plot_image(outputs_val[digit_idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lm4IG21tzA_",
        "colab_type": "code",
        "outputId": "f86bd8cd-8414-4abd-fcbd-96fadb8df041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "show_reconstructed_digits(X, outputs, './my_model_all_layers.ckpt')\n",
        "save_fig('reconstruction_plot')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving figure reconstruction_plot\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEYCAYAAAAK467YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAASRQAEgiAQKAEH/BxBBrRta2BsNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cPKF-a2vNaL",
        "colab_type": "text"
      },
      "source": [
        "### Tying Weights\n",
        "\n",
        "When you are using a near-symmetric autoencoder (like the one above) then you can _tie the weights_ of the decoder layers to th weights of the encoder layers. Specifically, if the autoencoder has $N$ layers and $\\mathbf{W}_L$ is the weights tensor of the $L$<sup>th</sup> layer, then the decoder layer weights can be defined as\n",
        "\n",
        "$$ \\mathbf{W}_{N-L+1} = \\mathbf{W}^{\\;\\,T}_L $$\n",
        "\n",
        "Below is a TensorFlow implementation of a stacked autoencoder which ties weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAL4rZnpxeZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the model graph.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 512\n",
        "n_hidden2 = 256\n",
        "n_hidden3 = n_hidden1\n",
        "n_outputs = n_inputs\n",
        "n_outputs = n_inputs\n",
        "\n",
        "learning_rate = 0.001\n",
        "l2_reg = 0.00005\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "\n",
        "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
        "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
        "\n",
        "weights1_init = he_init([n_inputs, n_hidden1])\n",
        "weights2_init = he_init([n_hidden1, n_hidden2])\n",
        "\n",
        "weights1 = tf.Variable(weights1_init, dtype=tf.float32)\n",
        "weights2 = tf.Variable(weights2_init, dtype=tf.float32)\n",
        "weights3 = tf.transpose(weights2)\n",
        "weights4 = tf.transpose(weights1)\n",
        "\n",
        "bias = lambda n: tf.Variable(tf.zeros(n))\n",
        "bias1 = bias(n_hidden1)\n",
        "bias2 = bias(n_hidden2)\n",
        "bias3 = bias(n_hidden3)\n",
        "bias4 = bias(n_outputs)\n",
        "\n",
        "hidden = lambda X, W, b: tf.nn.elu(tf.matmul(X, W) + b)\n",
        "hidden1 = hidden(X, weights1, bias1)\n",
        "hidden2 = hidden(hidden1, weights2, bias2)\n",
        "hidden3 = hidden(hidden2, weights3, bias3)\n",
        "outputs = tf.matmul(hidden3, weights4) + bias4\n",
        "\n",
        "reconstruction_loss = tf.reduce_mean(tf.square(X - outputs))\n",
        "loss = reconstruction_loss + regularizer(weights1) + regularizer(weights2)\n",
        "opt = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = opt.minimize(loss)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCMl8t-v0uUI",
        "colab_type": "code",
        "outputId": "4f6a7ed4-aa81-44f4-92e2-992ebd84ae0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Train the model.\n",
        "\n",
        "n_epochs = 5\n",
        "batch_size = 150\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    n_batches = mnist.train.num_examples // batch_size\n",
        "    for i in range(n_batches):\n",
        "      print('\\r{}%'.format((100 * i) // n_batches), end=\"\")\n",
        "      sys.stdout.flush()\n",
        "      X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "      sess.run(training_op, feed_dict={X: X_batch})\n",
        "    loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
        "    print('\\r{}'.format(epoch), 'Train MSE:', loss_train)\n",
        "    saver.save(sess, './my_model_all_layers.ckpt')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Train MSE: 0.0056030927\n",
            "1 Train MSE: 0.004015474\n",
            "2 Train MSE: 0.0043796534\n",
            "3 Train MSE: 0.003961304\n",
            "4 Train MSE: 0.00423302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPKL-lwC1s--",
        "colab_type": "code",
        "outputId": "2ae27a24-2d6c-45b3-ee25-4740100a1159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "show_reconstructed_digits(X, outputs, './my_model_all_layers.ckpt')\n",
        "save_fig('tying_weight_reconstruction_plot')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving figure tying_weight_reconstruction_plot\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEYCAYAAAAK467YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgxLRw3KJ5Cy",
        "colab_type": "text"
      },
      "source": [
        "### Train One Autoencoder at a Time in Multiple Graphs\n",
        "\n",
        "You can train a stacked autoencoder in parts:\n",
        "\n",
        "1. First you train the model to reproduce the input layer only using one hidden layer.\n",
        "\n",
        "2. You train an autoencoder which tries to reproduce the output of the first hidden layer (which trains the second and third hidden layer).\n",
        "\n",
        "You can then combine the two results for a fully-functional autoencoder. Below is an implementation of training a stacked autoencoder this way using multiple TensorFlow graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQH99g7CLYrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function for training an autoencoder in the first 2 steps.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "def train_autoencoder(X_train, n_units, n_epochs, batch_size,\n",
        "                      learning_rate=0.001, l2_reg=0.00005, seed=42,\n",
        "                      hidden_activation=tf.nn.elu,\n",
        "                      output_activation=tf.nn.elu):\n",
        "  graph = tf.Graph()\n",
        "  with graph.as_default():\n",
        "    tf.set_random_seed(seed)\n",
        "\n",
        "    n_inputs = X_train.shape[1]\n",
        "\n",
        "    X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "\n",
        "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
        "    regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
        "    dense = partial(tf.layers.dense, kernel_initializer=he_init,\n",
        "                    kernel_regularizer=regularizer)\n",
        "    \n",
        "    hidden = dense(X, n_units, activation=hidden_activation, name='hidden')\n",
        "    outputs = dense(hidden, n_inputs, activation=output_activation,\n",
        "                    name='outputs')\n",
        "\n",
        "    reconstruction_loss = tf.reduce_mean(tf.square(X - outputs))\n",
        "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
        "\n",
        "    opt = tf.train.AdamOptimizer(learning_rate)\n",
        "    training_op = opt.minimize(loss)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "      n_batches = len(X_train) // batch_size\n",
        "      for i in range(n_batches):\n",
        "        print('\\r{}%'.format(100 * i // n_batches), end='')\n",
        "        sys.stdout.flush()\n",
        "        indices = rnd.permutation(len(X_train))[:batch_size]\n",
        "        X_batch = X_train[indices]\n",
        "        sess.run(training_op, feed_dict={X: X_batch})\n",
        "      loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
        "      print('\\r{}'.format(epoch), 'Train MSE:', loss_train)\n",
        "    params = {\n",
        "      var.name: var.eval()\n",
        "      for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "    }\n",
        "    hidden_val = hidden.eval(feed_dict={X: X_train})\n",
        "  return hidden_val, params['hidden/kernel:0'], params['hidden/bias:0'], \\\n",
        "      params['outputs/kernel:0'], params['outputs/bias:0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lyzWX_pR8b1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "6dbf94cd-fe62-401c-d3eb-64d16fdfc8d4"
      },
      "source": [
        "# First step of training.\n",
        "\n",
        "hidden_output, W1, b1, W4, b4 = train_autoencoder(mnist.train.images,\n",
        "                                                  n_units=256, n_epochs=5,\n",
        "                                                  batch_size=150,\n",
        "                                                  output_activation=None)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Train MSE: 0.011765065\n",
            "1 Train MSE: 0.006190534\n",
            "2 Train MSE: 0.005030507\n",
            "3 Train MSE: 0.0047817174\n",
            "4 Train MSE: 0.0047518923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxAu22hgTy5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f1c55dc1-7a71-4c20-cb91-70bfce2328c0"
      },
      "source": [
        "# Second step of training.\n",
        "\n",
        "_, W2, b2, W3, b3 = train_autoencoder(hidden_output, n_units=128, n_epochs=5,\n",
        "                                      batch_size=150)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Train MSE: 0.03374616\n",
            "1 Train MSE: 0.013731597\n",
            "2 Train MSE: 0.0074159987\n",
            "3 Train MSE: 0.0049135415\n",
            "4 Train MSE: 0.0034204917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16e73kYuUFQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Putting the results together.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "hidden1 = tf.nn.elu(tf.matmul(X, W1) + b1)\n",
        "hidden2 = tf.nn.elu(tf.matmul(hidden1, W2) + b2)\n",
        "hidden3 = tf.nn.elu(tf.matmul(hidden2, W3) + b3)\n",
        "outputs = tf.matmul(hidden3, W4) + b4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctjtV9WjXHYZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "23b09330-d080-49ce-cbce-c60ab22489d6"
      },
      "source": [
        "show_reconstructed_digits(X, outputs)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT0AAAD/CAYAAACDzAGWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6QFwhaQHwBWSHgBXSHoAXCHpAXDlPz6qsMVlTLvxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkVsK0w-XdSq",
        "colab_type": "text"
      },
      "source": [
        "### Training One Autoencoder at a Time in a Single Graph\n",
        "\n",
        "Below is another implementation of the same technique, but this time using just a single TensorFlow graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpmoICWZZyLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the graph.\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 ** 2\n",
        "n_hidden1 = 256\n",
        "n_hidden2 = 128\n",
        "n_hidden3 = n_hidden1\n",
        "n_outputs = n_inputs\n",
        "\n",
        "learning_rate = 0.001\n",
        "l2_reg = 0.0005\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
        "\n",
        "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
        "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
        "\n",
        "init_weights = lambda n1, n2: tf.Variable(he_init([n1, n2]), dtype=tf.float32)\n",
        "W1 = init_weights(n_inputs, n_hidden1)\n",
        "W2 = init_weights(n_hidden1, n_hidden2)\n",
        "W3 = init_weights(n_hidden2, n_hidden3)\n",
        "W4 = init_weights(n_hidden3, n_outputs)\n",
        "\n",
        "init_bias = lambda n: tf.Variable(tf.zeros(n), dtype=tf.float32)\n",
        "b1 = init_bias(n_hidden1)\n",
        "b2 = init_bias(n_hidden2)\n",
        "b3 = init_bias(n_hidden3)\n",
        "b4 = init_bias(n_outputs)\n",
        "\n",
        "hidden1 = tf.nn.elu(tf.matmul(X, W1) + b1)\n",
        "hidden2 = tf.nn.elu(tf.matmul(hidden1, W2) + b2)\n",
        "hidden3 = tf.nn.elu(tf.matmul(hidden2, W3) + b3)\n",
        "outputs = tf.matmul(hidden3, W4) + b4\n",
        "\n",
        "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La0M7I5yjyBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the training objective for the 1st phase of training.\n",
        "\n",
        "with tf.name_scope('phase1'):\n",
        "  phase1_outputs = tf.matmul(hidden1, W4) + b4\n",
        "  phase1_reconstruction_loss = tf.reduce_mean(tf.square(X - phase1_outputs))\n",
        "  phase1_reg_loss = regularizer(W1) + regularizer(W4)\n",
        "  phase1_loss = phase1_reconstruction_loss + phase1_reg_loss\n",
        "  phase1_training_op = optimizer.minimize(phase1_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDPd947PluVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the training objective for the 2nd phase of training.\n",
        "\n",
        "with tf.name_scope('phase2'):\n",
        "  phase2_reconstruction_loss = tf.reduce_mean(tf.square(hidden1 - hidden3))\n",
        "  phase2_reg_loss = regularizer(W2) + regularizer(W3)\n",
        "  phase2_loss = phase2_reconstruction_loss + phase2_reg_loss\n",
        "  train_vars = [W2, b2, W3, b3]\n",
        "  phase2_training_op = optimizer.minimize(phase2_loss, var_list=train_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_E6Mdxm04Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BspLbkCm-lV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "d08bf7a3-7b15-4695-fa23-967e2e33c2b7"
      },
      "source": [
        "# Training the model.\n",
        "\n",
        "training_ops = [phase1_training_op, phase2_training_op]\n",
        "reconstruction_losses = \\\n",
        "    [phase1_reconstruction_loss, phase2_reconstruction_loss]\n",
        "n_epochs = 5\n",
        "batch_size = 150\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for phase in range(2):\n",
        "    print('Training phase {}'.format(phase + 1))\n",
        "    for epoch in range(n_epochs):\n",
        "      n_batches = mnist.train.num_examples // batch_size\n",
        "      for i in range(n_batches):\n",
        "        print('\\r{}%'.format(100 * i // n_batches), end='')\n",
        "        sys.stdout.flush()\n",
        "        X_batch, _ = mnist.train.next_batch(batch_size)\n",
        "        sess.run(training_ops[phase], feed_dict={X: X_batch})\n",
        "      loss_train = reconstruction_losses[phase].eval(feed_dict={X: X_batch})\n",
        "      print('\\r{}'.format(epoch), 'Train MSE:', loss_train)\n",
        "      saver.save(sess, './my_model_one_at_a_time.ckpt')\n",
        "  loss_test = reconstruction_loss.eval(feed_dict={X: mnist.test.images})\n",
        "  print('Test MSE:', loss_test)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training phase 1\n",
            "0 Train MSE: 0.017501539\n",
            "1 Train MSE: 0.0182057\n",
            "2 Train MSE: 0.018151395\n",
            "3 Train MSE: 0.018360477\n",
            "4 Train MSE: 0.01812754\n",
            "Training phase 2\n",
            "0 Train MSE: 0.0034679\n",
            "1 Train MSE: 0.0033041218\n",
            "2 Train MSE: 0.0033808684\n",
            "3 Train MSE: 0.003458103\n",
            "4 Train MSE: 0.0033613483\n",
            "Test MSE: 0.021169571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n9zsntvuYWY",
        "colab_type": "text"
      },
      "source": [
        "### Caching the Frozen Layer Outputs\n",
        "\n",
        "One way to speed up training is to cache the outputs of the previous phase of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoB0iEtlu1CZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "30bb386d-3aa3-4e3c-db81-3d4f89bccd2e"
      },
      "source": [
        "training_ops = [phase1_training_op, phase2_training_op]\n",
        "reconstruction_losses = \\\n",
        "    [phase1_reconstruction_loss, phase2_reconstruction_loss]\n",
        "n_epochs = 5\n",
        "batch_size = 150\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for phase in range(2):\n",
        "    print('Training phase {}'.format(phase + 1))\n",
        "    if phase == 1:\n",
        "      hidden1_cache = hidden1.eval(feed_dict={X: mnist.train.images})\n",
        "    for epoch in range(n_epochs):\n",
        "      n_batches = mnist.train.num_examples // batch_size\n",
        "      for i in range(n_batches):\n",
        "        print('\\r{}%'.format(100 * i // n_batches), end='')\n",
        "        sys.stdout.flush()\n",
        "        if phase == 1:\n",
        "          indices = rnd.permutation(mnist.train.num_examples)\n",
        "          hidden1_batch = hidden1_cache[indices[:batch_size]]\n",
        "          feed_dict = {hidden1: hidden1_batch}\n",
        "        else:\n",
        "          X_batch, _ = mnist.train.next_batch(batch_size)\n",
        "          feed_dict = {X: X_batch}\n",
        "        sess.run(training_ops[phase], feed_dict=feed_dict)\n",
        "      loss_train = reconstruction_losses[phase].eval(feed_dict=feed_dict)\n",
        "      print('\\r{}'.format(epoch), 'Train MSE:', loss_train)\n",
        "      saver.save(sess, './my_model_cache_frozen.ckpt')\n",
        "  loss_test = reconstruction_loss.eval(feed_dict={X: mnist.test.images})\n",
        "  print('Test MSE:', loss_test)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training phase 1\n",
            "0 Train MSE: 0.018148554\n",
            "1 Train MSE: 0.018245481\n",
            "2 Train MSE: 0.017851792\n",
            "3 Train MSE: 0.01792747\n",
            "4 Train MSE: 0.018353654\n",
            "Training phase 2\n",
            "0 Train MSE: 0.0034180086\n",
            "1 Train MSE: 0.0033926931\n",
            "2 Train MSE: 0.0034894291\n",
            "3 Train MSE: 0.0032823698\n",
            "4 Train MSE: 0.003308158\n",
            "Test MSE: 0.021020621\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}